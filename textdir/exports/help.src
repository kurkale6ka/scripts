help/AWS/aws.adoc
:toc: left
:toc-title: AWS

= AWS (Amazon Web Services)

== EC2 (Elastic Compute Cloud)

=== tenancy
* VPC dedicated tenancy => EC2 dedicated
* VPC   default tenancy => EC2 dedicated only if the launch config says so, else shared

EC2 tenancy can change from dedicated to host and vice-versa

=== Reserved Instances vs Capacity Reservations

Use reserved instances if you want to commit to 1 or 3 years

Amazon EC2 on-demand Capacity Reservations enable you to create and manage
reserved capacity on Amazon EC2 without any long-term commitment or fixed
terms. This can be very beneficial if you regularly face
`InsufficientInstanceCapacity` errors when AWS doesn't have enough available
on-demand capacity while starting or launching an EC2 instance.

For cluster placement groups, capacity also means running on the 'same'
hardware and that hardware needs to be reserved.

=== Placement Groups

  cluster -  low latency,  low availability: share hardware => same EC2 type                                                                     <= HPC
partition -  avg latency,  avg availability: spread partitions across hardware, 7 partitions per AZ                                              <= Big Data
   spread - high latency, high availability: multi AZ, 7 instances in different racks in 1 group per AZ, diff hw allows for different EC2 types  <= HA

=== Metadata
curl http://169.254.169.254/latest/meta-data/

the `cloud-init` service will make use of `user-data` in order to provision instances at boot.

== ELB (Elastic Load Balancer)

LBs are managed and highly scalable => behind the scenes there are many LB instances. +
cross-zone loadbalancing: traffic distributed evenly across all targets in all AZ (NLB: disabled by default + $)

=== ALB (Application Load Balancer)
* distribute load based on http/https/websocket (stateful unlike http)
* connection/SSL termination (else we can't parse the request as it is encrypted)

dispatch requests to target groups based on routes, hostnames, query string, headers:

                                         /url1 -> tg1
                        /url2, one.example.com -> tg2
two.example.com, ?platform=mobile, HTTP header -> tg3

then spread load to targets on multiple/single (e.g containers) machine(s) in
the target group, based on health checks (port + route) =>
seamless handling of downstream instances failures

=== NLB (Network Load Balancer)
* distribute load based on TCP, UDP, TLS + port
* pass through (but TLS offloading possible), request IP goes all the way to the app
* static IPs per AZ => customers can whitelist us

== CloudFront (CDN - cache content at the edge)

=== defenses
- endpoint ELBs will only see CloudFront IPs, not client ones =>
  NACL has no effect on allowing/denying traffic. our only line of defense is WAF
- whitelist/blacklist geo restriction

=== origins
* S3
* any http/https/rtmp endpoint +
                 +- real-time messaging protocol

multi-origins based on `/path/*` +
origin groups (primary/secondary) for failover

=== price classes
* class all - all regions
* class 200 - all, without the most expensive ones
* class 100 - least expensive regions only

=== field level encryption
extra security on top of https - specify up to 10 fields in your POST request +
asymmetric encryption (e.g credit card details) on the edge, decrypted by app's (e.g behind ALB origin) private key

== Global Accelerator (use edge locations to accelerate APPS (no S3), no caching)

It's a global load balancer! +
region endpoint groups (akin to TG) => health checks => fast regional failover +
                +- e.g ELBs but can also be EC2s

* UDP, IoT (MQTT), VOIP endpoint
* http/https            endpoint (if static IPs needed: 2 anycast IPs)

---

blue-green deployment:: both DNS routing and global accelerator can be used

== EBS (Elastic Block Store)
                            max
gp3 | 1 GiB to 16 TiB |  16 000  iops | 1000 MiB/s | not multi-attach
io2 | 4 GiB to 16 TiB |  32 000 piops |            |
    |                 |  64 000 piops |            | + Nitro
    | 4 GiB to 64 TiB | 256 000 piops |            | + block express

iops is about read/write performance

=== encrypt unencrypted EBS volume / RDS database

create a snapshot, then:

- create new encrypted EBS volume from it or
- copy it into an encrypted one, create new volume/db from it

== FSx: 3rd party high-performance file systems

shared storage (we need to create mount ENI targets, to be mounted on EC2, ...):

* EFS (NFS),
* FSx for Lustre (Linux cluster)
* FSx for Windows (SMB, NTFS)

---

FSx persistent file system:: data is replicated
FSx scratch    file system:: temp storage, faster, cheaper

== Storage Gateway (on-premises -> S3)

hybrid storage integration (storage gateways needed because S3 proprietary):

 on-premises                                                       | cloud
 ------------------------------------------------------------------+--------------------------------------
  app server,     file gateway (NFS, SMB), IAM, optional AD auth   | S3, S3 IA (both) -> S3 IA, S3 glacier
              FSx file gateway (SMB, NTFS, AD), cache              | FSx for Windows file server
  app server,   volume gateway (iSCSI)                             | S3               -> S3 EBS snapshots
 data server,     tape gateway (iSCSI), VTL (Virtual Tape Library) | S3               -> S3 glacier

=== volume gateway specific
cached volumes:: main data is on S3 with local on-prem cache for fast access
stored volumes:: main data is on-prem with async backup to S3

virtualization is needed to install the gateways, instead we can buy a HW appliance

== ASG (Auto Scaling Group)

- use a launch template to provision a mix of on-demand & spot instances
- increase deregistration delay to not interrupt long running processes when scaling-in

=== EC2 instances are in one of the following states
- InService
- Standby: helps you temporarily remove an instance from the ASG

=== EC2 status check (ok or impaired)
* instance status check
*   system status check => AWS responsibility to repair

a recovered instance is almost identical to the original one

- preserved: ID + metadata, IPs (private, public, Elastic)
-      lost: RAM data

after scale in/out activites ASG enters the `HealthCheckGracePeriod`, +
allowing health checks to stabilize before launching/terminating more instances

Use golden AMI so updates, app install, ... take less time => we can set a smaller `HealthCheckGracePeriod` (aka cooldown period)

== Spot instances

* a persistent spot request is like an ASG. it will keep launching/terminating instances till the end of its validity period
* you can only cancel spot requests that are open, active, or disabled!
* if a spot request is persistent, then it is reopened after your spot instance is interrupted (not stopped)
* spot blocks (instances) with a defined duration (1, 2, 3, 4, 5, or 6h) are designed not to be interrupted
* spot fleets = spot instances + optional on-demand instances

== RDS (Relational Database Service)

In AWS there is a network cost when data moves between AZs,
but not for read replicas (only cross-region)

== Aurora DB

auto scaling storage (10GB - 128TB, redshift: 1-128 nodes each up to 128TB) +
writer + reader OR custom endpoint(s)

read replicas:

* each replica is associated with a priority tier (0-15)
* failover: promote replica with lowest-tier & max-size (highest priority combination)

== Redshift

WARNING: *_no multi-AZ_*: better enable automated cluster snapshots cross-region COPY (every 8h, 5GB or on a schedule) for DR

spectrum:: perform queries directly against S3 (no need to load)
enhanced VPC routing:: stay within VPC, no public Internet

MPP (Massively Parallel Processing) +
both redshift and athena use Presto (distributed SQL query engine)

== ElastiCache

* heavy code changes required
* no IAM auth, Redis auth or Memcached SASL

=== Redis only features
* advanced data structures (e.g sorted sets for real-time leaderboards)
* snapshots
* replication
* transactions
* pub/sub
* lua scripting
* geospatial support

=== Memcached only feature
multithreaded architecture

== Route 53

Alias:: CNAME to 1 managed AWS resource (no EC2!),
        no TTL, can point to zone apex, free

health checks:: only return IPs for healthy resources,
                e.g give me a healthy ALB, then target group health check to give me a healthy EC2 instance

record with multiple A values -> the client will choose at random (client side LB)

=== routing policies

- simple (no health checks)
- weighted
  * weight.example.com 70 - 7.8.9.1
  * weight.example.com 30 - 3.4.5.6
  * weight.example.com 10 - 1.1.8.8
- latency
- failover (primary active / secondary passive)
- geolocation (default IP mandatory)
- geoproximity (traffic flow, bias -1 .. 99)
- multi-value (again client side LB but with health checks, return up to 8 IPs)

GoDaddy registrar with Route 53 DNS: +
register domain with GoDaddy but specify custom nameservers (AWS ones) where the records will be defined

== Beanstalk

dev centric view, infrastructure is transparent

 PaaS: versioned application / environment (dev,test,prod) +    web tier (ELB -> ASG) or
                                                             worker tier (SQS <- ASG)

you retain full control over the provisioned AWS resources and can access them at any time

== S3 (Simple Storage Service)

- object storage (vs file system) does not allow for in-place edits => not good for collaboration.
- by default, an S3 object is owned by the AWS account that uploaded it => the S3 owner might not have permission to view the objects
- web URL, http/https

3_500 PUT req/s per prefix +
5_500 GET req/s per prefix, *both limited by KMS* (5_500, 10_000, 30_000 req/s based on region, increase with quotas)

CAUTION: _naming_: 3-63 -> no upper, _, IP; start with [a-z0-9]

 s3://bucket-name/folder-1/folder-2/my-image.jgp - max 5TB, multi-part upload if >5GB
                  prefix          + name = key

static website endpoint (.region or -region)::
- http://my-bucket.s3-website.region.amazonaws.com
- http://my-bucket.s3-website-region.amazonaws.com

with versioning enabled, removal of an object adds a 'delete marker'. +
deleting a specific version or a 'delete marker' one is permanent.

you can place a retention period on an object version either explicitly (Retain Until Date) or through a bucket default setting.
like all other object lock settings, retention periods apply to individual object versions

=== storage classes
std | intelligent-tiering | std-ia | 1 zone-ia | glacier | glacier deep archive

        std, std-ia -> 30 days min stay before transition to std-ia or 1 zone-ia
intelligent tiering -> small monthly monitoring and auto-tiering fee
     amazon glacier -> vaults/archives naming
                       90 days min charge, 180 for deep archive, others 30 (bar std)
retrieval cost per GB for all but std/intelligent

lifecycle rules

- transition actions
- expiration actions (deletion)

*_replication isn't chained:_* +
A -> B -> C doesn't mean A -> C. +
objects in B replicated from A aren't considered new. only explicit new ones will be replicated to C

=== encryption
metadata is NOT encrypted

 SSE-S3  = "x-amz-server-side-encryption": "AES256",  in header
 SSE-KMS = "x-amz-server-side-encryption": "aws:kms", in header
 SSE-C   =                                       key, in header (https mandatory)
         => CloudHSM (hardware security module, must use client software)
            * single-tenant, multi-AZ
            * FIPS 140-2 Level 3 (Federal Information Processing Standard)
            * MFA + access & authentication management (users & keys) vs IAM
            * hardware acceleration
            * supported by Redshift
 CSE     = client side encryption (could use the Amazon S3 Encryption Client)

the default encryption setting will be applied only to non-encrypted objects,
meaning that if an object is already encrypted (e.g via bucket policy) it won't be altered.

=== block all public access
* to buckets/objects                 via new ACLs
* to buckets/objects                 via ANY ACLs (existing ones too)
* to buckets/objects                 via new public bucket or access point policies
* to buckets/objects + cross-account via ANY public bucket or access point policies

=== pre-signed URLs
generate GET ones with cli, GET/PUT ones with SDK (creator's get/put permissions inherited by users),
valid for a limited time only (3600s by default)

=== with CloudFront in front of our bucket, we must use CloudFront signed (e.g use lambda@edge)
* URLs for single files
* signed cookies for multiple files

because bucket access is restricted to the OAI

=== CORS: cross-origin resource sharing (web browser security mechanism)

 get index.html                         from www.example.com (origin - protocol://domain:port),
     index.html tries to get a resource from   net.games.com (cross-origin)
                                               net.games.com needs to send headers Access-Control-Allow-Origin:  https://www.example.com
                                                                                   Access-Control-Allow-Methods: GET, ...

=== S3TA (S3 Transfer Acceleration)
preferred over CloudFront + S3 for content bigger than 1GB

== EFS (Elastic File System)

file storage: managed NFS

protect EFS with:

* access points: manage app access
  override clients uid/gid then use rwx permissions (clients uid/gid trusted by default)
* VPC SGs to control traffic to and from the file system
* IAM policy for mount permissions (who can mount the fs)

== Snow family (EBS and S3 storage)

             snowcone (  8TB)             2  CPU,   4GiB RAM, no battery/cables, can use DataSync once online
snowball edge compute ( 42TB) optimized: 52 vCPU, 208GiB RAM, optional GPU
snowball edge storage ( 80TB) optimized: 40 vCPU,  80GiB RAM, up to 15 nodes storage cluster, cannot import to glacier directly
           snowmobile (100PB) - prefer to snowball if >10PB

OpsHub:: AWS 'snow' Console on your laptop

== Async app to app communication

                         queue model: SQS (256kb per msg, 4 to 14 days retention) <- poll for up to 10 messages
                       pub/sub model: SNS
real-time streaming (~pub/sub) model: kinesis data streams (records with partition key, same key goes to same shard => ordering can be achieved)

== Kinesis Data streams (retention: 1(default) - 365days, replay...)

        producers                     consumers
kinesis agent, SDK, KPL  ‒  SDK, KCL (=> EC2, lambda, ...)
 1 MB/s (or 1000 msg/s)  ‒  2 MB/s per shard per all      (shared)
              per shard  ‒  2 MB/s per shard per consumer (enhanced fanout)

== Kinesis Data Firehose

producers:: SDK, agent, data streams/logs/events/IoT
consumers:: batch writes (*near real time*) - 32MB or 60s

 => S3
 => redshift (via S3)
 => ElasticSearch (now OpenSearch)

custom data transform with lambda

== SQS (Simple Queue Service)

visibility timeout (30s):: message invisible to other consumers,
                           `ChangeMessageVisibility` API call if not done processing

MaximumReceives:: times a msg is allowed to go back to the queue,
                  then move it to DLQ (dead letter queue)

---

- delay queue: postpone all new messages for up to 15min,
               send with DelaySeconds can override this
- message timer: delay period for a single message

long polling (up to 20s) => less API calls. enable at Q level or WaitTimeSeconds API

=== ordering
FIFO + group ID:: block group A messages for other consumers while a group A +
                  batch is in flight (being processed, eg. A3-A2-A1), else a +
                  consumer could process say A4 before and the ordering would be broken => A3-A2-A1-A4

=== request-response systems (producers want a response back)
the producers (requesters) send messages ( [ID/response Q (answer expected there)] ) to a single request Q, +
the consumers (responders) reply via many virtual Qs (SQS Temporary Queue Java Client needed)

== SNS (Simple Notification Service)

         +- topic,
publish -+- phone (SMS),
         +- platform endpoint (e.g ADM: Amazon Device Messaging)

100_000 topics -> 12_500_000 subscriptions per topic (optional JSON policy to filter messages)

FIFO topic:: ordering of messages per group +
             subscribers can only be SQS FIFO (throughput limited to 300/s -> up to 3000/s in batch mode with batches of 10)

== ECS (Elastic Container Service)

launch types

* Fargate: 1 ENI (private IP) per task,     the task  will use  an ECS task role
*     EC2: 1 ECS agent        per instance, the agent will use the EC2 instance profile role <-> ECS, ECR, CloudWatch

 ECS cluster
    container instances (e.g EC2)
       services - our app will be a versioned service (v1, v2, ...)
           tasks - tasks are isolated in services, many services can be defined on the same container instances.

share data among tasks by mounting EFS volumes onto the tasks

scaling::
CloudWatch alarm (e.g on CPU service usage) -> service auto scale -> [for EC2 launch type we would also need an ASG for the container instances]

=== ports and SG

==== ALB - TG
individual processes run on separate EC2 instances => +
if ALB listens on port 80, the process can also listen on port 80

==== ALB - service on EC2
multiple tasks can reside on the same container instance => if ALB listens on
port 80, all tasks can't listen on port 80 so they listen on random ports which
the ALB will automatically find, therefore on the container instance's SG we
must allow all ports from ALB's SG

==== ALB - service on Fargate
on the ENIs SGs allow the task port from the ALB SG

== CloudWatch

=== Metrics
* namespaces (e.g EC2) + up to 10 dimensions (identification attributes)
* custom metrics:
+
`PutMetricData` API call (accepts data points 2 weeks in the past & 2h in the future)

 StorageResolution - std: 1min, high: write 1s
                                       read 1/5/10/30s
+
* metric filter: metric based on CloudWatch Logs filter

---

EC2 detailed monitoring:: metrics every 1 min vs 5 min, no RAM metric

=== Logs
- query logs with insights
- unified agent (EC2/on-prem, old: logs agent)
    * extra system-level metrics
    * centralized configuration with SSM parameter store
- export (up to 12h to become ready): `CreateExportTask` API call, *not real or near-real* time
- subscription filter: real time (pub/sub - lambda, kinesis)

== EventBridge (old CloudWatch events)

* can intercept any AWS events and define action targets for them.
* define CRON jobs (execute task with lambda)
* it can be used to simulate SQS between 3rd parties (SaaS)

---

event bus::
1. _default_: for AWS services
2. _partner_: receive events from 3rd party (send events too???)
3.  _custom_:  own bus

schema registry:: collection of JSON events to help generate code

== CloudTrail

90 days retention for events

* management events (e.g CreateSubnet; can separate Read/Write)
* data events (e.g GetObject; not logged by default)

 enable insights to continuously analyse management write events in order to detect unusual activity => console
                                                                                                     => S3
                                                                                                     => EventBridge event

== AWS Config

* record configuration changes
* evaluate compliance rules (managed or custom with lambda: e.g are all EBS disks of type io2?) +
                      +- eval/trigger per change or at intervals +
                      +- remediation of non-compliant resources with SSM automation documents (managed or custom)

== Lambda

400_000 GB-seconds of compute time per month for free: +
400_000 seconds if function is 1GB RAM

limits per region::
* 128MB - 10GB RAM
* 15min (900s)
* 1000 concurrent executions
* env    4KB
* /tmp 512MB, up to 10GB ($)
* size  50MB compressed or 250MB uncompressed

== Step Functions

coordinate and orchestrate multiple AWS services (lambda, glue, ...) into serverless workflows (visual or JSON state machine)

* maximum execution time of 1 year.
* possibility to implement human approval feature

use SWF (Simple Workflow Service, EC2 => not serverless) instead if:

- you need external signals
- you need child processes

== DynamoDB

react to changes by enabling streams (and we get 24h data retention)

== API Gateway (vs ALB)

- edge-optimized (CloudFront) by default
- serverless + we can add an ALB (not needed for lambda since lambdas spring into existence => there is always an 'idle' lambda ready to take on load => LB is N/A)
- environments (dev/test/prod)
- authentication & authorization via cognito
- request throttling/transform
- caching
- expose any AWS service

security

- internal: IAM permissions in headers (leverages sig v4)
- 3rd party (OAuth, SAML): token in headers, validate with lambda authorizer and return IAM policy (can be cached)
- CUP: authentication only

== Cognito (works with API gw, ALB, not CloudFront)

federated means 3rd party source (e.g Google, Facebook)

=== Authenticate to app
Cognito User Pools (CUP is an IdP, an identity provider: serverless db of users), +
sign-in (verif, MFA, ...) -> JSON web token

=== Authorize to use AWS resources
Cognito Identity (role) Pools (credentials provider, prefer to `AssumeRoleWithWebIdentity`) +
login to get token from IdP (Facebook, CUP, ...) +
Identity Pool verifies token and gets IAM creds from STS

AppSync (old Cognito sync):: save app state (20 datasets - 1MB), devices sync, offline, id pool needed

== STS (Security Token Service)

grant limited and temporary access to resources (token valid for 15min - 1h)

* AssumeRole... (STS APIs)
   - AWS: dev account -- assume UpdateProdBucket role: STS gives token --> modify prod account bucket
   - 3rd: IdP (e.g ADFS) sends SAML assertion, `AssumeRoleWithSAML`,          STS returns temp creds
                                               POST assertion to SSO endpoint
* `GetSessionToken` (for MFA)

federation with SAML 2.0 is the old way, prefer SSO federation

== SSO (Single Sign-On)

when we need to login to:

* many AWS accounts
* many 3rd party business apps (Slack, Dropbox, Office 365, ...)
* many custom SAML applications
+
Id store - [3rd IdP portal] - AssumeRoleWithSAML +
Id store - [3rd IdP portal] - AssumeRoleWithSAML +
Id store - [3rd IdP portal] - AssumeRoleWithSAML +
           +- with SSO no need to manage all these portals, we connect directly to the Id store

== Directory Services

Microsoft AD:: centralized users/assets management from the domain controller

---

* managed Ms AD: on-prem <=> AWS - manage users on both (MFA supported)
* AD connector:  on-prem <=      - proxy to on-prem AD, manage all users in there
* simple AD:       N/A       AWS - AD-compatible, manage on AWS, no on-prem connection

== Organizations

 Root OU
 └─ management account
    └─ OUs + member accounts

create accounts + OU (organizational units) per BU(business unit)/env/project/...

OU aren't accounts, they just help structuring the hierarchy

=== SCP (Service Control Policies), restrictive by default

* whitelist/blacklist IAM actions at the OU/account level
* does not affect service-linked roles

---

move account to another organization:: delete from current, invite from 2nd

== IAM (Identity and Access Management)

a role is both an identity and a 'resource' => it needs a trust policy to define who can assume the role +
e.g lambdas have exe roles and resource-based policies (GUI bottom) which define the allowed callers

IMPORTANT: when you assume a role, you give up your original permissions!

an instance profile is a container for a single role that can be attached to an EC2 instance when launched

permission boundary (user, role, NOT group)

 ex: if boundary = allow s3:*           on *,
                   allow iam:CreateUser on * won't work

useful to restrict one specific user instead of a whole account with SCP

Service-Linked Role::
only a specific service can use this role vs a regular role which can be assumed by many services/users

principal - user, app, service +
condition - aws:SourceIP, aws:RequestedRegion, ec2:ResourceTag +
            `Bool` or `BoolIfExists` (MFA doesn't apply to all resources): `{"aws:MultiFactorAuthPresent": false}`

* `arn:aws:s3:::my-bucket`   => bucket level permission (e.g ListBucket)
* `arn:awn:s3:::my-bucket/*` => object level permission (e.g Get/PutObject)

== Resource Access Manager

avoid resource duplication: share resources with any account

VPC subnets

* share within organization only
* network is shared => access via private IPs (cross-account SGs can be referenced but not viewed)

== Encryption

=== KMS (Key Management Service)
- share passwords/credentials/certificates
- encryption at rest

data > 4KB => use envelope encryption +
access: MANDATORY key policy + optional IAM policy +
                  +- default (complete access to root + allows access with IAM policies)

* AWS services use symmetric AES-256 CMK (Customer Master Key) keys
  ** AWS managed (free)
  ** customer imported
  ** customer managed (create, enable/disable, rotate)
    *** automatic rotation: once a year - same key id, new backing key (keep old one)
    *** manual rotation: if greater frequency needed or CMK is asymmetric so not eligible for automatic rotation
       **** new key id, new backing key (keep old one)
       **** apps use the key id so we need an alias to the id in this case

* RSA, ECC (elliptic-curve cryptography) asymmetric keys are used for:
  - sign/verify integrity checks
  - outside of AWS (no access to KMS API)

*A deleted CMK is in the 'pending deletion' status and can be recovered for 7 - 30days (default)*

== SSM Parameter Store

secure storage for configuration and secrets, version tracking
[source,python]
----
ssm.get_parameters(Names=['/site/prod/db-url'], WithDecryption=True)
----

std vs advanced (TTL in parameters policies, more params of bigger size + higher throughput)

== AWS Secrets Manager (mostly for RDS MySQL, PostgreSQL, Aurora)

* rotation of secrets + new auto generation
* KMS encrypted

== Shield

managed DDoS (Distributed Denial of Service) protection service +
Route 53, CloudFront, Global Accelerator, ELB, EC2

== WAF (Web Application Firewall)

CloudFront, ALB, API Gateway

web ACLs:

* IP filtering
* http based rules (header, body, URI string)
* rate (DDoS) + geo-match rules
* SQL injection + XSS (cross-site scripting)

== AWS Firewall Manager

common set of security rules at the organization level:

- WAF             (CloudFront, ALB,     API Gateway)
- Shield advanced (CloudFront, ALB/CLB, Elastic IP )
- SG              (EC2 + ENI)

== GuardDuty (uses ML, anomaly detection, 3rd party data)

threat discovery (cryptocurrency attacks, malicious IPs):

* DNS logs
* VPC Flow logs
* CloudTrail mgmt events
* S3 data events

== Amazon inspector

 EC2 - agent     => OS vulnerabilities, CIS (center for internet security) benchmarks
       agentless => network accessibility

inspector service to send report via SNS

== Amazon Macie

ML + pattern matching to alert about exposed (e.g in S3) sensitive data (PII: Personally Identifiable Information)

== VPC (Virtual Private Cloud)

IGW + routing table for the public subnets = Internet connectivity

* 5 VPC per region (soft limit)
* 5 CIDR per VPC: min /28 (    16 IPs) +
                  max /16 (65 536 IPs)
* reserved addresses (e.g 10.0.0.0/24)
  - 10.0.0.0: network
  - 10.0.0.1: router
  - 10.0.0.2: DNS (or 169.254.169.253)
  - 10.0.0.3: future use
  - 10.0.0.255: broadcast (not supported in VPC!)

best practice:

           VPC /16 - 65 536
 public subnet /24 - 256 (we don't need too many hosts in a public subnet)
private subnet /20 - 4096

=== NACL (Network Access Control List) aka subnet firewall

* stateless: always needs in + out rules
* rules are evaluated from lowest to highest number, 1st one wins (low num = high precedence)
* good way of blocking a specific IP address at the subnet level
* default NACL => allow everything, new NACL => deny everything
* best practice: use increments of 100 to allow room for more rules

==== networking security

  SG - statefull, if one way is allowed then the return way is automatically allowed
NACL - stateless, both ways are always evaluated

services behing SGs::
EC2, ELB, EFS, RDS, ElastiCache

==== ephemeral ports

- clients connect to a defined port, and expect a response on an ephemeral port
- because NACL are stateless, we lose info about source port of incoming
  traffic, therefore outbound return traffic must go to all ephemeral ports:

                |NACL|                                    |NACL|
  allow TCP/3306      to   db subnet  -->>  allow TCP/3306      from web subnet
  allow TCP/ephemeral from db subnet  <<--  allow TCP/ephemeral to   web subnet

=== DNS support/resolution (true + route 53 by default)
it's best to have a DNS server within the VPC to avoid unnecessary network traffic

=== DNS hostnames (false by default, true for default VPC)
* needs enableDnsSupport=true
* if true => add public DNS for public instances

both needed for custom private DNS

DNS resolver

    AWS wants mitko.example.com? route 53 outbound endpoint: forward queries to on-prem
on-prem wants bla.amazonaws.com? route 53  inbound endpoint: on-prem resolvers can forward queries to us

=== objects in public subnets

- bastion host => connect to private EC2 instances
+
make it highly available:

* ssh is layer 4 => multi-AZ NLB - ASG 1:1:1
* bonus - thanks to the NLB, the bastion can be moved to the private subnet
+
- NAT (EC2) instance (deprecated)
  * disable source/destination check (can forward traffic)
  * must have elastic IP
  * private subnets to route via it

- NAT Gateway => Internet connectivity for private EC2 instances
  * elastic IP
  * single AZ (must create multiple NAT Gateways in multiple AZs for HA)
  * no SG to manage
  * 5 to 45Gbps auto-scaling bandwidth
  * can't be used as bastion host

[[NAT-T]]
A private host behind NAT "can't" be contacted, for that you need NAT traversal:
Also known as UDP encapsulation, it allows traffic to get to the specified
destination which doesn't have a public IP address. In a S2S VPN connection, a
CGW behind NAT needs NAT-T enabled

=== VPC Peering: VPCs must not have overlapping CIDRs

=== VPC Endpoints (AWS PrivateLink)

connect to AWS services privately (from within your private subnets) +
DNS support must be on, route tables will need amending,
no need for IGW or NATGW

* interface endpoints: ENI (private IP => SG)
* gateway endpoints (at no cost!): S3, DynamoDB

=== VPC Endpoint Services (PrivateLink)

expose your own services (not AWS ones as above) through a NLB (or GWLB),
then consumers can connect via ENI thanks to PrivateLink

=== VPC Flow Logs

troubleshoot SG & NACL issues

capture IP traffic

* VPC, subnets, ENI
* ELB, RDS, ElastiCache, Redshift, WorkSpaces, NATGW, Transit Gateway... (managed interfaces)

-> |NACL| -> SG EC2:

  inbound accept, outbound reject => NACL issue
 outbound accept,  inbound reject => NACL issue

*format*:

 ver | account | eni | src + dst IPs | src + dst ports | proto | packets | bytes | epoch start + end | action | status
                                                                                                       v   v
                                                                                                       SG, NACL - ACCEPT/REJECT

=== Traffic Mirroring

capture actual IP traffic for deeper inspection: `tcpdump` +
e.g from EC2 ENI to another ENI or NLB

=== EC2-Classic (deprecated, CLB is another classic resource)

pre-VPC era: instances ran in a single network shared with other customers.
to link those old instances to our VPC, we need ClassicLink

== Site-to-Site (S2S) VPN or Direct Connect (DX)

=== VPN
IPsec over the public Internet

=== Direct Connect (DX)
unencrypted private connection (add VPN between DX location and DC to have IPsec encryption)

WARNING: 1 month to setup connection

- dedicated (1Gbps and 10Gbps)
- hosted (capacity on-demand: 50Mbps, 500Mbps, 1, 2, 5 to 10Gbps)

    high resiliency: multiple DX locations - multiple DCs
 maximum resiliency: multiple DX locations - multiple DCs
                      separate connections - separate connections
                      per location           per DC

     virtual private gateway - VPN or DX
    /
 VGW                  <--> CGW (customer gateway)
                      <--> CGW (CloudHub hub-and-spoke model for multi-DCs intercommunication)
      [ DX location ] <--> customer router in DC
        AWS|customer

 VGW region 1 \
               <--> DX Gateway <-> DX <-> DC (direct connect for same region, direct connect gw cross-regions)
 VGW region 2 /

=== AWS side
* must enable route propagation so subnets know how to contact the VPN gateway
* VPN concentrator (device that helps to manage multiple VPN connections => VPN on a larger scale)
* allows for custom ASN (Autonomous System Number). Edge location???

=== on-premises
* enable <<NAT-T>> if behind NAT

== Transit Gateway (IP multicast)

transitive peering (traffic passes through) of multiple VPCs:

- peering
- VPNs
- DX Gateways

---

* share cross-account with RAM
* peer with other Transit Gateways across regions
* use route tables to limit communications

=== ECMP (equal-cost multi-path)

define multiple S2S VPN connections to increase the bandwidth of your connection to AWS

 1x VPN gw = 2 tunnels = 1.25Gbps
 2x                         5Gbps
 3x                       7.5Gbps

== IPv6

* IPv6 addresses are public and Internet-routable (no private range)
* egress-only IGW => same effect as a NAT gw but IPv6 are public so no NAT is needed
* 2001:db8::1234:5678 -> the middle 4 segments are zero
* IPv4 + IPv6 = dual-stack mode

== Networking Costs in AWS

free for ingress traffic, we pay only when exiting AWS network

*  free  with private IPs within AZ
* $0.01  with private IPs  cross AZ
* $0.02  with  public IPs  cross AZ/region
* $0.09        out to S3  Internet (cross-region)
* $0.085 CloudFront + S3  Internet (actually cheaper and S3 requests are 7x cheaper => way better than S3 alone)

== Disaster recovery

- RPO: Recovery Point Objective => minimize data loss +
       `*/!\ disaster /!\*`
- RTO: Recovery  Time Objective => minimize downtime

on-premises to AWS cloud examples:

 1 Backup and restore                                  - backup/restore from snapshots:  cheapest ->   high RPO + RTO
 2 Pilot light  (bare-core up in the cloud)            -                    DB replica:     cheap ->  lower RPO + RTO
 3 Warm standby (full min-size system up in the cloud) -        ELB + ASG + DB replica: expensive ->    low RPO + RTO
 4 Hot site     (full     size system up in the cloud) -        ELB + ASG + DB replica:    COSTLY -> lowest RPO + RTO
   multi site active-active approach

---

chaos:: test your prod setup (ref. Netflix simian-army)

== Migration to AWS

* VM import/export (VMs <-> EC2, or ami.iso to use on-premises)
* Migration Hub
* Application Discovery Service
* SMS (Server Migration Service) - live DMS-like migration
*    Database Migration Service (section below)

=== DMS (Database Migration Service)

 source -- EC2 with DMS -- destination
 +- all dbs                +- all dbs
 +- S3                     +- S3
                           +- ElasticSearch
                           +- Kinesis data streams

* for heterogeneous migrations (different db engines), SCT (Schema ConversionTool) is needed beforehand
* continuous data replication with CDC (Change Data Capture): source remains available

=== DataSync

move large amounts of data to AWS (can be used together with snow family). storage gateway is for moving data to S3 only.

                                       => S3
on-prem NAS (NFS/SMB) + DataSync agent => EFS
                                       => FSx for Windows file server

         EFS + EC2 with DataSync agent => EFS (AWS to AWS)

== AWS Backup

centralize AWS snapshots management:

* we need a plan (frequency + retention policy) and AWS services => it all goes to S3
* supports PITR (Point In Time Recovery), tag-based backups, ...

== HPC (High Performance Computing)

EC2 enhanced networking (SR-IOV): single root i/o virtualization:
single NIC to present itself as several virtual NICs

* ENA (elastic network adapter): higher PPS (Packets Per Second) 100Gbps - or legacy Intel 82599 VF for up to 10Gbps
* EFA (elastic  fabric adapter): enhanced ENA leveraging MPI (Message Passing Interface) +
                                                         +- bypasses the underlying Linux OS for lower latency

---

AWS Batch:: multi-node EC2/spot parallel jobs
AWS ParallelCluster:: open source cluster management tool for HPC

== CICD (Continuous Integration/Delivery) CodePipeline

find/fix bugs early, deploy often

*                   push to CodeCommit - GitHub
*           build & test in CodeBuild  - Jenkins CI (continuous integration)
* deploy passing build with CodeDeploy - Jenkins CD (continuous delivery: create packages)
* provision with CloudFormation and/or Ansible (actual deploy???)

== CloudFormation

IaC (Infrastructure as Code)

* YAML/JSON templates go in S3, deploy stack via cli
  - AWS resources
  - parameters: dynamic inputs
  - mappings:   static vars
  - outputs
  - conditionals
  - metadata
* figures out the right order of creation (declarative programming)
* estimate costs thanks to resource tags using the CloudFormation template
* dev env: save money by auto deleting 5pm / creating 8am templates
* auto diagrams

=== StackSets

Manage stacks across multiple accounts/regions with a single operation. +
Update a stackset to update all stack instances.

          EMR: Elastic MapReduce - manage Apache Hadoop/Spark clusters to process/analyze big data
     OpsWorks: managed Chef & Puppet (alternative to AWS SSM)
   WorkSpaces: VDI (Virtual Desktop Infrastructure), managed, secure cloud desktop (Linux/Windows)
      AppSync: store and sync data across mobile and web apps in real-time (uses GraphQL from Facebook)
Cost Explorer: Savings Plan alternative to Reserved Instances
   Transcribe: ASR (Automatic Speech Recognition) service => convert audio to text

== Well Architected Framework 5 Pillars

Operational excellence::
  IaC, anticipate failure, AWS Config, monitoring, CICD

Security::
  IAM, security at all levels, encryption, keep people away from data

Reliability (scalability + HA?)::
  stop guessing capacity => ASG, test/automate recovery

Performance efficiency (scalability?)::
  use serverless, stay up-to-date: AWS News Blog

Cost optimization::
  - Cost Explorer, spot instances
  - Trusted Advisor
      * cost optimization
      * performance
      * security
      * faulttolerance
      * service limits/quotas

=== Reference
* https://aws.amazon.com/architecture/reference-architecture-diagrams[Architecture Diagrams^]
* https://aws.amazon.com/architecture/[Architecture Examples and Best Practices^]
* https://aws.amazon.com/solutions/[Solutions Library^]

== Misc

1 CPU = multiple cores + multiple threads. vCPU is the total of threads.

=== http statefullness

_can be achieved with:_

* ELB stickiness (session/client affinity)
* cookies stored on EC2 instances or sent by user (web cookies)
* single session_id cookie sent by client, session info stored in ElastiCache

=== *not* serverless

_you have to provision the EC2 instance/node type:_

* RDS
* Aurora (can be)
* Redshift
* ElastiCache
* EMR

// vim: fdm=expr fde=getline(v\:lnum)=~'^=='?'>'.(len(matchstr(getline(v\:lnum),'===*'))-1)\:'='
----------------------- File -----------------------
help/AWS/awsume.md
```bash
# The completion needs the venv path in ~/.awsume/zsh-autocomplete/_awsume +
# autoload -z ~/.awsume/zsh-autocomplete/_awsume in zshrc
alias awsume-autocomplete=~/py-envs/awsume/bin/awsume-autocomplete
alias awsumepy=~/py-envs/awsume/bin/awsumepy
alias awsume=". ~/py-envs/awsume/bin/awsume"
```

`awsume -otemp` will output a token in `~/.aws/credentials`  
`credential.https://git-codecommit...helper=!aws --profile ImportantDev codecommit credential-helper $@` will be then able to use the temporary credentials

```cfg
~/.aws/config
[profile ImportantDev]
...
source_profile = temp
...
```
----------------------- File -----------------------
help/AWS/cli.adoc
:toc: left
:toc-title: AWS
:toclevels: 3

= AWS cli

== Access key import

=== prepend _user name_ to the `.csv` file

 User Name,xxx,xxx
 default,xxx,xxx

then: `aws configure import --csv *file://*path.csv`

== Aliases

https://github.com/awslabs/awscli-aliases

[source,bash]
----
~/.aws/cli/alias

aliases =
  !f() {
  grep '^\S\+\s*=' ~/.aws/cli/alias | grep -v aliases | cut -d= -f1
  }; f

w =
  !f() {
  aws --no-cli-pager iam list-account-aliases --query 'AccountAliases[*].{Alias: @}'
  aws --no-cli-pager sts get-caller-identity
  }; f

# Input: ec2 instance id
ip =
  !f() {
  aws --no-cli-pager ec2 describe-instances --query "Reservations[*].Instances[?InstanceId==\`$1\`].PrivateIpAddress" --output text
  }; f
----

== Tips

=== filter by own account
`$ aws ec2 describe-images --owners *self*`
----------------------- File -----------------------
help/AWS/cloudformation.adoc
yaml

InstanceId:
  Ref: MyEC2Instance

or short version

InstanceId: !Ref MyEC2Instance
----------------------- File -----------------------
help/Dockerfile.adoc
== A default command that can be replaced by any command

 CMD [ "zsh" ]

== An entry command that can't be replaced and needs arguments

*NB*: CMD will supply default arguments

 ENTRYPOINT [ "perl" ]
 CMD [ "-e", "print 'Perl script expected\n'" ]
----------------------- File -----------------------
help/LICENSE
GNU GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. {http://fsf.org/}
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU General Public License is a free, copyleft license for
software and other kinds of works.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
the GNU General Public License is intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.  We, the Free Software Foundation, use the
GNU General Public License for most of our software; it applies also to
any other work released this way by its authors.  You can apply it to
your programs, too.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  To protect your rights, we need to prevent others from denying you
these rights or asking you to surrender the rights.  Therefore, you have
certain responsibilities if you distribute copies of the software, or if
you modify it: responsibilities to respect the freedom of others.

  For example, if you distribute copies of such a program, whether
gratis or for a fee, you must pass on to the recipients the same
freedoms that you received.  You must make sure that they, too, receive
or can get the source code.  And you must show them these terms so they
know their rights.

  Developers that use the GNU GPL protect your rights with two steps:
(1) assert copyright on the software, and (2) offer you this License
giving you legal permission to copy, distribute and/or modify it.

  For the developers' and authors' protection, the GPL clearly explains
that there is no warranty for this free software.  For both users' and
authors' sake, the GPL requires that modified versions be marked as
changed, so that their problems will not be attributed erroneously to
authors of previous versions.

  Some devices are designed to deny users access to install or run
modified versions of the software inside them, although the manufacturer
can do so.  This is fundamentally incompatible with the aim of
protecting users' freedom to change the software.  The systematic
pattern of such abuse occurs in the area of products for individuals to
use, which is precisely where it is most unacceptable.  Therefore, we
have designed this version of the GPL to prohibit the practice for those
products.  If such problems arise substantially in other domains, we
stand ready to extend this provision to those domains in future versions
of the GPL, as needed to protect the freedom of users.

  Finally, every program is threatened constantly by software patents.
States should not allow patents to restrict development and use of
software on general-purpose computers, but in those that do, we wish to
avoid the special danger that patents applied to a free program could
make it effectively proprietary.  To prevent this, the GPL assures that
patents cannot be used to render the program non-free.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Use with the GNU Affero General Public License.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU Affero General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the special requirements of the GNU Affero General Public License,
section 13, concerning interaction through a network will apply to the
combination as such.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU General Public License from time to time.  Such new versions will
be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    {one line to give the program's name and a brief idea of what it does.}
    Copyright (C) {year}  {name of author}

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see {http://www.gnu.org/licenses/}.

Also add information on how to contact you by electronic and paper mail.

  If the program does terminal interaction, make it output a short
notice like this when it starts in an interactive mode:

    Manuals  Copyright (C) 2013  Dimitar Dimitrov
    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
    This is free software, and you are welcome to redistribute it
    under certain conditions; type `show c' for details.

The hypothetical commands `show w' and `show c' should show the appropriate
parts of the General Public License.  Of course, your program's commands
might be different; for a GUI interface, you would use an "about box".

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU GPL, see
{http://www.gnu.org/licenses/}.

  The GNU General Public License does not permit incorporating your program
into proprietary programs.  If your program is a subroutine library, you
may consider it more useful to permit linking proprietary applications with
the library.  If this is what you want to do, use the GNU Lesser General
Public License instead of this License.  But first, please read
{http://www.gnu.org/philosophy/why-not-lgpl.html}.
----------------------- File -----------------------
help/Makefile
# Rule
target: prerequisites (sub targets)
<tab>recipe # uses prerequisites to make the target

# substitute variables for the above
$@: $<
	@echo compiling...
	${CC} ...

.DEFAULT_GOAL := the 1st target, often called 'all'

.PHONY: all clean # not actual files, run regardless of mtime

-include Makefile # '-' for no errors

# this is a 1 line recipe thus we need the ;s
# $$ is used as $variables are valid in Makefiles
setup:
	@for d in *.db; \
         do \
            [[ -f $$d ]] && ln -sf "$${d%.db}" "$${d%.db}.in"; \
         done

Example

CC := gcc

hack: privileges.o intrusion.o
	linker ...

privileges.o: privileges.c
	${CC} ...

intrusion.o: intrusion.c
	${CC} ...

hack
    depends on
*.o
    which in turn depend on
*.c

privileges.c mtime has changed => rebuild
privileges.o => relink
hack
----------------------- File -----------------------
help/README.md
* [AWS](AWS.adoc)
* [Perl](perl/perl.md)
* [patch](patch.md)
* [disk usage](du.md)
* [logrotate](logrotate.txt)
* [SSH port forwarding](ssh/port-forwarding.txt)
* ...
----------------------- File -----------------------
help/SQL/join.md
```sql
SELECT ...
FROM Table1 JOIN Table2
ON id1=id2;
```

      (INNER) JOIN: records that have matching values in both tables
 LEFT (OUTER) JOIN: all from  left table, plus matched from right
RIGHT (OUTER) JOIN: all from right table, plus matched from  left
 FULL (OUTER) JOIN: all when there is a match in either
----------------------- File -----------------------
help/SQL/mysql.txt
For a verbose command line, use:
mysql -v db <<< 'your query; SELECT row_count();'

mysql.user: global privileges (*.*)
mysql.db: database privileges (db.tbl)
Note: for every user/password pair in mysql.db, an entry in mysql.user is
      created anyway in order to store the password

GRANT SELECT, SHOW VIEW ON db.* TO ''@'1.2.3.4' IDENTIFIED BY '';
REVOKE SELECT, SHOW VIEW ON db.* FROM ''@'1.2.3.4';

Problem:
We want to grant access to all existing users from an additional IP (8.8.8.8)

Solution:
#         db   tbl1 tbl2
mysqldump mysql db user --where='host = "1.2.3.4"' --no-create-info --compact | sed 's/1.2.3.4/8.8.8.8/g' | mysql mysql
flush privileges;

Storage Engines
---------------

MyISAM: table-level locking
        great for very low INSERT/UPDATE rate and a very high SELECT rate

InnoDB: row-level locking
        parallel INSERT/UPDATE/DELETE queries
        data integrity trough foreign key functionality
        caching for data and indexes in memory (servers with lots of RAM), as well as on disk

Transactions
------------

START TRANSACTION -- or BEGIN WORK
update tableX...
insert values...
SAVEPOINT tableXupdated;
update tableY
insert...
COMMIT -- or ROLLBACK, or ROLLBACK TO tableAupdated;

Note 1: A transaction will never be complete unless each individual operation
        within the group is successful

Note 2: Transactions can't be used on ALTER, DROP, RENAME, TRUNCATE...

Use backticks to escape special characters such as a dash:
drop database `wp-database`;
----------------------- File -----------------------
help/SQL/postgresql.md
# psql

## set pager
`\setenv PAGER less`

### disable line wrapping
`-S` to chop lines in `less`

## check database activity
```sql
SELECT * FROM pg_stat_activity;
```
----------------------- File -----------------------
help/SQL/sqlite.md
```sh
cd ~/.mozilla/firefox/g3iwn869.default
sqlite3 places.sqlite
```

```sql
sqlite> .databases
sqlite> .tables
sqlite> .schema moz_places
sqlite> SELECT url FROM moz_places WHERE url LIKE '%dailymotion%broca%';
sqlite> SELECT url FROM moz_places WHERE url LIKE '%ulozto%medved%';
```
----------------------- File -----------------------
help/ads.txt
DAI
dynamic ad insertion

HLS
HTTP Live Streaming

MPEG-DASH
Motion Pictures Expert Group Dynamic Adaptive Streaming over HTTP

VOD
video on demand

PVR
personal video recorder

VMAP
Video Multiple Ads Playlist

VAST
Video Ad Serving Template

transcode
e.g convert from video analogue signal to digital (MPEG)
----------------------- File -----------------------
help/ansible.txt
Ansible works by connecting to your nodes and pushing out small programs,
called "Ansible modules" to them. Ansible then executes these modules, and
removes them when finished.

ansible hosts -mmodule -aarguments options
                  \
                   command (default)
                   shell if you need |s and >s

The command and shell modules are the only modules that just take a list of
arguments and don’t use the key=value form.

hosts can be: qa1, *.example.com, group1:group2, ~regex...

Inventory:
/etc/ansible/hosts
[qa]
qa[1:5] ansible_user=mitko # host variable

[qa:vars] # group variables

[new_group:children]
qa
other_child

Variables outside of the inventory in YAML files:
/etc/ansible/group_vars/group1
/etc/ansible/group_vars/group2
/etc/ansible/host_vars/host1

Also with subdirs:
/etc/ansible/group_vars/group1/db_settings
/etc/ansible/group_vars/group1/cluster_settings

Useful examples:
ansible <hosts> -m copy -a 'src=/etc/hosts dest=/tmp/hosts'
ansible <hosts> -m yum -a 'name=vim-enhanced state=latest'
ansible <hosts> -m service -a 'name=httpd state=restarted'

Facts:
ansible <hosts> -m setup

Playbooks:
ansible-playbook playbook.yml options

---
- hosts: webservers # play 1
  remote_user: root

  tasks: # modules (idempotent)
  - name: ensure apache is at the latest version
    yum: name=httpd state=latest # action: yum name=httpd state=latest is deprecated
    # or the equivalent:
    # yum:
    #   name: httpd
    #   state: latest
  - name: write the apache config file
    template: src=/srv/httpd.j2 dest=/etc/httpd.conf
    notify: # only if /etc/httpd.conf above has changed
      - restart memcached # handler name
      - restart apache
    # or notify: "restart web services"

  handlers:
    - name: restart memcached
      service: name=memcached state=restarted
      listen: "restart web services"
    - name: restart apache
      service: name=apache state=restarted
      listen: "restart web services"

- hosts: databases  # play 2
  remote_user: root

  tasks:
  - name: ensure postgresql is at the latest version
    yum: name=postgresql state=latest
  - name: ensure that postgresql is started
    service: name=postgresql state=started

Roles

roles let you automatically load related vars, files, tasks, handlers, and other Ansible artifacts based on a known file structure.
After you group your content in roles, you can easily reuse them and share them with other users.

roles are just automation around ‘include’ directives

ex: auto include roles/*/tasks/main.yml (equiv to init.pp in puppet land)
    vs - include: tasks/foo.yml

ex 2:
---
- hosts: webservers
  roles:
    - common
    - webservers

Autoload structure:
roles/
  webservers/
    tasks/
    handlers/
    library/
    files/
    templates/
    vars/
    defaults/
    meta/

~/.ansible.cfg:
[defaults]
forks = 50

[ssh_connection]
# ProxyCommand related:
# fix the unreachable issue related to the sftp subsystem not being enabled
scp_if_ssh = True

jinja2

tests:
* filter - variable | filter
*   test - variable is test_name # match/search/regex/truthy/falsy/all/any/directory/file/exists/failed/...

ex:
ansible localhost -m debug -a "msg={{ 'hello.world'|replace('.world','') }}"
----------------------- File -----------------------
help/apache/apache.txt
Apache
------

configuration files:
* httpd.conf
*   -> Include directives
* .htaccess (if no access to httpd.conf)

configuration:
<section>
  directives
</section>

http://www.example.com/work/ - serve index.html
                      |
                      +- DocumentRoot (/var/www/html)

Virtual hosts:

1. multiple hosts resolve to the same IP, both vhost sections match, differentiate on servername
   www.example.com -> <VirtualHost *:80> ServerName www.example.com
   hi.travel.com   -> <VirtualHost *:80> ServerName hi.travel.com

2. with ssl, not using sni, the Host header isn't available for matching against a ServerName. In that case we could set it for further processing:
   www.example.com -> <VirtualHost 1.1.1.1:443> RequestHeader set Host www.example.com
   hi.travel.com   -> <VirtualHost 2.2.2.2:443> RequestHeader set Host hi.travel.com

For a single domain with alternative names, a multi domain certificate could be used instead of SNI.

Location:
<Location "/private">  -> match /private, /private/, /private/text, ...
<Location "/private/"> -> match           /private/, /private/text, ...

In <Directory>, rewrite rules match against a relative path, the dir one + slash being stripped, so use:
RewriteRule "^$" "/moodle" # vs / -> /moodle

Modules {{{1

Modules can be statically compiled into the httpd binary when the server is
built. Alternatively, modules can be compiled as Dynamic Shared Objects (DSOs)
that exist separately from the main httpd binary file. You must use a
LoadModule directive in httpd.conf to tell Apache to activate a module.

Options {{{1

The Options directive controls which server features are available in a
particular directory

Parent:
<Directory /web/docs>
Options Indexes FollowSymLinks
</Directory>

Not merging with parent:
# No -/+ => only Includes will be set for /web/docs/spec
<Directory /web/docs/spec>
Options Includes
</Directory>

Merging with parent:
# FollowSymLinks and Includes are set for /web/docs/spec
<Directory /web/docs/spec>
Options +Includes -Indexes
</Directory>

Alias {{{1

Alias /docs /var/web (server config, virtual host)
http://www.example.com/docs/dir/file.html will be served from /var/web/dir/file.html
                                                           vs /DocumentRoot/docs/dir/file.html

This is to allow web access to parts of the filesystem that are not strictly
underneath the DocumentRoot.
Alternative: use symlinks, but directory options must include FollowSymLinks

Virtual Hosts {{{1

IP-Based Virtual Host:
The physical server should have as many NICs as served websites

Name-Based Virtual Host:
Apache looks for the hostname in the HTTP header, and depending on the
hostname, it serves different websites. You need only one ip-address on the
physical server; but, you update the DNS with multiple website names pointing
to the same ip-address.

IP on which the server will receive requests for the name-based virtual hosts:
NameVirtualHost 111.22.33.44:80 - listen on this NIC
NameVirtualHost *:80            - listen on all NICs (not reliable!?)

<VirtualHost *:80>
ServerName server.domain.com   # the vhost name
ServerAlias server2.domain.com # an alias for that name (analogue to /etc/hosts)
                               # CNAME set in DNS must exist
</VirtualHost>

Note on Name-Based Virtual Hosts and SSL:
It is impossible to host more than one SSL virtual host on the same IP address
and port. This is because Apache needs to know the name of the host in order to
choose the correct certificate to setup the encryption layer. But the name of
the host being requested is contained only in the HTTP request headers, which
are part of the encrypted content. It is therefore not available until after the
encryption is already negotiated. This means that the correct certificate cannot
be selected, and clients will receive certificate mismatch warnings and be
vulnerable to man-in-the-middle attacks.

Allow, Deny {{{1

mod_authz_host

All Allow and Deny directives are processed, unlike a typical firewall, where
only the first match is used. The last match is effective (also unlike a
typical firewall).

Match                   | Allow,Deny result                   | Deny,Allow result
------------------------+-------------------------------------+-------------------------------------
Match Allow only        | Request allowed                     | Request allowed
Match Deny only         | Request denied                      | Request denied
No match                | Default to second directive: Denied | Default to second directive: Allowed
Match both Allow & Deny | Final match controls: Denied        | Final match controls: Allowed

<Directory /www>
Order Allow,Deny (A,D means D is the default; D,A means A is)
Allow from apache.org
Deny from foo.apache.org
</Directory>

.htaccess {{{1

* Any directive that you can include in a .htaccess file is better set in a
  Directory block (in httpd.conf), as it will have the same effect with better
  performance.

* The .htaccess file is placed in a particular document directory, and the
  directives apply to that directory, and all subdirectories thereof.

* What you can put in these files is determined by the AllowOverride directive.

Proxy {{{1

A proxy server acts on behalf of another machine - client or server

Forward proxy - ProxyRequests
-----------------------------

A forward proxy provides proxy services to clients

client1                                  +--------+
client2 --> FORWARD PROXY -- firewall -- |internet| -- target server
clientn                                  +--------+

From the point of view of the server, it is the proxy server that issued the
request, not the client. Depending on the forward proxy's settings, a request
can be allowed or denied. The proxy can serve as a single point of access and
control. It is primarily aimed at enforcing security on client computers in your
internal network.

Notes:
* the client must be specially configured to use the forward proxy
* a typical usage of a forward proxy is to provide Internet access to internal
  clients that are otherwise restricted by a firewall.
* because forward proxies allow clients to access arbitrary sites and to hide
  their true origin, it is essential that only authorized clients can access the
  proxy before activating it.

Reverse proxy (gateway) - ProxyPass
-----------------------------------

A reverse proxy accepts requests from external clients on behalf of servers
stationed behind it.

           +--------+                                 server1
client --> |internet| -- firewall -- REVERSE PROXY -- server2
           +--------+                                 servern

To the client in our example, it is the reverse proxy that is providing
services. Thus, a reverse proxy hides the identities of servers, whereas a
forward proxy hides the identities of clients. Just like forward proxy servers,
reverse proxies also provide a single point of access and control.

Reverse proxies can be used for:
* load balancing
* caching for a slower back-end server
* bring several servers into the same URL space
----------------------- File -----------------------
help/apache/mod_authz.txt
mod_authz_host

Require (not) ip ...
Require (not) host domain

mod_authz_core

Containers

<RequireAll>  : all directives must succeed
<RequireAny>  : 1 or more directives must succeed
<RequireNone> : all directives must fail

Directives

Require env ...
Require all granted|denied : grant or deny all requests
Require method GET POST ...
Require expr ...
----------------------- File -----------------------
help/apt.md
# Search with regex within names only (`man apt-cache`)
```bash
apt search \^exa -n # --names-only
```

# Info about remote package
```bash
apt show emacs
dpkg --no-pager -l emacs # check if installed

yum info vim-enhanced
```
----------------------- File -----------------------
help/asterisk.txt
asterisk -r # connect to cli

core show channels      # check calls
core restart gracefully
----------------------- File -----------------------
help/audio.txt
Stereophonic sound creates an illusion of directionality and audible
perspective. Stereo uses an array of microphones (for true stereo) to capture
sound (1 channel per micro) and two or more loudspeakers are necessary in order
to recreate it. Simulated stereo is when a mono signal is manipulated by
software to create delays and changes in amplitude.
The reason we use stereo is because we have two ears thus we perceive sounds
with delay and pressure difference in each ear. Mono uses a single audio
channel.
----------------------- File -----------------------
help/audit.txt
                                  Audit - RHEL

auditctl -l # list defined rules
auditctl -s # check status

Examples of FS rules

Watch for writes (w, a - attributes) to a file:
auditctl -w /etc/passwd -p wa -k passwd_changes # random tag (key) definition

Log the execution of the insmod command:
auditctl -w /sbin/insmod -p x -k module_insertion

Permanent rules

/etc/audit/audit.rules:
-w /etc/passwd -p wa -k passwd_changes # same as above

Read rules from a file:
auditctl -R /usr/share/doc/audit-version/stig.rules

By default, the Audit system stores log entries in the /var/log/audit/audit.log

ausearch -i (human readable)
----------------------- File -----------------------
help/augeas.txt
[ ] is for creating lenses (tree nodes)

del /:\s+/ ": " means that there might be several spaces but we want only one when reconstructing?

file.aug:16.56-.71 exception on line 16, col 56 to 71 (the line is omitted since it remains the same)

let lens is forbidden, use let lns instead

No ~ in filter! Only absolute paths

Don't try to not store

a label doesn't consume text from the concrete view. So if you expect a lens to produce { "<<" = "production" }, you need to use [ key "<<" ... ]

labels are only produced in the abstract view

the tree is composed of nodes
each node can have a label (a), an optional value (b), and optional children (c)

[ key "a" . store "b" . [ key "c" ] ] will parse "abc" as { "a" = "b" { "c" } }

Example:
augtool -I . ls /files/path/to/file.yml/host1/

augtool -I . -At "YAML incl '/home/xxx/bla.yml'" print '/augeas//error'
or
augtool -I . -At "YAML incl '/home/xxx/bla.yml'" errors

note:
if running in non-interactive mode, you won't be able to access the errors as kept in RAM.
Instead:
augtool> your_command
augtool> errors
----------------------- File -----------------------
help/awk.adoc
== print all but the first column

 history | awk '{$1="";print}'
 history | awk '{$1=$2="";print}' # bar first two columns

* `history | perl -lane 'print "@F[1..$#F]"'` works but it's too complex
* `cut -d' ' -f2-` won't work as the delimiter must be a single char
----------------------- File -----------------------
help/bash.txt
|| vs if/else pitfall
---------------------

touch /tmp/brr && ech 'created' || echo 'fail'

Here the touch succedes so we only want ech 'created' to be executed
but since that is wrong we also get the second echo

compare with:
if touch /tmp/brr
then
  ech 'created'
else
  echo 'fail'
fi

Quotes
------

Q. In FAQ096, why is this ok? ssh localhost '~/bin/args make CFLAGS="-g -O"'
   Why do we end up with 2 args and not 3 because of remote word splitting?
A. it gets parsed by the local shell, then the remote shell
   it'll run                             "$SHELL" -c '~/bin/args make CFLAGS="-g -O"' on the remote end
   if you omit the '' quotes, it'll run  "$SHELL" -c '/home/localhomedir/bin/args make CFLAGS=-g -O'

nesting single quotes:
   ssh 'grep -iE '\''blah'\'' file'
1. ssh 'grep -iE '  'blah'  ' file' -> break it in 3 sets of quotes
2.                \''blah'\'        -> use literal 's that bash won't parse
   ssh grep -iE 'blah' file         -> our remote command

Completion
----------

complete -d -Fmyfunc cd: cd will use myfunc and dir completion
compgen -d m: generate completion list of dirs    named m... (<=> -Adirectory)
compgen -a m: generate completion list of aliases named m...
compgen must put the possible completions in the COMPREPLY array
COMP_WORDS: command line args during completion - [ cd m ]<tab>
COMP_CWORD: index in COMP_WORDS of the word containing the cursor

Misc
----

parent shell PID: $$, current subshell: BASHPID

shell:

[ x"$var" = xyes ] && ... # the x at the front prevents a leading dash from being picked up as an option to test (-n, -z)

# Use PROMPT_COMMAND to aggregate users' history into a single file
# /var/log/user-history.log                                  whoami | bash PID |         history 1        |  $?
#                                                            oge    | [21118]: | 2013-09-09_10:46:34 su - | [1]
export PROMPT_COMMAND='RETRN_VAL=$?; logger -p local6.debug "$LOGNAME [$$]: $(history 1 | command sed "s/^[ ]*[0-9]\+[ ]*//" ) [$RETRN_VAL]"; ...'

source aka .
tells Bash to read the commands in myscript and run them in the current shell
environment. Since the commands are run in the current shell, they can change
the current shell's variables, working directory, open file descriptors,
functions, etc.
----------------------- File -----------------------
help/bonding.txt
# slave
HWADDR=18:66:da:b1:a6:6d
TYPE=Ethernet
DEVICE=em1
NAME="bond0 slave 1"
IPV4_FAILURE_FATAL=no
ONBOOT=yes
BOOTPROTO=none
MASTER=bond0
SLAVE=yes

# bond
TYPE=Bond
DEVICE=bond0
NAME=bond0
IPADDR=192.168.99.22
GATEWAY=192.168.99.1
PREFIX=24
IPV4_FAILURE_FATAL=no
ONBOOT=yes
BOOTPROTO=none
BONDING_OPTS="mode=5 miimon=100"
BONDING_MASTER=yes

grep Status /proc/net/bonding/bond0
----------------------- File -----------------------
help/brew.txt
$ brew search
$ brew info
$ brew ls

Remove including dependencies
$ brew rmtree

Free space
$ brew cleanup

Diagnose problems
$ brew doctor

Fix issues
$ brew update-reset
----------------------- File -----------------------
help/bsd/carp.txt
Common Address Redundancy Protocol
----------------------------------

  common address: CARP, VRRP
     @  |  /
      \ | /
 reverse proxy: relayd, nginx
      / | \
     @  @  @
web 1   2   3 servers

similar to VRRP and Cisco's HSRP

create a group of redundant firewalls (failover):

Allow multiple hosts on the same network segment to share a virtual/floating
IP. This is not loadbalanced. One host (the master) only is active and
broadcasts it's status to all hosts in the redundancy group. If it fails, a
backup host from the group takes over.

Change master role to backup:
ifconfig -g carp carpdemote 50

Down:
ifconfig carp0 destroy

Up:
sh /etc/netstart carp0
----------------------- File -----------------------
help/bsd/packages.md
# Select a mirror

`export PKG_PATH=ftp://ftp.icm.edu.pl/pub/OpenBSD/4.8/packages/$(uname -m)/`

or:

## 6.0 and below

```
/etc/pkg.conf
installpath = https://ftp.icm.edu.pl/pub/OpenBSD/6.0/packages/amd64/
or
installpath = ftp://ftp.icm.edu.pl/pub/OpenBSD/4.8/packages/i386/
```

## 6.1 and later

```
/etc/installurl
https://cdn.openbsd.org/pub/OpenBSD
```

# Search
`pkg_info -Q screen`

# Install
`pkg_add screen`
----------------------- File -----------------------
help/bsd/pf.txt
Packets are evaluated against all (unless using 'quick') filter rules, the last
one winning. Unmatched packets pass by default.

Simplified filter rule syntax:
pass/block, in/out, [log|quick], on eth0, inet(6), proto tcp, from A port 22, to B port 22, flags, state

NAT
===

match     on ext-nic from 10.0.0.40 to any     binat-to 88.0.0.40 # priv <-> Internet (bidirectional)
-----------------------------------------------------------------
match out on ext-nic from 10.0.0.40 to any       nat-to 88.0.0.40 # priv -> Internet (uni)
match  in on ext-nic from any       to 88.0.0.40 rdr-to 10.0.0.40 # Internet -> priv (uni)

match  in on int-nic from int-net   to 88.0.0.40 rdr-to 10.0.0.40 # priv -> priv

Filter
======

# 1. Internet <-> priv IP rules makes sense for filtering since we've defined
#    how to talk to the private IPs in the NAT rules
#
# 2. pass in doesn't mean being contacted from Internet. It just means packets
#    entering the interface, potentially coming from internal sources


# Firewall with several external NICs, gateways to different ISPs
pass in on ext-nic from any to 10.0.0.40 port 443 reply-to (ext-nic gw2) # route back trough gw2

test config:
pfctl -nf /etc/pf.conf

show rules:
pfctl -sr

pfctl -Tshow -t{table_name}
----------------------- File -----------------------
help/bsd/relayd.txt
NAT (+port redirection) and loadbalance to internal IPs

test config:
relayd -nf /etc/relayd.conf

start:
rcctl start relayd

relayctl show summary

relayd LBs, then pf passes...
----------------------- File -----------------------
help/c.txt
Arrays

name is the addr of the first elem
this addr is immutable, unlike pointers

name <=> &name[0]
name[n] <=> *(name + n); // dereference the addr pointed by name + n
----------------------- File -----------------------
help/capistrano.txt
                                   Capisrano

Summary:
   - define servers + repos in deploy.rb
   - cap deploy (ssh to server, clone app or pull)

Details:

cd /location/from/where/you/want/to/deploy

capify .
       └→ Capfile
       └→ config/deploy.rb - deployment recipes # <=> project name click in Webistrano (Project configuration)

          require 'capistrano/ext/multistage' # for production.rb...
          set :stages, ["staging", "production"]
          set :default_stage, "staging"

          set :application, "fancy_shoes"
          set :repository, "git@account.git.beanstalkapp.com:/account/repository.git"
          ---------------------------------------------------------------------------
          set :user, "server-user-name"
          set :deploy_via, :remote_cache # use clone once to the server then use git pulls

          namespace :deploy do
            # default
            # task :restart, :roles => :web do
            #   run "touch #{ current_path }/tmp/restart.txt"
            # end

            desc "Restarting all daemons!"
            task :restart_daemons, :roles => :app do
              sudo "monit restart all -g daemons"
            end
          end

          after "deploy", "deploy:restart_daemons"
          # after "deploy", "deploy:restart" # built into Capistrano so not explicitly needed

          config/production.rb
          set :branch, 'master'

          config/staging.rb
          set :branch, 'staging'
          server "my_fancy_server.com", :app, :web, :db, :primary => true # 3 roles for that server
          set :deploy_to, "/var/www/fancy_shoes"
          --------------------------------------

cap deploy:setup # look for equivalent link in Webistrano
       └→ ssh to server (deploy_to) and create a special directory structure

cap deploy:check

cap production deploy
cap deploy # default
----------------------- File -----------------------
help/certificates/gpg.txt
1. Generate key pair
--------------------
gpg --gen-key
   * length   : 2048
   * validity : 1y
   * comment  : nickname - work key

2. Make key publicly visible
----------------------------
gpg --keyserver pgp.mit.edu --send-keys 48867B54
gpg --keyserver pgp.mit.edu --search-keys kurkale6ka # or check online: http://pgp.mit.edu/pks/lookup?search=kurkale6ka&op=vindex

3. Use a web of trust (as opposed to CA in PKI world)
-----------------------------------------------------
# Validate a key:
# Your correspondent:
gpg --fingerprint 160C6FA8

# You:
gpg --edit-key 160C6FA8
gpg> fpr # to view its fingerprint
gpg> sign # if fingerprints match
gpg> check

4. Setup Thunderbird / sparrow / mutt to use gpg
------------------------------------------------

Tips:
-----

# List key IDs on one's public keyring as used above
gpg --list-keys

# Encrypt a file with multiple recipients
gpg -ae -r 48867B54 -r 160C6FA8 -o aliases.test.txt aliases.test

# Manually export / import a key
# This is achieved in one go with --send-keys / --search-keys
* gpg -a -o pubkey.txt --export 'Dimitar Dimitrov'
* send it to your correspondant
* He can import it with: gpg --import pubkey.txt
----------------------- File -----------------------
help/certificates/ldap.txt
OpenSSL: certificate generation for OpenLDAP
============================================

Set openssl defaults:
---------------------
/etc/pki/tls/openssl.cnf (OPENSSLDIR: openssl version -d)

[ req ]
default_bits = 2048
req_extensions = v3_req

# Share the same certificate among several servers
[ v3_req ]
subjectAltName=DNS:vm_centos1,DNS:vm_centos2
NB: DNS:host must be a fqdn as returned by hostname -f

CA self-signed certificate:
---------------------------
1. key pair generation
   cd /etc/openldap/cacerts &&
   openssl genrsa -out cakey.pem 2048
   chmod 600 cakey.pem
   Note: cakey.pem contains BOTH keys, same for serverkey.pem below

2. new self-signed ca certificate
   openssl req -new -x509 -days 1095 -key cakey.pem -out cacert.pem # -subj "/C=GB/ST=State/L=Locality/O=Org/OU=IT/CN=CA"

LDAP Master certificate:
------------------------
1. mkdir -p /etc/CA/{newcerts,private}
   touch /etc/CA/index.txt
   echo '01' > /etc/CA/serial ?
   (cd /etc/CA/private && ln -s /etc/openldap/cacerts/cakey.pem)
   (cd /etc/CA && ln -s /etc/openldap/cacerts/cacert.pem)

2. key pair generation + new CSR
   openssl req -new -newkey rsa:4096 -nodes -keyout serverkey.pem -out server.csr # -subj "/C=GB/ST=State/L=Locality/O=Org/OU=IT/CN=ldap_master_hostname_fqdn"
   chmod 600 serverkey.pem

3. Sign the CSR in order to create a valid digital certificate
   openssl ca -days 1095 -extensions v3_req -in server.csr -out servercert.pem

4. chown ldap:ldap *

* Install cacert.pem, servercert.pem and serverkey.pem on the slave,
          cacert.pem on all clients

* /etc/openldap/cacerts must also contain a set of symlinks pointing to the
  certificate files, each named using a hash with extension .0 of the
  corresponding certificate's subject's Distinguished Name:

  for c in *cert.pem; do  ln -s "$c" $(openssl x509 -hash -in "$c" -noout).0; done

  http://jw35.blogspot.co.uk/2010/05/doing-certificate-verification-in.html
  man openssl-verify

Location with a bundle of certificates trusted by default:
/etc/pki/tls/certs on RedHat/Fedora
/etc/ssl/certs     on Ubuntu/Debian

CRL: certificate revocation list - permanent (revoked) or temporary (hold)
----------------------- File -----------------------
help/certificates/openssl.txt
http://users.dcc.uchile.cl/~pcamacho/tutorial/crypto/openssl/openssl_intro.html#htoc6

Key pair generation (gendsa if relevant)
----------------------------------------
openssl genrsa -out keys.pem 2048         # generate a public/private key pair
openssl genrsa -out keys.pem -aes256 2048 # with passphrase

Key pair processing
-------------------
openssl rsa -in keys.pem 2>/dev/null                # print the private key
openssl rsa -in keys.pem -pubout 2>/dev/null        # print the public key
openssl rsa -in keys.pem -text -noout               # print out the components of a key pair
openssl rsa -in keys.pem -outform DER -out keys.der # convert from PEM to DER format
openssl rsa -in keys.pem -out keys2.pem             # remove the pass phrase

CSR: Certificate Signing Request
--------------------------------
1. ssh to apache server, cd /etc/httpd/conf/certs

# check the csr
openssl req -in server.csr -noout -subject

1.a with key pair generation

openssl req -nodes -newkey rsa:2048 -keyout myserver.key -out server.csr
             +-- no DES passphrase

1.b without key pair generation

openssl req -new -key myserver.key -out server.csr

Self signed certificate (see: PKCS#10)
--------------------------------------
openssl req -new -x509 -days 1095 -key ca_keys.pem -out ca_cert.pem -subj '/C=GB/ST=State/L=London/O=Company/OU=IT/CN=FQDN/emailAddress=@'

Certificate creation by the CA (Certificate Authority)
------------------------------------------------------
openssl ca -days 1095 -extensions v3_req -in host.csr -out host_cert.pem # create a valid digital certificate by signing a CSR

Certificate display
-------------------
openssl x509 -in host_cert.pem                     # see certificate
openssl x509 -in host_cert.pem -noout -dates       # check validity period
openssl x509 -in host_cert.pem -noout -subject
openssl x509 -in host_cert.pem -noout -fingerprint
openssl x509 -in host_cert.pem -noout -pubkey
openssl x509 -in host_cert.pem -noout -serial

Notes:
* -noout: no private key or x509 certificate output
* -text: key pair components
* pem: privacy enhanced email
* man genrsa|rsa|req|ca|x509
* openssl enc    -aes-256-cbc -iter 100000 -in plain.txt -out encrypted.bin
  openssl enc -d -aes-256-cbc -iter 100000 -in encrypted.bin

* des < des3 < aes128 # data encryption standard < advanced encryption standard
               aes192
               aes256
----------------------- File -----------------------
help/certificates/pki.txt
                                      PKI
                           Public Key Infrastructure
                           =========================

Public Key -> CA (sign key) -> digital certificate (PK ownership garanteed)

Note about ssh:
server cert ok => server is who it claims to be -> put PK in ~/.known_hosts

Note: public key certificate == digital certificate

CA - certificate authority:
---------------------------
Issues digital certificates certifying the ownership of a public key
It uses a digital signature to bind a public key with an identity

Ex: AAAAB3NzaC1yc2EAAAADA + mitko is RELIABLE

Note: a CA is identified by its own self-signed certificate

Note: The CA certificates of many well known and trusted CA companies already
      come installed on your PC as part of your Web browser installation. So if
      one of these CAs has signed the certificate presented by a web server, you
      can trust it to encrypt your communication (ex: payment via web browser)

Scenario: User sends CSR (certificate signing request) to CA, then CA creates
          the digital certificate

Digital signature:
------------------
In public-key systems integrity is guaranteed by using digital signatures. A
digital signature is a message digest which is attached to a message and which
can be used to find out if the message was tampered with during the
conversation. Even the slightest change in the message produces a different
digest. The message digest is encrypted using the sender's private key. The
receiver uses the same message digest algorithm used by the sender to generate
a message digest of the received message.

http://gdp.globus.org/gt4-tutorial/multiplehtml/ch09s03.html

Asymmetric key algorithms: the key used to encrypt a message is not the same as
                           the key used to decrypt it
----------------------- File -----------------------
help/certificates/server.txt
                           server certificate change

1. hostname -f must match the certificate's subject

* NB: hostname    uses static info (ex: from /etc/sysconfig/network)
      hostname -f uses DNS so you must be able to resolve the IP based on the static value.
                  you will need to change bind/AWS/... zones

* For info I used these change domain for bla from bad_example.com to example.com:
  cli53 rd bad_example.com bla A
  cli53 rc example.com 'bla 600 A 11.10.10.11'

2. rsync your certificate, key and CA and amend in apache:
   SSLCertificateFile
   SSLCertificateKeyFile
   SSLCACertificateFile # not sure this is actually needed

3. test with:
   /usr/local/opt/openssl/bin/openssl s_client -connect bla.example.com:443
   depth=3 C = SE, O = AddTrust AB, OU = AddTrust External TTP Network, CN = AddTrust External CA Root
   depth=2 C = GB, ST = Greater Manchester, L = Salford, O = COMODO CA Limited, CN = COMODO RSA Certification Authority
   depth=1 C = GB, ST = Greater Manchester, L = Salford, O = COMODO CA Limited, CN = COMODO RSA Domain Validation Secure Server CA
   depth=0 OU = Domain Control Validated, OU = PositiveSSL Wildcard, CN = *.example.com
   verify return:1

test.chn:
/usr/local/opt/openssl/bin/openssl s_client -showcerts -connect www.example.com:443 </dev/null 2>/dev/null | sed -n '/-----BEGIN/,/-----END/p' > /tmp/test.chn

cat .crt .ca.crt > .chn

* ^ certificate chain from end user, trough intermediate certificates to root
    certificate available in the trust store of your device

* NB: On a Mac I've used /usr/local/opt/openssl/bin/openssl since
      /usr/bin/openssl isn't aware of the correct location for the root certificates
      and reports them as untrusted

* there might still be a 'not fully secure' message from the browser but that
  could be parts of the site such as images still being served over http. check in firefox

Misc:

# wildcard/star certificate
openssl x509 -in example.com.crt -noout -subject
subject= /OU=Domain Control Validated/OU=PositiveSSL Wildcard/CN=*.example.com

# Issuer
openssl x509 -in example.com.crt -noout -issuer
issuer= /C=GB/ST=Greater Manchester/L=Salford/O=COMODO CA Limited/CN=COMODO RSA Domain Validation Secure Server CA
----------------------- File -----------------------
help/chkconfig.txt
chkconfig --list mysqld

# chkconfig --level 2345 mysqld on
chkconfig mysqld on
----------------------- File -----------------------
help/chmod.txt
File exe bit: X
---------------

chmod -R o=X /folder

* d........x set x on all dirs
* f..x..x--x set x on exe files
      \  \
       `--`--> at least 1 x bit already on
  f......--- else: same as o=

Ex: make sure no file has x for others but all dirs do
chmod -R o-x,o=X /folder

Make others same as group
-------------------------

chmod o=g /folder
----------------------- File -----------------------
help/chroot.txt
copy exes and their libraries to the chroot
----------------------- File -----------------------
help/cidr.txt
classless inter-domain routing

VLSM (variable-length subnet masking): any /prefix can be used

       Network           min           max          ips

Private
A:    10.0.0.0/8  [    10.0.0.0 - 10.255.255.255  ] 16,777,216 <= 2^24 (32-8)
B:  172.16.0.0/12 [  172.16.0.0 - 172.31.255.255  ] 1,048,576  <= 2^20
C: 192.168.0.0/16 [ 192.168.0.0 - 192.168.255.255 ] 65,536     <= 2^16

Link-local (auto-assigned private IPs)
?: 169.254.0.0/16 [ 169.254.0.0 - 169.254.255.255 ] 65,536

localhost
?:   127.0.0.1/8  [   127.0.0.0 - 127.255.255.255 ] 16,777,216
----------------------- File -----------------------
help/cloud/openstack.txt
OpenStack

- Compute (Nova)
  manage pools of computer resources. KVM, VMware, and Xen...

- Networking (Neutron)

- Block storage (Cinder)
  block-level storage. Ceph, GlusterFS...

- Identity (Keystone)
  LDAP

...
----------------------- File -----------------------
help/cloud/xaas.md
# Everything as a Service

## Infrastructure as a Service (**IaaS**)
- servers
- storage
- networking
- virtualization

e.g. AWS

## Platform as a Service (**PaaS**: _IaaS_ + OS + runtime)

e.g. Kubernetes, AWS Elastic Beanstalk, Heroku

## Software as a Service (**SaaS**: _PaaS_ + apps + data)

e.g. Dropbox
----------------------- File -----------------------
help/clush.txt
List nodes
nodeset -ll @xx | sed -r 's/^@\S+\s//' | tr , '\n'

Copy a local file to remote hosts
# default dest is /tmp
$ clush -brw@xx -c ~/login.css --dest /home/user

Groups patterns:
/etc/clustershell/groups
map:  perl -ne 'print "$1" if /\b$GROUP\b.*?:\s*(.+?)\s*(?:#.*)?$/ && !/^\s*#/' /etc/clustershell/groups
list: perl -ne 'print "$1\n" if /^\s*(\S.*?):/ && !/^\s*#/' /etc/clustershell/groups

/etc/ansible/hosts
map: sed -n "/^\[$GROUP/,/^\[/p" /etc/ansible/hosts | grep '^[^[#;]' | sed 's/\([0-9]\+\):\([0-9]\+\)/\1-\2/' | tr '\n' ' '
list: sed -n 's/^\[\([^:]\+\)\]/\1/p' /etc/ansible/hosts
# note: we will miss hosts that aren't members of any group
----------------------- File -----------------------
help/color.txt
ANSI color escape sequences

 Esc[ Value ; Value ; Value m
\033[---m

Emphasis:
--------
00=none 01=bold 04=underscore 05=blink 07=reverse 08=concealed

Foreground:
----------
30=black 31=red 32=green 33=yellow 34=blue 35=magenta 36=cyan 37=white

Background:
----------
40=black 41=red 42=green 43=yellow 44=blue 45=magenta 46=cyan 47=white

Example: 01   ; 32    ; 40
         Bold   Green   Black
         em     fg      bg

ANSI escape sequences graphics mode:

256 color escape codes:
   38;5;colorN for fg
   48;5;colorN for bg

LS_COLORS:
eval "$(dircolors /path/to/.dir_colors)"
----------------------- File -----------------------
help/comm.txt
# lines only in file1
comm -23 <(sort file1) <(sort file2)

# lines only in file2
comm -13 <(sort file1) <(sort file2)

# lines common to both files
comm -12 <(sort file1) <(sort file2)
----------------------- File -----------------------
help/cron.txt
                          Crontab

SHELL = /bin/bash

 *      *      *      *      *
0-59 | 0-23 | 1-31 | 1-12 | 1-6     | user /path/to/command
Min    Hour   Day    Month  Mon-Sat     \
                            Sun: 0,7     system crontab only

- %s are newlines and need to be escaped:
  % -> \%

- Ranges and steps:
     */5 → 0,5,10,15,20,25,30,35,40,45,50,55
  1-56/5 → 1,6,11,16,21,26,31,36,41,46,51,56 (⇔ */5+1 ?)

- No $PATH as variables can't be de-referenced in cron

- No comments at EOL

- Make sure there is a new line after the last cron command

Anacron
-------
nb-days | delay after start | job-identifier | command
 |         |                   |
 |         |                   name for the job’s timestamp file (ex: /var/spool/anacron/fstrim.daily)
 |         minutes to wait before executing the job after the machine starts
 7 (@weekly)
----------------------- File -----------------------
help/curl.txt
Send data via POST: -XPOST auto assumed
curl -d@dat_file

Send json object from STDIN
curl -d@- -H"Content-Type:application/json" -Ksecret "https://api.github.com/repos/..." << TAG
...
TAG
Note: -Ksecret introduces a config file:
user login:password ## --user option without the --

curl -XDELETE -ulogin:password "https://api.github.com/repos/.../git/tags/deployed"

curl -I # get headers
----------------------- File -----------------------
help/cut.txt
only splits on a single character, use perl or awk instead:

% echo $'Feb  9 2020\nFeb 14 2020' | cut -d' ' -f3-
9 2020
2020
----------------------- File -----------------------
help/date.txt
# epoch -> std
date -d@1584057600 +%d-%b-%Y

# std  -> epoch - seconds since UNIX epoch: 1 Jan 1970 UTC
date +%s -d'13 Mar 2020'

date +%Y
----------------------- File -----------------------
help/dd.txt
show progress

# native
dd if=/dev/urandom of=/dev/null bs=4M status=progress

# pv
dd if=/dev/urandom | pv -s 2G | dd of=/dev/null bs=4M

# signal
watch pkill -USR1 ^dd
            -INFO for BSD systems

dd can be run from Linux itself as it allows for self destruction (unlike
OpenBSD). Once in memory, if not interrupted it will write the whole HD. Of
course when finished, there will be no OS files and most likely it’ll kernel
panic.

What shred does extra is random data but you can also create some ‘randoms’
with openssl or take from urandom (but it’s slower) or from random which is
even slower.
----------------------- File -----------------------
help/dhcp.txt
Static lease:

/etc/dhcp/dhcpd.conf

default-lease-time 120;
max-lease-time 120;

option domain-name-servers 1.1.1.1, 8.8.8.8;
option routers 192.168.77.1;

subnet 192.168.77.0 netmask 255.255.255.0 {
}

host pearl {
   hardware ethernet c4:ee:69:bc:60:a6;
   fixed-address 192.168.77.232;
}

Notes:
subnet is just needed so dhcpd knows what interface to listen on
the lease duration has been decreased to 2min from the default 12h
----------------------- File -----------------------
help/diff.txt
                                      diff
                                      ====

Unified format:

--- /path/to/old <tab> ''timestamp''
+++ /path/to/new <tab> ''timestamp''

       .- Line number 1 in the old file
      / .-- Sum of contextual lines and minuses in this hunk (1)
     / /
@@ -1,3 +1,8 @@
          \ \
           \ *-- Sum of contextual lines and pluses in this hunk (2)
            *- Line number 1 in the new file

- line: only present in old file
+ line: only present in new file

Differences:

+ This is an important        (2)
+ notice! It should           (2)
+ therefore be located at     (2)
+                             (2)
  This part of the        (1) (2)
- document has stayed the (1)
  same from version to    (1) (2)
+ the beginning of this       (2)
+ document!                   (2)

Multiple files comparison:

diff -uq --from-file file1 file2...filen

Compare directories:

diff -qr staging production
----------------------- File -----------------------
help/dig.txt
Short (try 'long' if failing)
=====
dig +short
dig +short +authority

Long
====
dig +noall +answer
dig +noall +answer +authority # if querying -t ns

Longest
=======
dig +noadditional +nocmd +noquestion +nostats

NB:
dig, host and nslookup
won't consult /etc/hosts as they query name servers only.

to check if nsswitch resolution is working:
getent hosts my-site
----------------------- File -----------------------
help/dkim.txt
Mail server:
generate keys with opendkim-genkey
publish public key in DNS

configure opendkim/postfix

sign emails with private key

Clients:
receivers are now able to check emails integrity by using the public key found
in the DNS record of the sender.
----------------------- File -----------------------
help/dmesg.txt
dmesg -H
dmesg -Hx
dmesg -HP -lerr

dmesg -eL # if no -H

-H time + delta, color, pager # human
-e time + delta
-L color
-P no pager
-l warn,err # restrict to levels
-x show facility:level
-u userspace
----------------------- File -----------------------
help/dns.txt
resolve name to IP

port 53

             zones
       /      /     /
bob.sales.example.com
 |    |      |     +-- top-level domain (on root nameservers)
 |    |      +-- subdomain of com
 |    +-- subdomain of example
 +-- resource record (RR) part of the sales.example.com domain

Zones are defined in zone files on primary nameservers. They contain RRs such as bob.

Authoritative nameservers answer to resource records that are part of their
zones only. Recursive nameservers offer resolution services, but they are not
authoritative for any zone (ie. they don't have direct answers).

BIND
* named: nameserver
* rndc: admin utility
* dig

AWS: cli53

The mechanism for name resolution is decided in:
/etc/nsswitch.conf
hosts files DNS
        |    +-- use DNS servers declared in /etc/resolv.conf (often overwitten by NetworkManager or DHCP)
        +-- /etc/hosts

gotcha: the host command doesn't respect /etc/nsswitch.conf, thus not /etc/hosts either

www.example.com -> 12.34.56.78
web.example.com -> CNAME to www.example.com (~symlink, target must be A/AAAA + not zone apex)
    12.34.56.78 ->   PTR to web.example.com (~reverse link)
----------------------- File -----------------------
help/docker.md
# Docker

Lightweight virtualization<br>
daemon + containers are OS processes =><br>
containers share kernel/network/disk/memory with the host

Containers abstract applications from operating systems, much like
virtualization abstracts operating systems from physical hardware

The kernel isolates network and disk resources per processes running in a kernel namespace: the container.<br>
Kernel namespaces are possible thanks to `runc` (run containers) and previously Linux Containers (LXC)?<br>
`aufs` is used for disk sharing. All containers get a ro shared acces to all common parts of the OS + get their own mount for writing.

Each docker image contains only the differences from the base. When you run
your image, you also need the base, and it layers your image on top of the base
using a layered file system (`aufs`)

docker daemon
: not a hypervisor, it's the containerization runtime

image
: provides filesystem, dependencies, config

container
: runnable instance of an image aka sandboxed process (akin to `chroot`)

```bash
docker help ps
docker search
docker ps
docker build # Dockerfile
docker volume ls # persist data
docker images # docker image ls
docker image prune # manage
docker container prune
docker rm id

docker run -it --rm centos:7 bash # interactive + tty + auto rm container on exit
docker run -d -p 80:80 docker/getting-started # detached mode
docker exec -it id cat /etc/passwd # exec a command in a running container
docker stop id

docker pull debian:stable-slim
docker push kurkale6ka/catnip
```

containers need to be on the same network in order to talk

```bash
docker network ls
docker network create my-app # docker run --network my-app ...
```

# Docker Compose

* define your application stack in a versioned file
* docker-compose up/down/... `-d`
* application can be multi-container: e.g app & mysql in the below example

```yaml
version: "3.7"

services:
  app:
    image: node:12-alpine
    ports:
      - 3000:3000
    environment:
      MYSQL_HOST: mysql

  mysql:
    image: mysql:5.7
    volumes:
      - mysql-data:/var/lib/mysql

volumes:
  mysql-data:
```

versus 2 docker run commands:

```bash
docker run -dp 3000:3000 \
  -w /app -v "$(pwd):/app" \
  --network my-app \
  -e MYSQL_HOST=mysql \
  node:12-alpine \
  sh -c "yarn install && yarn run dev"

docker run -d \
  --network my-app --network-alias mysql \
  mysql:5.7
```
----------------------- File -----------------------
help/du.md
# Disk usage

## basic
```bash
du -ah -d1
#   ││  └─ depth
#   │└─ human readable
#   └─ all files, not only dirs
```

> **Warning**
> ```bash
> du -sh * # .files/folders would be missed!
> #   └─ summarize (same as -d0)
> ```

## filtered and sorted
```bash
du -ah -d1 -t100m | sort -hr
#           │             └┴─ human, reverse
#           └─ min threshold
```

> **Note**
>
> with old versions of `du`/`sort`, use:
> ```bash
> du -a --max-depth 1 | sort -nr
> ```

## filtered, sorted and pretty output
```bash
du -ah0 -d1 -t100m | sort -hrz | perl -0lane 's:^\./:: for @F; print shift @F, " ", `ls -d --color "@F"`'
```

> **Note**
> * `du`: remove/increase `-d1` in order to descend into directories
> * `du`: skip files/folders with `--exclude`
>
> with old versions of `du`/`sort`, use:
> ```bash
> du -a --max-depth 1 | sort -nr | head | perl -ane 's:^\./:: for @F; print shift @F, " ", `ls -d --color "@F"`'
> ```

# Total
`du -sh /folder`
----------------------- File -----------------------
help/easy-rsa.txt
index.txt:
The second column is the certificate expiration date in ASN1_TIME format.
The entry '180301135738Z' means the certificate end date is 2018, March 01 13:57:38 GMT
----------------------- File -----------------------
help/exiftool.txt
Options can't be grouped since they would form a valid tag name
-G -S but not -GS

Output tag with custom formatting
exiftool -p '${createdate;s/:/-/g}' my_pic.jpg

Delete a cached tag in Shotwell
* rm -r ~/.local/share/shotwell ~/.cache/shotwell
* exiftool -xmp:subject= -xmp:lastkeywordxmp= -exif:xpkeywords= .

View embedded thumbnail
exiftool -thumbnailimage '2012-04-23 11.29.04.jpg' -b | ffplay -v error -

TODO: how to avoid duplication when adding tags?
after running the below command 2 times, -keywords will have a value of my_tag, my_tag
exiftool -iptc:keywords+=my_tag my_pic

Implicit tr/// to remove weird characters
${make;}

Data in computer batch mode, not human readable
$createdate#

Add tags
exiftool -sep ', ' -iptc:keywords+='tag1, tag2' picture.jpg
exiftool -sep ', ' -xmp:subject+='tag1, tag2' video.mp4

Delete originals without prompting
exiftool -delete_original! .

Renaming
see 'RENAMING EXAMPLES' in 'man exiftool' + -dateFormat

Copying
see -tagsFromFile in 'man exiftool'

Append the camera 'make' to the file name (check for already existing 'make')
exiftool -ext jpg '-filename<%f ${make;}.%e' -if '${filename;s/\.\S+//} !~ /[[:alpha:]]/i' .

Find file names with a specific tag
exiftool -if '($iptc:keywords && $iptc:keywords =~ /birthday/i) or ($xmp:subject && $xmp:subject =~ /birthday/i)' -p '"$directory/$filename"' . 2>/dev/null

List files without 'tags'
exiftool -if 'not $iptc:keywords and not $xmp:subject' -p '"$directory/$filename"' . 2>/dev/null

Fix wrong creation date
don't forget to apply to datetimeoriginal too + update the file name after!
exiftool -createdate='2007:07:21 07:29:59' '2007-05-27 07.29.59.jpg'

Change the keywords reusing the old ones
exiftool -q -if '$iptc:keywords =~ /^@@@$/i' -p 'exiftool -sep ", " -iptc:keywords="$iptc:keywords" "$filename"' . 2>/dev/null | v -

Look for problems detected by Dropbox. Look for backups
ll **/*(conflict*|_original|~)(D)

List 'incorrectly' named files
ls -1 **/^????-??-??\ ??.??.??*.[^.]##(.)

Find non-media files
ll **/^*.(jpg|jpe|png|avi|mov|mp4|mts|3gp)(D.)

Print files having camera 'make' != samsung|apple
ls -1 **/^*(samsung|apple)*(.) # requires unsetopt case_glob

List files without camera brands in the name
ll **/[^a-zA-Z]##.[^.]##

Mass tagging
exiftool -if '$iptc:keywords !~ /christmas/i' -iptc:keywords+='christmas eve' **/[12]???-12-24*(.)
exiftool -if '$iptc:keywords !~ /christmas/i' -iptc:keywords+='christmas' **/[12]???-12-25*(.)
exiftool -if '$iptc:keywords !~ /birthday/i' -sep ', ' -iptc:keywords+='mitko, birthday' **/October/*-10-19*(.)
exiftool -if '$iptc:keywords !~ /birthday/i' -sep ', ' -iptc:keywords+='iva, birthday' **/May/*-05-30*(.)
----------------------- File -----------------------
help/ffmpeg.txt
          input file             video filter, short for -filter:v
         /                      /
ffmpeg -i video.mp4 -c:a copy -vf 'transpose=1, transpose=1' out.mp4 # 180° rotation
                       \
                        audio/video codec to use, copy meaning preserve

info:
ffprobe -v error -show_format video.mp4
ffprobe -v error -show_entries format=duration jumping.mov

cutting small sections:
ffmpeg -i video-in.mp4 -ss 11 -t duration -c copy cut.mp4
ffmpeg -i video-in.mp4 -ss 11 -to position -c copy cut.mp4
                         \
                          seek: start position

play video in a scaled down version and twice the speed:
ffplay jumping.mov -vf 'scale=220:-1,setpts=.5*PTS'

map:
a way to tell ffmpeg which streams you want to select/copy from input to output

Stream #0:0(eng): Video: h264 (High), yuv420p, 1920x800, 23.98 fps, 23.98 tbr, 1k tbn, 47.95 tbc (default)
Stream #0:1(ger): Audio: dts (DTS), 48000 Hz, 5.1(side), s16, 1536 kb/s (default)
Stream #0:2(eng): Audio: dts (DTS), 48000 Hz, 5.1(side), s16, 1536 kb/s
Stream #0:3(ger): Subtitle: text (default)

Ex: extract audio streams:
ffmpeg -i input.mkv -map 0:1 -map 0:2 -c copy output.mkv

## Import CD audio tracks

findmnt
cd /run/user/1000/gvfs/cdda:host=sr0

parallel ffmpeg -i {} -codec:a libmp3lame -qscale:a 0 ~/Dropbox/music/clarinet_abracadabra/cd_1/{.}.mp3 ::: *.wav
----------------------- File -----------------------
help/find.md
# Examples

```bash
find . -name '*.gz' -delete
find . -name '*.gz' -exec rm {} + # means 'in bulk', {} must precede +

find /usr -perm -o=r -print   # permissions
find /tmp -mtime +30 -print   # modification time
find . ! -user dimitar -print # user
```

# Exclude stuff

`find [path] [conditions to prune] -prune -o [usual conditions] -print`

`-name goo -prune` without a `-print`, includes an explicit `-print`:
only files or directories matching _goo_ are printed, but not descended, thus a
second folder goo under the first one won't be printed

`-type d -name goo -prune -o -print`:
because of the explicit `-o -print`: we either exclude (without printing) or
print when we don't exclude
----------------------- File -----------------------
help/fonts.adoc
A patched font is a font that has a bunch of icons patched into it

1. Download

https://github.com/ryanoasis/nerd-fonts#patched-fonts
https://github.com/ryanoasis/nerd-fonts/tree/master/patched-fonts/DejaVuSansMono/Regular/complete
https://github.com/ryanoasis/nerd-fonts/blob/master/patched-fonts/DejaVuSansMono/Regular/complete/DejaVu%20Sans%20Mono%20Nerd%20Font%20Complete%20Mono.ttf

2. Install

 mkdir ~/.local/share/fonts
 mv '~/Downloads/DejaVu Sans Mono Nerd Font Complete Mono.ttf' ~/.local/share/fonts

3. Clear and regenerate your font cache

`fc-cache -fv`

3.1 Check

`fc-list | grep -i nerd`

4. Change terminal preferences to use the font

5. Find fonts

https://www.nerdfonts.com/cheat-sheet
----------------------- File -----------------------
help/fzf.txt
% vim ^t
% vim [pattern]**<tab> # not **/<tab>
% vim $(fzf)

% fzf # same as FZF_DEFAULT_COMMAND | fzf
                 \
                  find . -type f ! -name '.*' -print

Inside vim, use :FZF or :Files, :VF will use locate first...

zsh's vf will also prefer locate

Options:
   FZF_DEFAULT_COMMAND / FZF_DEFAULT_OPTS
    FZF_CTRL_T_COMMAND / FZF_CTRL_T_OPTS
                         FZF_CTRL_R_OPTS
     FZF_ALT_C_COMMAND / FZF_ALT_C_OPTS
FZF_COMPLETION_TRIGGER / FZF_COMPLETION_OPTS # for **

-1, automatically select the only match
-0, exit immediately when there's no match
----------------------- File -----------------------
help/games/list
Roms
          Rom Hustler: http://romhustler.net/
            Free ROMs: https://www.freeroms.com/
             Cool ROM: http://coolrom.com/
 Coleco Vision Addict: http://cvaddict.com/list.php

No legal restrictions: http://www.mamedev.org/roms/

Games

chrono trigger
diablo 2
donkey kong country
dungeons and dragons
earth worm jim
flashback
mario
mortal kombat 2
myst
oddworld: Abe's Oddysee
ridge racer
riven a sequel to myst
shinobi
super metroid
super probotector
wonder boy
zelda
----------------------- File -----------------------
help/games/mortal_kombat_2.md
# Baraka

```
   Blade Swipe: Back               + High Punch
   Blade Spark: Down + Back        + High Punch
      Shredder: Back + Back + Back + Low Punch
   Double Kick: HK, HK

  Decapitation: Hold Block, Back + Back + Back + Back + High Punch
     Stab Lift:    Back + Forward + Down + Forward + Low Punch (close)
Stage Fatality: Forward + Forward + Down           + High Kick (close)

    Friendship: Hold Block, Up + Forward + Forward + High Kick
      Babality: Forward + Forward + Forward + High Kick (anywhere)
```

# Jax

```
   Gotcha Grab: Forward + Forward + Low Punch
    Wave Punch: Back + Down + Forward + High Kick
  Back Breaker: Block (in the air next to opponent)
     Quad Slam: Tap High Punch while throwing opponent
  Ground Pound: Hold Low Kick for 5 seconds

     Head Clap: Hold Low Punch, Forward + Forward + Forward (close)
       Arm Rip: Block + Block + Block + Block + Low Punch (close)
Stage Fatality: Hold Block, Up + Up + Up + Low Kick (close)

    Friendship: Hold Block, Down + Down + Up + Up + Low Kick
      Babality: Hold Block, Down + Up + Down + Up + Low Kick (anywhere)
```

# Johnny Cage

```
        High Fireball: Forward + Down + Back + High Punch
         Low Fireball: Back + Down + Forward + Low Punch
          Shadow Kick: Back + Forward + Low kick
      Shadow Uppercut: Back + Down + Back + High Punch
             Low Blow: Block + Low Punch (close)

            Torso Rip: Down + Down + Forward + Forward + Low Punch (close)
Decapitating Uppercut: Forward + Forward + Down + Up (close)
       Stage Fatality: Down + Down + Down + High Kick (while close)

           Friendship: Down + Down + Down + Down + High Kick (anywhere)
             Babality: Back + Back + Back + High Kick (anywhere)
```

# Kitana

```
       Fan Swipe: Back              + High Punch
        Fan Lift: Back + Back       + High Punch
       Fan Throw: Forward + Forward + High and Low Punch
    Square Punch: Down + Back       + High Punch

   Kiss of Death: Hold Low Kick, Forwad + Forward + Down + Forward
Fan Decapitation: Block + Block + Block + Block + High Kick (close)
  Stage Fatality: Forward + Down + Forward + High Kick (while close)

      Friendship: Hold Block, Down + Down + Down + Up + Low Kick (anywhere)
        Babality: Down + Down + Down + Low Kick (anywhere)
```

# Kung Lao

```
        Dive Kick: Up + Down + High Kick
Spinning Tackdown: Up + Up   + Low Kick (Tap Low Kick to keep spinning)
         Teleport: Down + Up
         Hat Toss: Back + Forward + Low Punch

        Hat Slice: Up + Up + High Punch (sweeping distance)
 Hat Decapitation: Hold Low Punch, Back + Forward, Release Low Punch
   Stage Fatality: Forward + Forward + Forward + High Punch (while close)

       Friendship: Back + Back + Back + Down + High Kick (anywhere)
         Babality: Back + Back + Forward + Forward + High Kick (anywhere)
```

# Liu Kang

```
         Fireball: Forward + Forward + High Punch (also while jumping)
     Low Fireball: Forward + Forward + Low Punch
      Flying Kick: Forward + Forward + High Kick
     Bicycle Kick: Hold Low Kick for 5 seconds

           Dragon: Down + Forward + Back + Back + High Kick (while close)
Spinning Uppercut: Hold Block + 360 on D-Pad (away fullscreen)
   Stage Fatality: Back + Forward + Forward + Low Kick (while close)

       Friendship: Forward + Back + Back + Back + Low Kick (sweeping distance)
         Babality: Down + Down + Forward + Back + Low Kick (sweeping distance)
```

# Mileena

```
 Teleport Kick: Forward + Forward + Low Kick
   Ground Roll: Back + Back + Down + High Kick
     Sai Throw: Hold High Punch for 3 seconds and release

       Surgery: Forward + Back + Forward + Low Punch (close)
     Man Eater: Hold High Kick for 3 seconds and release (close)
Stage Fatality: Forward + Down + Forward + Low Kick (close)

    Friendship: Hold Block, Down + Down + Down + Up + High Kick
      Babality: Down + Down + Down + High Kick (anywhere)
```

# Raiden

```
   Lightning Blast: Down + Forward + Low Punch
Flying Thunderbolt: Back + Back + Forward (can also be done in air)
          Teleport: Down + Up
           Shocker: Hold High Punch for 3 seconds (close)

     Electrocution: Hold Low Kick for 5 seconds, Release then tap Block + Low Kick rapidly (close)
Uppercut Explosion: Hold High Punch for 8 seconds, Release (close)
    Stage Fatality: Hold Block, Up + Up + Up + High Punch (close)

        Friendship: Down + Back + Forward + High Kick (anywhere)
          Babality: Hold Block, Down + Down + Up + High Kick (anywhere)
```

# Reptile

```
     Acid Spit: Forward + Forward + High Punch
    Force ball: Back + Back + Back + High And Low Punch
         Slide: Back + Low Punch + Low Kick + Block
  Invisibility: Hold Block, Up + Up + Down + High Punch (release Block)

   Tongue Lash: Back + Back + Down + Low Punch (fullscreen)
  Invisi-Slice: Forward + Forward + Down + High Kick (while invisible)
Stage Fatality: Down + Forward + Forward + Block (while close)

    Friendship: Back + Back + Down + Low Kick (anywhere)
      Babality: Down + Back + Back + Low Kick
```

# Scorpion

```
         Spear: Back + Back + Low Punch
     Air Throw: Block (in the air next to opponent)
      Teleport: Down + Back + High Punch
      Takedown: Forward + Down + Back + Low Kick

 Flaming Skull: Up + Up + High Punch (sweeping distance)
   Spear Slice: Hold High Punch, Down + Forward + Forward + Forward + Release High Punch
Stage Fatality: Down + Forward + Forward + Block (while close)

    Friendship: Back + Back + Down + High Kick (anywhere)
      Babality: Down + Back + Back + High Kick (anywhere)
```

# Shang Tsung

```
         Liu Kang: Back + Forward + Forward + Block
         Kung Lao: Back + Down + Back + High Kick
         Sub-Zero: Forward + Down + Forward + High Punch
         Scorpion: Up + Up
          Reptile: Up + Down + High Punch (block before up but release before HP)
           Raiden: Down + Back + Forward + Low Kick
              Jax: Down + Forward + Back + High Kick
      Johnny Cage: Back + Back + Down + Low Punch
           Kitana: Block + Block + Block
          Milenna: High Punch for 3 seconds
           Baraka: Down + Down + Low Kick

Flaming Skull (1): Back + Back                     + High Punch
Flaming Skull (2): Back + Back + Forward           + High Punch
Flaming Skull (3): Back + Back + Forward + Forward + High Punch

        Inner Ear: High Kick for 3 seconds (close)
       Soul Steal: Hold Block + Up + Down + Up + Low Kick (close)
    Kintaro Morph: Hold Hold Low Punch for 30 seconds during match
   Stage Fatality: Hold Block + Down + Down + Up + Down (while close)

       Friendship: Back + Back + Down + Forward + High Kick (anywhere)
         Babality: Back + Forward + Down + High Kick (anywhere)
```

# Sub-Zero

```
     Ice Blast: Down + Forward + Low Punch
    Ground Ice: Down + Back + Low Kick
         Slide: Back + Low Punch + Low Kick + Block (same as reptile)

   Ice Shatter: Forward + Forward + Down + High Kick (sweep) Then Forward + Down + Forward + Forward + High Punch
   Ice Grenade: Hold Low Punch, Back + Back + Down + Forward + release
Stage Fatality: Down + Forward + Forward + Block (while close)

    Friendship: Back + Back + Down + High Kick (anywhere)
      Babality: Down + Back + Back + High Kick (anywhere)
```

# Cheat

## Acid Bath
Uppercut while holding Low Punch + Low Kick
Hold D + BL after Stage Fatality to hear Oh Wow.

## Kombat Tomb
Hold D + BL + Start to force the opponent to fall from the spikes after a Stage Fatality

## Disabling Throws
Press and hold D + HP for both players before the match starts to disable throwing.

## Babality and Friendship
No punches during final round

## Random Select
Press Up + Start on Liu Kang (For Player One) and on Reptile (For Player Two).

## Beat up the Logo
When you turn on the SNES, hold L and R. Watch Shao Kahn and Kintaro beat up the Acclaim logo.

## Elimination Mode:
Hold L and R at the main menu. This code will enable an 8-player elimination mode.

## Selectscreen Kodes
Enter these codes after having selected a player.

Fight Shao Kahn:   F, U, U, F, B, Select
Fight Kintaro:     U, D, D, F, F, Select
Fight Noob Saibot: B, U, D, D, F, Select
Fight Jade:        U, D, D, B, F, Select
Fight Smoke:       U, B, U, U, F, Select

Super Damage:           D, U, F, U, B, Select
Extended Fatality Time: U, U, B, U, D, Select
Enable 30 Credits:      B, U, F, D, B, Select

## Jade
Fight:
Win one round during the match before the "Question Mark" during the main
single player game using only Low Kick inputs (No blocks, High Punch, Low
Punch, or High Kick).

## Noob Saibot
Fight: Win 25 versus matches in a row

## Smoke
Fight:
On the red portal stage, keep doing upper cuts until Dan "Toasty" Forden (the
guy who always says toasty) comes out. Then very quickly, press Down + Start

Play:
You must have a Game Genie. Attach Mortal Kombat 2 to the Game Genie and enter
the password: B4MT-BE76. Start the game,then switch to Controller two. Choose
your character, and when you go into battle you will be Smoke.

## Ping Pong
Beat a second player 250 matches in a row and you will be transported to
another dimension where the two fighters become ping pong rackets. You and your
opponent will then play a brief game of ping pong before being transported back
to the normal fighting.

## Pizza Man
Activate Test Mode (see below) and choose NO DAMAGE TO PLAYER 1. Then enter the
contest with Shang Tsung. When the guy who says "Toasty" comes out he will
resemble a pizza like creature.

# vim: fdm=expr fde=getline(v\:lnum)=~'^#'?'>'.len(matchstr(getline(v\:lnum),'##*'))\:'='
----------------------- File -----------------------
help/git.md
# Ranges

## commits
```sh
git log main.. # commits in HEAD only
git log A...B  # commits in B only and in A only
```

## diff
```sh
git diff foo..bar  # changes introduced by both
git diff foo...bar # changes in bar only
```

# Branches

* a branch is a pointer to a commit
* `HEAD` points to the current branch # `git checkout br1` moves `HEAD->br1`
* The `HEAD` branch moves forward when a commit is made

`git ls-remote origin -> refs/heads/main # *heads* refers to 'branches'`

## Track a remote(-tracking) branch (e.g origin/main)

you do that in order to use git pull/push without arguments (implied origin + remote)

_note_: `origin/main` is called a remote-tracking branch. it's a local reference/pointer that can't be moved
```sh
git checkout -b <branch> <remote>/<branch>
git checkout --track origin/serverfix
git checkout serverfix

git branch -u origin/serverfix
git push -u origin my-branch # set origin/my-branch (@{u[pstream]}) as upstream for my-branch
                └─ or HEAD if on the branch
```

## Prune remote-tracking branches
```sh
git fetch --prune
git remote prune origin
```

# Rebase

## rebase experiment onto main => experiment moves/points further ahead

`git rebase main experiment` or
```sh
git checkout experiment
git rebase main
```

## ff merge experiment into main => main catches up with experiment
```sh
git checkout main
git merge experiment
```

# Cherry Pick

Apply any commit to the current branch
```sh
git cherry-pick 3c9b10a # this commit comes from another branch
```
_we now have a duplicate commit (it exists in 2 branches) => only do if you don't want the whole other branch!_

# Index
Everything tracked is in your staging area - _the index_
```sh
git rm --cached # untrack: remove from index
git rm          #      rm: remove from index and working copy
```

## redo last commit: change index plus/or commit message.

`git commit --amend`, same as:
* `git reset --soft HEAD^`
* modify staging area
* `git commit`

_uncommit file example_:
- `git reset @~ file`
- `git commit --amend -m'...'`

# git reset

## reset with a ref (commit) but without a path (file)

<ins>Our need is to change the commits...</ins>
1. move the `HEAD->branch` pair to another commit.
2. update index: _stop here by default (change with --*soft*/*hard*)_
3. optionally update the working dir

HEAD (--soft)  ⇒  Index  ⇒  Working dir (--hard)

_example 1_: `git reset HEAD~`
* move `HEAD->branch` to previous commit => undo last commit
* update index with the snapshot `HEAD->branch` points to => unstage everything

_example 2_: `git reset main`
* move `HEAD->branch` to where `main` points
* ...

## reset with a path (and optional commit)

1. <ins>Here, commit history isn't the issue, plus changing commit would affect many files => so skip this step</ins>
2. unstage file or more accurately, copy file from HEAD to index

* `git reset file`: unstage file (opposite of git add file)
* `git reset`: unstage all files
* `git reset --hard`: unstage everything + reset working dir
* `git checkout main~2 file`: update the index + working dir from `main~2` commit (default is `HEAD`) `git reset --hard main~2 file` would do the same thing.

# reflog

* it's a *local* history of *all* (no history rewriting as with `git log`) commits
* ring buffer with a limited amount of data (a few months)

# git checkout

## without paths

```sh
git checkout [commit]     # same as:
git reset --hard [commit] # but working-dir safe + only moves HEAD
```

## with paths

```sh
git checkout [commit] file     # same as:
git reset --hard [commit] file # but not implemented in git-reset
```

## create topic from local main and check it out
`git checkout -b topic main`

same as:
```sh
git branch topic main
git checkout topic
```

# refspec

- fetch = `+src(_remote_):dst(_local_)`
- push = `+src(_local_):dst(_remote_)`

`+`: update the reference even if it isn’t a fast-forward

remote branches `refs/heads/\*` go under `refs/remotes/origin/*` locally: `fetch = +refs/heads/\*:refs/remotes/origin/*`

_these are equivalent:_
```sh
git push origin serverfix
git push origin serverfix:serverfix
git push origin refs/heads/serverfix:refs/heads/serverfix
```

## delete remote reference

* `git push origin --delete topic`
* `git push origin :topic` <- push empty `src` to remote

# Operations in bulk
```sh
fd --strip-cwd-prefix -FIH -td .git | parallel --tag --tagstring '{//}' 'git -C {//} branch' | grep -v 'main$'
fd --strip-cwd-prefix -FIH -td .git | parallel --tag --tagstring '{//}' 'git -c color.status=always -C {//} status -sb'
```

# Configuration

## store credentials for use with https
```sh
git config --global credential.https://github.com.username kurkale6ka
git config --global credential.helper store
```

then `git push` will ask for the token/password

# Glossary

_parent commit of_ `HEAD`: `HEAD^` or `HEAD~` or `@~`
----------------------- File -----------------------
help/gitlab.adoc
== Runners

  EC2          separate EC2
GitLab <---> GitLab Runner app (golang)
==============================
job 1    |   runner 1
job ...  |     executor
         |       shell  (e.g PowerShell if GitLab Runner is on a Win server)
         |       docker (GitLab Runner on a Linux server)
         |   runner ...

runners need to be registered with the GitLab instance

== https://docs.gitlab.com/ee/ci/yaml/[Keywords]

needs:: Relationships between jobs to speedup runs.
With `needs: []`, a job runs immediately as it has no needs

trigger:: Start a downstream pipeline

exends:: Reuse configuration sections. Replaces YAML anchors

rules:if:: replaces only/except
----------------------- File -----------------------
help/gpg.txt
# generate
gpg --(full-)gen-key

# list
gpg -k # pub
gpg -K # private

# share
gpg -a --export <ID>
      \
       if 'armor' not given it's exported in binary vs ASCII

# import
gpg --import ~/Downloads/key

# encrypt with multiple public keys
gpg -e -r me -r recipient1 /tmp/secret

# encrypt with passphrase
gpg -c --cipher-algo AES256 file.zip

# decrypt
gpg -d /tmp/secret.gpg

If no command is supplied, gpg tries to be helpful:
gpg: WARNING: no command supplied. Trying to guess what you mean ...
----------------------- File -----------------------
help/graphite.txt
Server

graphite needs metrics
|_ carbon listens for them

Client

- statsd
- sensu
- ...

  send the metrics

Visualize with Graphite-Web or with alternatives via the Graphite API:

- Grafana
- ...
----------------------- File -----------------------
help/growisofs.txt
dvd+rw-tools:
growisofs -speed=1 -dvd-compat -Z /dev/sr0=fossapup64-9.5.iso
----------------------- File -----------------------
help/grub.txt
c - command-line
grub>

List vars
set

List files
ls
ls (hd0)
ls (hd0)/
ls -l (hd0)/

(hd0,0) - 1st drive, 1st partition

echo $root

help search

pager=1
cat /boot/grub/grub.cfg
----------------------- File -----------------------
help/hardware.txt
Host controller

        host controller           network
host    host adapter              storage devices (usb: xhci, sata: ahci)
        host bus adapter (HBA)
        (PCI cards or directly on the mobo: usb, sata)
----------------------- File -----------------------
help/haskell.txt
Concepts
--------

In purely functional programming you don't tell the computer what to do as such
but rather you tell it what stuff is in the form of functions.

If you say that a is 5, you can't say it's something else, a is immutable.

In purely functional languages, a function has no side-effects. The only thing a
function can do is calculate something and return it as a result. If a function
is called twice with the same parameters, it's guaranteed to return the same
result. That's called referential transparency.

Haskell is lazy. That means that unless specifically told otherwise, Haskell
won't execute functions and calculate things until it's really forced to show
you a result.

Haskell uses a very good type system that has type inference. Type inference
allows your code to be more general.

Functor
-------

instance Functor (Either a) where    -- Either a has nothing to do with currying, it is simple substitution
    fmap f (Right x) = Right (f x)   -- Right is usually used for the result
    fmap f (Left x) = Left x         -- Left is usually used for the error

:t fmap
Functor f => (x -> y) -> f x -> f y

So we would have ... (x -> y) -> Either a x -> Either a y
                                 --------      --------
To match Either type1 type2,         \            /
  we use Right x                     Left unchanged
     and Left x

--------------------------------------------------------------------------------

Functions are functors
                     g
instance Functor ((->) r) where
    fmap f g = (\x -> f (g x))
          g x         f (g x)
OR
    fmap = (.), so f . g is the same as fmap f g

a -> b is a type, the type of a function =>
(->) a is a type constructor expecting one parameter only

Resemblances: [2,11,..] :: [Int]
              Just 'a'  :: Maybe Char
              g 2.3     :: r -> Float
----------------------- File -----------------------
help/hcl/packer.md
# source block
launch EC2

# build
- modify the launched EC2, e.g. via Ansible
- take an image
----------------------- File -----------------------
help/hcl/terraform.adoc
:toc: left

= Terraform

== State

`terraform 'refresh'` checks if objects in my 'real world' (state file) exist and are the same in the actual world (_e.g_ cloud). +
Thus, if I `terraform state rm ADDR`, I've got less things to compare => +
a `'refresh'` on an empty state would report 'No changes' (_nothing to compare_)!

== HCL (HashiCorp Configuration Language)

=== Syntax

 blk_type "label1" ... "labeln" {
    # argument
    name = "value"
 }

 terraform {
    required_providers {
       aws = {
          source = registry.terraform.io/hashicorp/aws ...

remote registry:: [hostname]/namespace/name/provider//subdir

`provider "aws" { ...`

`resource "aws_instance (*type*)" "app_server (*name*)" {`

id:: aws_instance.app_server

`for_each` = `map` or `set` => `each.key` [+ `each.value`]

== Data sources

Resources take arguments and export attributes for use in configuration, but
while managed resources cause Terraform to create, update, and delete
infrastructure objects, data resources cause Terraform only to read objects

== Modules

folder with _.tf_ files in it

 +-- elasticache/
 ¦   +-- main.tf
 ¦   ¦   # resource "aws_elasticache_replication_group" "elasticache-cluster" {
 ¦   ¦   #   replication_group_id = "tf-${var.environment}-rep-group"
 ¦   ¦   #   availability_zones   = ["${var.availability_zones}"]
 ¦   +-- variables.tf
 ¦       # variable "environment" {}
 ¦       # variable "availability_zones" { type = "list" }
 +-- environments/
 ¦   +-- production/
 ¦   ¦   +-- main.tf
 ¦   ¦       # module "prod-elasticache" {
 ¦   ¦       #   source = "../../elasticache"
 ¦   ¦       #   environment        = "dev"
 ¦   ¦       #   availability_zones = ["us-east-1a", "us-east-1b"]
 ¦   +-- staging/
 ¦       +-- main.tf
 ¦           # module "stg-elasticache" {
 ¦           #   source = "../../elasticache"
 +-- main.tf
     # module "production" {
     #   source = "environments/production"
     # }

== Deprecated

* `terraform [.line-through]#taint#` vs `terraform apply -replace aws_instance.web1`
* `terraform [.line-through]#refresh#` vs `terraform apply -refresh-only`
----------------------- File -----------------------
help/hcl/variables.md
# Call

```hcl
   var.name
"${var.name}"
```

# Define

blocks in _*.tf_ for terraform and _*.pkr.hcl_ for packer:

```hcl
variable "name" {
    description = "..."
    type        = string
    default     = "..."
}
```

# Assign (or use `default = ...` in the definition):

## auto load _key = value_ files:

### terraform
- `terraform.tfvars`
- *.`auto.tfvars`

### packer
- *.`auto.pkrvars.hcl`
----------------------- File -----------------------
help/hcl/vault.md
# Secrets

A *kv* secret is a *dict* of key/value pairs

```
[/namespace/]engine(e.g kv)/secret
  field1: val1
  field2: val2
```

## list

```
vault secrets list
vault      kv list GitLab
```

## put vs patch

`vault kv patch` creates a new version same as `vault kv put`,
the difference is it merges the new _K#V_ pairs with the existing ones

# PKI

```bash
vault path-help pki # show supported API

# Certs
vault kv list xxx/certs # read doesn't work for that
vault kv get -field=... xxx/cert/ca | openssl x509 -noout -subject -issuer -dates # kv can be used instead of read
vault kv get -field=... xxx/cert/00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00

vault read xxx/config/urls # crl

# Roles
vault list xxx/roles
vault read xxx/roles/my_role
vault kv get -field=allowed_domains xxx/roles/my_role | tr '[] ' '\r\n'

vault write xxx/issue/gitlab_https common_name=... # issue new cert
vault write xxx/intermediate/generate/internal common_name="myvault.com Intermediate Authority" ttl=43800h # generate CSR
```
----------------------- File -----------------------
help/head.txt
cat file from line 4 to $
tail -n+4 file # mnemo: don't change the tail

cat file from line 1 to $-4
head -n-4 file # mnemo: don't change the head
----------------------- File -----------------------
help/history.txt
fc -... old=new start end

   id  command
   ==  =======
    1  ssh jumphost
    2  echo $PATH
-4) 3  ls
    4  vim
    5  exec zsh
-1) 6  cal -------> latest

Lines from the beginning get discarded after reaching HISTSIZE (in RAM)
but ids aren't renumbered

# fix last command
fc
fc -1

# list recent
history
fc -l
fc -l -16

# list all
history 1

# list from 3 to 6
fc -l  3
fc -l  3  6
fc -l  3 -1
fc -l -4
fc -l -4 -1

# reexecute previous
!!
r
fc -e-

# reexecute 3
!3
fc -e- 3

# execute 2 without editing (- editor),
# globally replace a string beforehand
fc -e- HOME=PATH 2
^old^new^:G
^old^new # single

# bash only
fc -s old=new 2
!!:gs/old/new
!!:s/old/new
----------------------- File -----------------------
help/hostname.txt
hostnamectl set-hostname new

amazonlinux
- echo 'preserve_hostname: true' > /etc/cloud/cloud.cfg.d/30_hostname.cfg
- hostnamectl set-hostname amazonlinux.localdomain
- 127.0.0.1   amazonlinux amazonlinux.localdomain localhost4 localhost4.localdomain4
----------------------- File -----------------------
help/imagemagick.txt
                                  ImageMagick

Ex: convert eye.gif news.gif -append storm.gif tree.gif \
            -background skyblue +append result.gif

Argument                          Action Performed                                Images
----------------------------------------------------------------------------------------
convert             Initialize and Create an empty 'image list'                   empty seq
eye.gif             Read in image and add to end of current image list            1 image
news.gif            Add a second image into list (now with two images)            2 images
-append             Take all images in current list, and append vertically.
                    All images are replaced by a single image.                    1 (merged)
storm.gif           Add another image to the image list                           2
tree.gif            And another                                                   3
-background skyblue Set a 'background color' to be used later.
                    No changes are made to any images.                            3
+append             Join all 3 images in the list horizontally
                    Current background color is used to fill the empty space      1 (merged)
result.gif          As this is last argument, an explicit -write operation is
                    performed with this argument. The single image in the current
                    image the list is written using the given filename and image
                    file format.                                                  written

Command Line:
1. + input filenames.
2. * image settings.  Ex: ‑adjoin, ‑affine, ‑alpha, ‑antialias # an image setting stays in effect until it is reset
3. * image operators. Ex: ‑contrast, ‑convolve, ‑crop, ‑cycle  # an operator is applied to an image and forgotten
4. * image sequence operators.
5. * image stacks.
6. ? output image filenames (required by convert, composite, montage, compare, import, conjure).

Print image dimensions:
identify -format '%w x %h (width x height)\n' image.jpg

Create tiles (a single image) from multiple images, keeping original sizes:
montage -geometry +1+1 *.(jp|pn)g all.jpg
montage -geometry 333x333+1+1 *.jpg all.jpg # set all images to the same size
----------------------- File -----------------------
help/install.txt
Use make install (in /usr/local)
vs sudo make install

Avoid making local installs into system directories. The system directories eg
/usr, are reserved for the package manager (yum, apt, ...)
so it's best to leave things pristine
----------------------- File -----------------------
help/ip.txt
# Show
ip -4 a | ifconfig -a

# Bring up
ip link set eth0 up | ifconfig eth0 up
ifup eth0

# IP
ip a add 192.168.0.77/24 dev eth0 | ifconfig eth0 192.168.0.77 [up]
                         \                                     \
                          broadcast ...                         netmask ... broadcast ...

                                    ifconfig eth0 netmask ...
                                    ifconfig eth0 broadcast ...

# Alias
ifconfig eth0:0 192.168.0.77/24 up
ifconfig eth0:0 down

ip a add 192.168.0.77/24 dev eth0:0
ip a add 192.168.0.77/24 dev eth0 label eth0:0

# Routing
ip route add 192.168.0.77 via 192.168.0.1 | route add 192.168.0.77 gw 192.168.0.1
----------------------- File -----------------------
help/ipsec.txt
Site to Site VPN

                 local        VPN tunnel        remote
       VPN domain - Security GW ====== Security GW - VPN domain
       1.2.3.4        1.2.3.1            5.6.7.1        5.6.7.8

/etc/ipsec.conf:
ike esp from { local domain } to { remote domain } local GW peer GW

Notes:
- the VPN/encryption domain could be a single IP or a whole net (ex: London offices)

Example:
ike esp from { ... } to { ... } local ... peer ... \
  main  auth hmac-sha1 enc aes-256 group modp1024 lifetime 8640 \ # <- phase 1 mode and cryptographic transforms
  quick auth hmac-sha1 enc aes-256 group modp1024 lifetime 3600 \ # <- phase 2 cryptographic transforms
  psk "XXXXXXXXXXXXX"

- modp1024: DH group 2 (dh2)
- For phase 2, Perfect Forward Secrecy (PFS) is enabled unless group none is specified
----------------------- File -----------------------
help/iptables.txt
                                    iptables

Extensions

-m module
 load this module

example:
-m tcp
 brings --sport and --dport

note:
-p module -m module is redundant,
-p module will auto load module IF an option from the module is used
ex: -p tcp --dport 22 is enough

Policy chains

→ INPUT:   incoming connections
⇒ FORWARD: incoming connections going forward (connections to routers only)
← OUTPUT:  outgoing connections

NB: for ping to succeed it will need BOTH the input/output chains. Same for ssh...

Policy chain default behavior - ACCEPT (check with iptables -nvL)
change with:
iptables --policy INPUT DROP
iptables --policy OUTPUT DROP
iptables --policy FORWARD DROP

  DROP - drop the connection, no notification to source =>
                              source is unaware of our system
REJECT - drop the connection, notify source

-A: append rules to an existing chain
-I: [chain] [number] - insert rule to a specific place in the list of rules
                       giving it higher priority

Examples:
iptables -A INPUT -s 10.10.10.0/24 -j DROP
iptables -A INPUT -p tcp --dport 22 -j DROP # block from any IP (--dport ssh works too)
iptables -F # flush all rules

note: omit iptables when using the config file (/etc/sysconfig/iptables)

States:
# Allow 2 way ssh communication but the connection can be established inbound
# only (NEW). Outbound is ok as long as the connection's been already established!
iptables -A INPUT -p tcp --dport ssh -s 10.10.10.10 -m state --state NEW,ESTABLISHED -j ACCEPT # allow from 10.10.10.10 to OUR ssh port!
iptables -A OUTPUT -p tcp --sport 22 -d 10.10.10.10 -m state --state ESTABLISHED -j ACCEPT

Save changes: (else lost on restart)
iptables-save # Ubuntu
service iptables save # CentOS

Tables:
filter, raw, mangle, and nat

NAT example:
iptables -t nat -A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j REDIRECT --to-port 1443
iptables -t nat -A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j REDIRECT --to-port 1480

Flush existing rules in memory and load a new set: (all in this case)
iptables-restore < /etc/sysconfig/iptables
# From memory to file
iptables-save > /etc/sysconfig/iptables

# [ number of packets : number of bytes ], [in:out] packet counter
:INPUT DROP [0:0]

# Act on that table
*filter
# Set default policy
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
# Create an empty chain <=> iptables -N RH-Firewall-1-INPUT
:RH-Firewall-1-INPUT - [0:0]

Jump to the RH-Firewall-1-INPUT chain and traverse all rules ↓,
then come back to INPUT and traverse all rulles too ↓
-A INPUT -j RH-Firewall-1-INPUT

-A FORWARD -j RH-Firewall-1-INPUT

Annexe:
https://github.com/QueuingKoala/netfilter-samples/blob/master/rules-edge-router/iptables.rules
----------------------- File -----------------------
help/irc.txt
                                      IRC

List the admin users of a channel:
   /msg chanserv access #bash list

Login:
   /msg nickserv identify ********
----------------------- File -----------------------
help/j.txt
   J:   atom,   list,  table
math: scalar, vector, matrix

Ranks #$

   i.2 3 4

 0  1  2  3
 4  5  6  7
 8  9 10 11  NB. 2 planes
             NB. 3 rows
12 13 14 15  NB. 4 columns
16 17 18 19
20 21 22 23

"0 is the same as "+ since + 0 0 0

Compositions

@: at

  [: f g                   capped fork, useful in building long trains
  (f @: g) y  =  f (g y)   monad and monad
x (f @: g) y  =  f (x g y) monad and dyad

&: appose

  (f &: g) y  =  f (g y)       monad and monad
x (f &: g) y  =  (g x) f (g y)  dyad and monad

@ atop
& compose

are used for tracking: applications of f track the applications of g

&. under

  (f &. g) y  =  g inv f g y NB. inv is ^:_1

Trains

  (f g) y  =  y f (g y) monadic hook
x (f g) y  =  x f (g y)  dyadic hook

  (f g h) y  =    (f y) g (h y)   monadic fork
x (f g h) y  =  (x f y) g (x h y)  dyadic fork

4-train   (e f g h) => hook:   (e (f g h)) NB. (verb fork)
5-train (d e f g h) => fork: (d e (f g h)) NB. (verb verb fork)

_9: to 9: - constant functions are useful because they can occur in trains

Operators: adverbs and conjunctions

Verbs

       1                 2  <- fret (negate to not include the fret)
<;._2 '-aa-bb-cc 11-22-33 '

cut, using the fret as a separator, then box the results

Puzzles

f(y) if y > 1, else y
f ^: (>&1) <=> ] ` f @. (>&1)
----------------------- File -----------------------
help/jenkins.txt
node: machine/container. e.g the controller + agents

// Jenkinsfile, declarative pipeline
pipeline {
    agent any
    stages {
        stage('build') {
            steps {
                sh 'python3 --version'
            }
        }
    }
}
----------------------- File -----------------------
help/jira.txt
Filters:

- Fixed issues
(assignee = currentUser() OR assignee was currentUser() OR assignee changed from currentUser()) AND (resolution = Fixed OR status = Closed) ORDER BY updated DESC

- Open issues
assignee = currentUser() AND resolution = Unresolved ORDER BY updated DESC

- Reported issues
reporter = currentUser() ORDER BY updated DESC

- This week
(assignee = currentUser() OR assignee was currentUser() OR assignee changed from currentUser()) AND (created >= startOfWeek() OR updated >= startOfWeek() OR resolved >= startOfWeek()) ORDER BY updated DESC
----------------------- File -----------------------
help/jmespath.adoc
:toc: left
:toclevels: 3

= JMESPath

== Specification

=== literals
literals (including numbers, e.g \`5`) must be 'quoted' or \`quoted`

=== projections
`null` values are not included in the final list (unlike multiselect)

==== list: `id[*]`

`id[?...]` (filters):

 $ aws ec2 describe-images --owners self --query 'Images[?Description != None && contains(Description,`ed`)].[ImageId,Description]' --output table

==== object: `id.*`
create a list with the objects' values:

 "ops": {                        \         \    [
   "functionA": {"numArgs": 2},   \  ops.*  \     {"numArgs": 2},
   "functionB": {"numArgs": 3}    /         /     {"numArgs": 3}
 }                               /         /    ]

=== function evaluation: `&`

arguments are evaluated before calling a function

`sort_by(Contents, &Date)`

 Date would return null (it's not in the outer hash)
&Date will be evaluated later

 {
   "Contents": [
     {
       "Date": "...",
       ...
     },
     {
       "Date": "...",
       ...
     }
   ]
 }

=== Current element: `@`

`aws iam list-account-aliases --query 'AccountAliases[*].{Alias: @}'`

  [              \   [
      "alias1",   \      { "Alias": "alias1" },
      "alias2"    /      { "Alias": "alias2" }
  ]              /   ]

== Integrations

=== aws cli outputs

 $ aws kms list-aliases --query 'Aliases[*].AliasName'                        # list of scalars
 $ aws kms list-aliases --query 'Aliases[*].[AliasName]'        --output text # list of lists   => newlines
 $ aws kms list-aliases --query 'Aliases[*].{Alias: AliasName}' --output text # list of objects => newlines

=== jmespath.search()

 >>> jmespath.search('ls[*][1]', {"ls":[[1,2],[1,2]]})
 [2, 2]                          +-- dict
                                 +-- json.load(file)
----------------------- File -----------------------
help/jq.txt
Use identity to get a pretty output:
curl ... | jq .

Arrays: []

  * .[] - get all
  * .[0]

.foo | .[] is the same as .foo[]

Objects: {}

  * .foo.bar <=> .foo | .bar
                   \
                    +-- filter (data IN, data OUT)

Responses can be constructed like so:
{ "site": .Name, "IP": .Value }

.foo, .bar
     \
      +-- concatenate both value streams
      +-- surround with []s and use -r to get on the same line

Examples:

% jq -r '.solutions|.[]|select(.lang=="perl"and.scoring=="bytes")|"# "+.hole,.code+"\n"' code-golf-export.json

% aws route53 list-resource-record-sets --hosted-zone-id ... |
  jq -r '.ResourceRecordSets | .[] | if .Type == "A" then [.Name, (.ResourceRecords | .[] | .Value)] else empty end | @tsv'
  jq -r '.. | .ResourceRecords? | .[]? | .Value'
          \                           \
           +-- like // in xpath        +-- ignore errors
----------------------- File -----------------------
help/kak.txt
Print the value of an option
:echo %opt{filetype}

Two ways of displaying the same thing
echo %sh{echo $kak_selections}
echo %val{selections}

status line
unnamed0@[.......] -> clientname@[session id]

"%val{bufname}[%opt{filetype}] %val{cursor_line}:%val{cursor_char_column}"

:e -scratch *scratch* (or any name)
creates a buffer that you can simply discard with :q without !

addhl group <name> <- container... => regex under will only match within that container

...for subgroups:
addhl -group <name> group <subname>
                \
                 if name is /, it references the shared highlighters group (windows on the same filetypes)
                 addhl ref <name> (reference a shared highlighter in this window)

...and highlighters:
addhl -group <name>(/<subname>) highlighter

                                this is a group name => thus we can add subregions:
                                addhl -group <name>/<region_name>
                               /
addhl regions -default code <name> <region_name1> <opening1> <closing1> <recurse1>  \
                         \         <region_name2> <opening2> <closing2> <recurse2>...
                          +->      <code>: default region/group name when none matched

                               ex: region_name { } { <- } closes only once all
                                   the recurse { have been closed

Ex file:
#include <...         / code
                      \
func                  / reg 1
                      \
print                 / code
                      \
bla                   / reg 2
                      \
//                    ...


:exec wwd
selection moving, as if I did wwd myself
:exec -draft wwd
selection will stay where it is, but the next word will be deleted

-itersel evaluates the commands for every selections individually, instead of
all together in the same context. The main reason for that is to avoid selections
getting merged when they overlap.
So, if you have 3 selections, instead of applying the keys/commands as if you
had 3 selections, it will apply the keys/commands 3 times, first time starting
with first selection only, second time with second selection only, ...

<tab>, <a-i> are key names unlike
\n, \t which are character names

* write the current buffer state in a temporary file
* pass this file to your linter and capture the output
* parse the linter output, and fill 2 options
  - a line_flags option, used by a flag_lines
highlighters to display a gutter
  - an option storing the line -> diagnostic mapping,
in whatever format you can easily parse afterwards
clang-diagnositcs-next/clang-show-error-infos parse
that second option to respectively jump to the next
line on which there is a diagnositcs, and display an
info box for the diagnostic on the current line
----------------------- File -----------------------
help/keepalive.txt
- Reuse the same tcp connection for HTTP traffic
- "Connection: Keep-Alive" vs "connection: close" http header
- httpd.conf: KeepAlive On

NB: keepalived is not keepalive!
    It's a loadbalancing linux daemon similar to relayd
----------------------- File -----------------------
help/keytool.txt
keytool -genkey -keyalg rsa -keysize 2048 -alias ALIAS -keystore Data.jks

keytool -list -keystore Data.jks

keytool -export -alias ALIAS -file ALIAS.pub -keystore Data.jks
keytool -import -file ALIAS.pub -alias ALIAS -keystore Data.jks

keytool -delete -alias ALIAS -keystore Data.jks
----------------------- File -----------------------
help/kubernetes.txt
nodes
* ECS instances
* master/worker + kubelet (ECS agent)

pods - ECS tasks but pods contain several containers

deployments - ECS ASG + HA

services connect to pods with label selectors, they LB between pods
----------------------- File -----------------------
help/last.txt
login history

last -n10

last user
last reboot

lastb # last bad
----------------------- File -----------------------
help/ldapsearch.txt
ldapsearch -x -D 'cn=Manager' -W -b 'dc=example,dc=com' dn

ldapsearch -x -LLL '(|(uid=flname)(cn=flname)(memberUid=flname))' dn

(&(filter)(filter)(filter)...)
(|(filter)(filter)(filter)...)
(!(filter))
----------------------- File -----------------------
help/less.adoc
== search
`ESC-u`:: undo search highlighting
`&pattern`:: show only matching lines (`&CR` to stop)

== options
`-S`:: chop long lines
`-N`:: display line numbers
----------------------- File -----------------------
help/libvirt.txt
                     Hypervisors (virtual machine monitors)

native, bare metal hypervisors: (KVM)
hardware -> hypervisor -> guest OSs

hosted hypervisors: (VMware, VirtualBox, QEMU (Quick Emulator), KVM)
hardware -> OS -> hypervisor -> guest OSs

KVM (Kernel Virtual Machine) is a Linux kernel module that allows a user space
program to utilize the hardware virtualization features of various processors
QEMU uses emulation; KVM uses processor extensions (HVM) for virtualization.

Libvirt

System URIs connect to the system libvirtd daemon (started by system
init scripts), which is shared among all users on the machine, and which
allows access to anything that requires superuser capabilities (such as
creating tun devices for networking).

Session URIs connect to a per-user daemon (which will be started
automatically if needed), and which cannot access anything that the user
cannot access (and unfortunately, given that no one has yet figured out
how to make Linux share tun devices with ordinary users, that means that
session guests are currently rather limited because they will not have
networking support). The goal is to make things support full
virtualization under session URIs, once the kernel adds the support we
need for networking.

# uuid: globally unique identifier for the virtual machine, generated if omitted
# acpi on/off? acpid would be needed (libvirt tips)
#
# --os-variant is HIGHLY RECOMMENDED:
# osinfo-query os
# virt-install --os-variant=list # for older versions
#
# full virtualization (--hvm) is the default
# virsh desc domain
# virsh list --title --all
# virt-install --memory=?
# virsh domdisplay
# --disk vol=images_lvm/home,size=64,sparse=no,format=lvm2 \
# --extra-args console=ttyS0,115200 \
# --console pty,target_type=serial \
#
# fallocate -l64G centos7.img

# virsh pool-define-as images_lvm logical - - /dev/sda2 cl /dev/cl
# virsh pool-autostart images_lvm

virt-install \
--name xxx \
--memory 4096 \
--metadata title='xxx',description='xxx' \
--events on_crash=restart,on_poweroff=destroy,on_reboot=restart \
--vcpus 2 \
--features acpi=off \
--clock offset=utc \
--location http://mirror.cov.ukservers.com/centos/7/os/x86_64/ \
--os-variant centos7.0 \
--boot hd \
--disk /home/libvirt/images/centos7.img,sparse=no \
--network bridge=virbr0 \
--graphics vnc,port=5901,password=XXX \
--hvm \
--virt-type kvm \
--dry-run

Change virbr0 to br0 later with virsh edit your_domain + manually set the network via VNC

VNC:
command ssh -fNL 5901:127.0.0.1:5901 remote_host
screen sharing: vnc://127.0.0.1:5901
----------------------- File -----------------------
help/limits.txt
/etc/security/limits.conf

who          type          what         value
usr,@grp,* | soft,hard,- | core,nproc | unlimited,infinity,-1
              \         \               0
               \         both
                default that can be adjusted, unlike hard

example:
* hard core 0 # core dump size limited to 0 for everyone

you might need to allow setuid processes to dump cores:
sysctl -w fs.suid_dumpable = 1

/proc/PID/limits

# grep '^Limit\|core' /proc/$(pgrep -f Rocket)/limits
Limit               Soft Limit  Hard Limit  Units
Max core file size  0           unlimited   bytes

prlimit:
get/set process resource limits.
use if a process shouldn't be restarted.

prlimit -pPID -cunlimited

core dump:
core memory (RAM) dump of a program

location of core dumps: (default /proc/PID/cwd/)
sysctl kernel.core_pattern

ulimit:
manage current shell resources
----------------------- File -----------------------
help/linux_arch_xfce_install.txt
https://wiki.archlinux.org/index.php/beginners%27_guide#Installation

1. ping -c3 www.google.com

2. Make the install process friendlier :) {{{1

loadkeys uk # /usr/share/kbd/keymaps/
systemctl start gpm

3. Partitioning of a 500GB hard drive {{{1

disk=sda
fdisk/gdisk /dev/"$disk"

(d)elete all partitions

* boot: /dev/sd_1: (n)ew, (p)rimary, +1G, (t)ype ef00, (a) bootable
* swap: /dev/sd_2: (n)ew, (p)rimary, +6G, (t)ype 8200
* root: /dev/sd_3: (n)ew, (p)rimary, +20G
*  LVM: /dev/sd_4: (n)ew, (p)rimary,      (t)ype 8e00

(p)rint, (w)rite

Note: (p)rimary and (a) are not relevant with gdisk
}}}1

4. reboot # or the new partitions are not seen!

5. Creating Filesystems {{{1

* mkfs.fat -F32 /dev/"$disk"1 # boot (UEFI system)
  OR
  mkfs.ext2 /dev/"$disk"1 # boot (BIOS system)

* mkswap /dev/"$disk"2
  swapon /dev/"$disk"2

* mkfs.ext4 /dev/"$disk"3 # root

* pvcreate /dev/"$disk"4
  vgcreate vdisk /dev/"$disk"4
  vol() { lvcreate -L"$1"G -n"$2" vdisk && mkfs.ext4 /dev/vdisk/"$2"; }
  vol 210 home
  vol 11 var
  vol 3 opt

Checks: fdisk -l, lsblk
        pvdisplay, pvscan...

6. Mount the partitions {{{1

mount /dev/"$disk"3 /mnt # root

for i in boot home var opt
do
   mkdir -p /mnt/"$i"
done

mount /dev/"$disk"1 /mnt/boot

for i in home var opt
do
   mount /dev/vdisk/"$i" /mnt/"$i"
done

7. Mirror list {{{1

vi /etc/pacman.d/mirrorlist
# :g/united\s\+kingdom\n.\+\c/,+m0
# then reverse from 2 to 1
#                   1    2
# Note: vi don't recognise \n and \c, so /United Kingdom, dj, {, P...
}}}1

8. pacstrap -i /mnt base # exclude these: ^22,^31-32,^36,^40,^52

9. fstab {{{1

genfstab -U -p /mnt >> /mnt/etc/fstab
cat /mnt/etc/fstab # to check
}}}1

10. arch-chroot /mnt

11. Local settings {{{1

pacman -S ed gvim ttf-dejavu ctags # + repeat step 2.
# \\1 - \\ needed so $'' doesn't parse it
ed -s /etc/locale.gen <<< $'H\n%s/#\(bg_BG\|fr_FR\|es_ES\|en_GB\|ru_RU\).UTF-8/\\1.UTF-8/\nwq'

locale-gen

echo LANG=en_GB.UTF-8 >> /etc/locale.conf
export LANG=en_GB.UTF-8

loadkeys uk
echo KEYMAP=uk >> /etc/vconsole.conf

ln -s /usr/share/zoneinfo/Europe/London /etc/localtime

hwclock --systohc --utc

12. Networking {{{1

echo myhostname > /etc/hostname

systemctl enable dhcpcd.service # systemctl enable dhcpcd@interface_name.service
# Or: systemctl stop dhcpcd.service && systemctl disable dhcpcd.service
#     systemctl enable dhclient.service && systemctl start dhclient.service

13. GRUB {{{1

pacman -S grub efibootmgr dosfstools # only grub is needed for a BIOS system

# UEFI system
efivar -l # test if booted into UEFI mode
mount -t efivarfs efivarfs /sys/firmware/efi/efivars
grub-install --target=x86_64-efi --efi-directory=/boot --bootloader-id=grub --recheck # --debug
OR
grub-install /dev/"$disk" # BIOS system

# pacman -S os-prober # for a dual boot setup (doesn't detect Windows)
grub-mkconfig -o /boot/grub/grub.cfg

14. Finish {{{1

passwd
exit
umount -R /mnt
reboot

Post install {{{1
============

useradd -m mitko
passwd mitko

# gvfs for automounting
# User switching: (deprecated)
# ln -s /usr/lib/lightdm/lightdm/gdmflexiserver /usr/bin/gdmflexiserver
pacman -S \
xorg-server xorg-server-utils xorg-xinit xscreensaver \
xfce4 xfce4-notifyd gvfs orage ristretto ffmpegthumbnailer \
lightdm lightdm-gtk-greeter accountsservice numlockx \
clipit xclip wgetpaste \
firefox flashplugin \
sudo keychain x11-ssh-askpass \
git openssh bash-completion mlocate ntp tree bc

lspci | grep VGA # -> XXX
pacman -Ss xf86-video | grep -i -C1 --color=auto XXX
pacman -S xf86-video-XXX

pacman -S alsa-utils # for alsamixer

# optional
pacman -S cups vlc

# Distro / Desktop Environment specific
pacman -S --needed base-devel

# Work
pacman -S \
dnsutils rsync clusterssh openldap virtualbox thunderbird tcpdump terminator \
xterm cifs-utils

# Services
systemctl enable ntpd lightdm

Settings
--------

# AUR: /etc/makepkg.conf
CFLAGS and CXXFLAGS: -march=native
MAKEFLAGS: -jn (with n being the output of nproc)
BUILDDIR=/tmp

# pacman: /etc/pacman.conf
color
VerbosePkgLists

# Terminal colors
Palette:
tango

Modifications:
cursor: #666666, rgb(102, 102, 102)
  text: #bebebe, rgb(190, 190, 190)
    bg: #262626 (html color grey15), rgb(38, 38, 38)

# Orage
Line 1: %a %d %b
Tooltip: %A %d %B %Y

# gVim shortcut
gvim -u /home/"$USER"/.vimrc -U /home/"$USER"/.gvimrc -f %F

# Git
"$REPOS_BASE"/config/git.bash # run config script
----------------------- File -----------------------
help/linux_iphone.txt
mkdir -p /tmp/iphone && ifuse /tmp/iphone
fusermount -u /tmp/iphone
----------------------- File -----------------------
help/ln.md
# Broken link

```bash
ln -s ad.1 /man/man1/active-dir # /man/man1/: active-dir -> ad.1
```

We now have a broken link since `ad.1` isn't a valid path from within `man1/`,  
_ln_ stores the path we give literally, without any check

_Solution:_ use `~/ad.1` which is resolvable from within `man1/`

# Make sure we create a link named _man1_, NOT a link within a folder named `man1/`

```bash
ln -sT ~/ad.1 /man/man1 # /man/: man1 -> ~/ad.1
```

_Race condition:_ if `man1/` gets created before _ln_ finishes, ensure _ln_ fails, we don't want a link inside `man1/`

# We do now want our link to go inside `man1/`

```bash
ln -st /man/man1/ ~/ad.1 # /man/man1/: ad.1 -> ~/ad.1
```

_Race condition:_ if `man1/` gets deleted before _ln_ finishes, ensure _ln_ fails, we don't want a link named _man1_
----------------------- File -----------------------
help/locate.txt
Indexing when using an encrypted home directory

Because ~ is BIND mounted and of type ecryptfs so normal updatedb won't work unless:
PRUNEFS-=ecryptfs and PRUNE_BIND_MOUNTS='no'

/etc/zsh/zprofile
export LOCATE_PATH=/home/user/var/mlocate.db

/etc/updatedb.conf
PRUNEPATHS = "... /home/.ecryptfs"

Index your home files:
updatedb -l 0 -o ~/var/mlocate.db -U ~
TODO: run on a daily basis

Tips

locate / # all indexed files since absolute paths are recorded
----------------------- File -----------------------
help/logrotate.txt
- logrotate is triggered by cron, usually from /etc/cron.daily/logrotate:

  #!/bin/sh
  /usr/sbin/logrotate /etc/logrotate.conf # main config
  ...
  include /etc/logrotate.d # extra configs

- debugging:

  logrotate -df /etc/logrotate.d/...
             |+-- force
             +-- debug (dry run with implied -v)

  NB: options specified in /etc/logrotate.conf won't be read! We would need to
      run our test with logrotate -df /etc/logrotate.conf for that. Any missing
      options in /etc/logrotate.d/... must be copied over for the test.

  notes:
  * make sure crond is running and there aren't duplicate instances
  * check:
    /var/log/cron
    /var/lib/logrotate.status

- syntax:

  /var/log/httpd/access.log
  /var/log/httpd/error.log
  {
    rotate 5
    ...
  }

- copytruncate vs create:

  copytruncate should be used with services like apache that keep files open
  continuously. Such services won't notice the file has been renamed (ex:
  error.log -> error.log.1.gz) and will keep writing to the old file descriptor
  now pointing to error.log.1.gz. copytruncate won't change the file
  descriptor, it will simply copy the contents of the current file to a new
  file (error.log.1.gz) then truncate to 0 the current one and continue to
  write there. If in your prerotate script you stop the service there is no
  point using copytruncate since when the service starts it will open new file
  descriptors (the newly created error.log).

- rotating on size:

  minsize: size and time (ex: 10M + daily)
    * DANGER: even if size is reached before time, no rotation.

  maxsize: size or time
    * sub optimal: even if size isn't reached at time, rotation.

  size: size
    * DANGER: time is ignored but make sure logrotate runs often enough or size could
      grow above it's threshold.

  possible solution for 'minsize' and 'size'
  ------------------------------------------
  add a script in /etc/cron.hourly:
  /usr/sbin/logrotate -s /var/lib/logrotate.status /opt/graphite/conf/logrotate-carbon.conf

  When using dateext, if size grows above threshold, rotation will be skipped
  till the next day with an error message reading that file /var/log/----.Y-m-d
  already exists. dateformat must be used in that case:

  dateext
  dateformat .%Y-%m-%d_%H-%s

- logrotate wrongly sets up $HOME to / so you are screwed if you use
  $HOME in your init scripts
----------------------- File -----------------------
help/lsof.txt
                                      lsof

lsof                          - procs for all files
lsof /var/log/httpd/error_log - procs for a single file
lsof +D /var/log/             - procs for files in a dir # slow because it recurses through the filesystem ~ fuser -mv /var/log/
lsof -c ssh                   - files opened by a command
lsof -u ^oge                  - files not opened by user
lsof -p 1911                  - files opened by PID
lsof -i :25                   - internet procs listening on port
                                (standard port syntax as in: http://localhost:25/)
lsof -P -n
      \  \
       \  *-- numeric hosts
        *- numeric ports

Note: options are ORed by default, use -a for AND

lsof -i -sTCP:LISTEN # netstat -nltp

lsof -FR0 # list pids and ppids only separated by nulls (vs \n)
          # p12765R63987

List open files that are completely unlinked. To free the space they take, kill
the processes that hold to them.
lsof +L1
----------------------- File -----------------------
help/macosx.txt
                            Mac OS X for Linux users

brew:
ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"

zsh:
brew install zsh
echo /usr/local/bin/zsh >> /etc/shells # as root (sudo su -)
chsh -s /usr/local/bin/zsh

GNU replacements for some BSD commands:
https://www.topbug.net/blog/2013/04/14/install-and-use-gnu-command-line-tools-in-mac-os-x/

PATH setup for the below: https://github.com/kurkale6ka/zsh/blob/master/.zshrc#L60-L74

brew install coreutils # to get gls...
brew install ed
brew install findutils # to get glocate...
...
https://github.com/kurkale6ka/scripts/blob/master/mkconfig

nvim:
brew tap neovim/neovim
brew install --HEAD neovim
sudo easy_install pip
pip2 install --user neovim # even root is not allowed to write to some locations!

Updating formulas:
brew update && brew upgrade # as your normal user
brew reinstall --HEAD neovim

fzf:
Don't use brew
https://github.com/junegunn/fzf/wiki/On-MacVim-with-iTerm2

locate:
* updatedb from findutils is different than its Linux counterpart. It doesn't use /etc/updatedb.conf. Run it like so:
* updatedb --prunepaths='/afs /media /mnt /net /sfs /tmp /udev /var/cache /var/lock /var/run /var/spool /var/tmp /Recovered.Items /.DocumentRevisions-V100 /.MobileBackups /.Spotlight-V100 /.Trashes /.fseventsd /Groups /Shared /Shared.Items /private/tmp /private/var/cache /private/var/lock /private/var/run /private/var/spool /private/var/tmp /Applications /Library /System'

  /.DocumentRevisions-V100 (equivalent of .git for Versions)
  /.Spotlight-V100         (spotlight indexes)

* Replace spaces with dots because of some oddity related to updatedb using regexes

* cron jobs
http://alvinalexander.com/mac-os-x/mac-osx-startup-crontab-launchd-jobs

iTerm2:
-------

tmux ^←|→ not working:
disable keyboard shortcuts: mission control/move left|right a space

Extending selection in vim not working:
Preferences/Pointer: add Ctrl to the right button single click

Shortcuts:
            fn ← - Home
            fn → - End
         fn <bs> - del
           Esc . - Alt .
           alt 3 - #
           ⇧ ⌘ g - open location bar in Finder (so you can go to /usr/local say)
Ctrl + Shift + ↑ - PageUp
Ctrl + Shift + ↓ - PageDown
       ⇧ ⌘ Enter - maximize window

Custom layout:
Window/Restore Window Arrangement

Finder:
-------
search - kind:image|movie|png...
space bar to preview images in a slideshow

cmd + click on URL opens empty new window:
problem: browser's been updated but not relaunched

Become root:
------------
1. su - admin # ask ops for password
sudo su - will only work if you have admin access thus you are in the sudoers file

2. sudo su -
su - won't work as the root pass isn't known/setup by default

Expand Google Chrome Horizontally and Vertically:
Alt+Shift and click the green button (alt alone expands vertically only)

Change terminal hostname:
sudo scutil --set HostName aldebaran.taurus
----------------------- File -----------------------
help/mail.txt
                           Sender Policy Framework
policy (aka SPF record) defining mail servers authorized to send from a domain

Allow 3rd party to send emails on our behalf:

mydomain.com
TXT - "v=spf1 ip4:mxserver1 ip4:... include:spf.3rdparty.com -all"

NB: the replies come back to our mail servers so we also need to create mail
    accounts for 3rd party.

Check recipient for TLS support:
% dig eurobpo.com mx
eurobpo.com. 3600 IN MX 0 eurobpo-com.mail.protection.outlook.com.

% openssl s_client -connect eurobpo-com.mail.protection.outlook.com:25 -starttls smtp

or

send a test email, then check logs/headers
----------------------- File -----------------------
help/man.md
# View local page
`man -l app.1`

# Groff
```bash
.RB [ \-b | # roman (normal text) / bold alternating => [ will be rendered as is, -b will be bold, | normal
.B Hello \fIWorld\fR # inline markup with \fX...\fR

# bullet points
.PD 0 # for no extra newlines
.IP \(bu 2
item1
.IP \(bu 2
item2
```
_Ref_: `man groff_man`

# Misc
```bash
LESS= man manpage # -F was the problem
nroff -man <(zcat /usr/share/man/man1/env.1.gz) | LESS= less
groff -man -Tascii <(zcat /usr/share/man/man1/env.1.gz) | LESS= less

man -M/Users/XXX/.fzf/man fzf
```
----------------------- File -----------------------
help/math/ideal
Even numbers is an ideal in the ring Z

Addition and subtraction preserves evenness,
and multiplying an even number by any integer results in another even number
----------------------- File -----------------------
help/misc/birthdays.md
# January
 5 Светла
 7 Ивето (_Имен ден_)
18 Наско (_Имен ден_)

# February
 7 Роси
18 Баба Мария
20 Майка
22 Дядо Веско

# Mars
17 Леля
26 Сара

# May
 9 Jérôme
22 Пепа
27 Бени
30 Ивето

# September
2 Наско

# November
11 Демона
12 Sébi
16 Нели
19 Татко
28 Баба Лука
----------------------- File -----------------------
help/misc/bodybuilding_breakfasts.txt
A rule of thumb for most people is to get 20 to 30 grams of protein at a meal.
This can be particularly difficult during breakfast. To get started, here are
seven examples of higher protein breakfasts:

1. Day One Toast with nut butter: Two slices of whole-wheat bread with 1
   tablespoon of peanut butter on each, topped with sliced banana. One cup of
   skim milk to drink. Total: 22 grams of protein.

2. Day Two Strawberry smoothie: Blend together 1/2 cup of strawberries, 6
   ounces of plain Greek yogurt, 1/4 cup of uncooked oatmeal, a drizzle of
   honey (as needed) and 1/2 cup of skim milk or soy milk. Total: 21 grams of
   protein.

3. Day Three Mediterranean sandwich: Whole-wheat pita with 4 tablespoons of
   hummus, tomato slices, 1 ounce of goat cheese and 1/4 cup of sliced almonds.
   Have a café latte to drink, made with 1/2 cup of steamed skim milk. Total:
   22 grams of protein.

4. Day Four Melon bowl: Half of a cantaloupe (using the center as a bowl),
   filled with 1 cup of cottage cheese. Total: 25 grams of protein.

5. Day Five Breakfast burrito: Corn tortilla filled with two scrambled eggs,
   sautéed onions, 1/4 cup of black beans and pico de gallo. Total: 25 grams of
   protein.

6. Day Six Apple walnut oatmeal: Cook 3/4 cups of dry oatmeal with 1 and 1/4
   cup of skim milk, and add 1/4 cup of chopped walnuts, plus 1 chopped apple.
   Sprinkle with cinnamon and drizzle with honey. Total: 24 grams of protein.

7. Day Seven Salad for breakfast: Toss together 1/2 cup of shelled soybeans,
   1/2 cup of chopped tomato and 1 ounce of mozzarella cheese. Drizzle with
   balsamic vinegar, and serve a whole-wheat breadstick on the side. Total: 25
   grams of protein.
----------------------- File -----------------------
help/misc/bodybuilding_shawn.txt
Shock Treatment: {{{1

Five shocking principles for outrageous growth

When you first start training, you make phenomenal gains. Your hitherto
untrained body is so unused to the stresses of weight training that it is forced
to respond with a rapid increase in muscle size and strength. That's the upside
for the beginner.

Eventually, however, your body adapts to the level of training stress you are
exposing it to; your progress begins to slow and perhaps even stop. This
syndrome is compounded by the fact that most of you who have the determination
to go to the gym four or five times a week also like to have a clear-cut routine
when you get there. You religiously follow a rigid six- or seven-day split--the
same exercises, in the same order, using the same weights and reps.

Well, guess what? That's the best way to make sure you continue to look the
same. Once you've hit a plateau, you won't make progress unless you change your
approach. Your training needs to evolve if you want your body to follow suit.

That's what my style of training--I call it "shock treatment"--is all about. It
allows you to constantly make improvements in your physique by throwing
something new at your body every time you go to the gym. Your body never has the
chance to adapt under these circumstances and will respond to the ever-changing
stimulus by continuing to grow.

I follow this philosophy every time I walk into a gym. This is a thinking man's
program, not some cookie-cutter routine that spells out how many exercises, sets
and reps you need to perform. It's up to you to take the principles I explain
and apply them in designing a training regimen that suits your needs. If you
adopt this philosophy with total dedication from day one, you can build and
strengthen your body exactly the way you want to.

This is an exciting approach because you're doing something different at every
training session, and the variety keeps your internal fire burning. You never
know when that extra pump or max rep will hit if you don't challenge your
training by mixing things up and shocking your body. Push, pull, stretch,
squeeze--it's all the same, from beginner to advanced. In the end, you control
your bodybuilding destiny.

The principles I discuss should be applied to every bodypart, but I've
simplified matters by using chest as an example of how to integrate these
principles into your own bodypart split.

1 USE DIFFERENT EXERCISES EVERY WORKOUT {{{1

It's easy to use your favorite exercises as a crutch. I see some guys in the gym
bench pressing every day. If you don't know much about bodybuilding and how
muscles respond to training, it's probably easy to convince yourself that the
best way to build massive pecs is to train them day in and day out using the
same exercises.

A much better way to get your chest to grow, though, is to attack it with a
variety of exercises and incorporate sufficient rest and recovery into your
training cycle. For example, if my chest routine consists of bench presses,
incline dumbbell presses, dumbbell flyes and dips for one rotation, the next
time I work chest, I change up the exercises so I'm doing incline bench presses,
flat dumbbell presses, incline dumbbell flyes and dips. The third time, I might
go back to bench presses followed by incline Hammer Strength presses, dumbbell
flyes and cable crossovers.

The formula is that there is no formula. I change all my exercises every time I
go to the gym, for every bodypart. Not only does the variety keep me growing, it
keeps me focused on my goal. If you start every chest routine with bench
presses, your goal subtly starts to shift from increasing your muscle mass to a
comparison of how much weight and how many reps you can perform. That's a head
game that will eventually derail your training--and your success--as a
bodybuilder.

2 CHANGE YOUR EXERCISE ORDER {{{1

You'll also notice that the guy who bench presses every time he goes to the gym
also bench presses first. If he does any other chest exercise first, he won't be
able to bench as much weight and his ego will suffer. One of the best ways to
make gains in how much you bench press, though, is to place it further back in
your routine.

Too many beginning and intermediate bodybuilders buy wholeheartedly into the
concept that you have to start with mass building exercises, then work in detail
and defining exercises afterward. Followed exclusively, however, that philosophy
ignores a whole host of Weider Principles.

If, instead, you perform incline bench presses first, then flyes and finally
bench presses, of course you won't be able to lift as much weight on the flat
bench presses as you did when you placed them first. Your pectorals are already
pre-exhausted--a Weider Principle--and you're taking advantage of muscle
confusion--another Weider Principle. This means that your pecs are not adapted
to bench pressing at this point in your workout and will have to work that much
harder. You might not be able to lift as much weight, but you'll get far more
bodybuilding benefit from this new order of exercises.

Change your rotation every time out. I assure you that even though you might
perform the same number of sets--and might even use less weight--you'll get a
more trying workout and will feel like you've pushed your pecs to the limit.

I apply this principle to all bodyparts, including legs. Sometimes, I even end
my leg workouts with heavy squats, after I've done leg extensions, hack squats
and lunges.

3 CHANGE REP AND WEIGHT SCHEMES {{{1

Another favorite technique for shocking my muscles is to vary rep and weight
schemes from one training session to the next. For instance, if I train heavy
during the first routine, I perform four sets of eight reps; then, next time, I
choose four different exercises and work in the 12-15 rep range.

Regardless of which rep scheme I choose, I still pyramid up in weight. I know
ahead of time the target weight with which I can perform eight reps, reaching
failure on the last. For the first of the four sets, I may use only 50-60% of
that weight for eight reps. I could easily pump out 12 to 15 reps at that
lighter weight, but my goal for that day is to reach failure on the eighth rep
of that fourth set, so I hold back a little on the first set.

For the second set, for another eight reps, I use about 80% of my target weight.
For my third set, I use 90-100% of my target weight, depending on how I feel
that day. Then, for the fourth set, I use my target weight--and a spotter--to
make sure I pump out eight quality reps.

When I go into the gym again, I readjust my target weight so that I have a
poundage with which I can successfully complete 12 to 15 reps--instead of eight.
For instance, if I bench press in the higher-rep range, my target weight may be
only 80% of what it would be on eight-rep days. Once my target weight is
established, I use the same formula for pyramiding up in weight--about 50-60%
for the first set of 12-15, 80% for the second, 90-100% for the third and 100%
for the fourth set.

4 USE INTENSITY TECHNIQUES {{{1

One of the keys to increasing muscular gains is intensity. Here are a few
techniques to increase the intensity of individual sets. These can be excellent
tools for stimulating muscular growth--a couple of occasional more intense sets
really shock the muscle. Don't overuse these techniques, though, because you run
the risk of overtraining.

Rest-pause:For my last three sets, I choose the maximum weight with which I can
bench press 10 reps. I pump out a set of 10, rest for only 60 seconds, then do a
second set. After that, I rest again for just a minute before completing my
final set. Even though the weight doesn't change, the number of reps I'm able to
complete in each set decreases because the target muscle can't fully recover
before it is hit again.

Drop sets:This is an excellent technique to use for going past failure. When I
can't squeeze out another rep on my last set, I have a partner strip off about
40% of the weight, then I pump out as many reps as I can with the lighter
weight. The drop set should be performed with as little rest as possible. As
soon as the weight comes off, I'm good to go.

Forced reps:This may be the most intense technique of all because it allows for
no rest, and thus no recovery. When you reach failure, your partner assists as
you complete two or three more reps with perfect form. The tendency at this
point for most people is to squirm and try to use body English to lift the
weight, but that only encourages injury and negates the benefits of the
technique. You need to put 100% effort into using perfect form. Your spotter
should give you only enough assistance to keep the bar moving slowly. Do
everything else on your own.

As a rule of thumb, use one intensity technique every other training session. In
other words, during the first training session, you might use the rest-pause
technique for incline bench presses. The next time you hit your chest, don't use
an intensity technique. The third time you work your chest, you might use a drop
set to finish up your bench presses.

5 REVISE YOUR BODYPART SPLIT FREQUENTLY {{{1

For most people, falling into a rhythm is the best way to set up a bodypart
split. A lot of guys like a seven-day split, because it fits a busy schedule. In
a split like this, a person might work chest and triceps on Monday, back and
biceps on Tuesday, and so on.

The problem with this regularity is that the body accommodates to the training.
After you finish training your chest, your body expects to train triceps.

It's much better to shake it up. Follow a seven-day split for the first cycle,
then switch to a five-day split for the next cycle--training chest, shoulders
and triceps on day one; back, biceps and abs on day two; rest on day three;
train legs on day four; and do cardio on day five.

For the third cycle, return to a seven-day split but match up your bodyparts
differently. Train back and triceps on day one; quads and abs on day two, rest
on day three; train chest and biceps on day four; shoulders, hamstrings and
calves on day five; rest on day six; cardio and abs on day seven. The next cycle
might be a six-day split.

The point is that you shouldn't get too formulaic with the breakdown of your
bodypart split.

CONCLUSION {{{1

If you keep weight training the way you always have, your body will eventually
adapt and stop growing. The old bodybuilding adage is true: Every exercise
works, but none of them work forever. Change things up, and you can shape, build
and strengthen your body any way you desire. Give these five principles a try,
and you will get the shock of your life.

# vim: foldmethod=marker foldmarker={{{,}}}
----------------------- File -----------------------
help/misc/chess.txt
Portable Game Notation (pgn)
============================

The board is placed so that a white square is in each player's near-right corner

Tag pairs: [Name "Value"]
  Example: [Result "1-0"     (white wins)
                   "0-1"     (black wins)
                   "1/2-1/2" (draw)
                   "*"       (other, e.g., the game is ongoing)

Movetext in algebraic chess notation
------------------------------------

                                    file (a → h for white, h → a for black)

                                        ↑

8   ♜ ♞ ♝ ♛ ♚ ♝ ♞ ♜   1   ♖ ♘ ♗ ♔ ♕ ♗ ♘ ♖   →   rank (1-8 from white upwards)
7   ♟ ♟ ♟ ♟ ♟ ♟ ♟ ♟   2   ♙ ♙ ♙ ♙ ♙ ♙ ♙ ♙
6                     3
5                     4
4                     5
3                     6
2   ♙ ♙ ♙ ♙ ♙ ♙ ♙ ♙   7   ♟ ♟ ♟ ♟ ♟ ♟ ♟ ♟
1   ♖ ♘ ♗ ♕ ♔ ♗ ♘ ♖   8   ♜ ♞ ♝ ♚ ♛ ♝ ♞ ♜

    a b c d e f g h       h g f e d c b a

Standard Algebraic Notation:
-~-~-~-~-~-~-~-~-~-~-~-~-~-~

. (next move white) or ... (next move is black)
1. e4 e5
1. e4 1... e5

K - king   (♔)
Q - queen  (♕)
R - rook   (♖)
B - bishop (♗)
N - knight (♘)
  - pawn   (♙)

Move: PIECE [file[,rank]] [x] SQUARE (letter first)
              ambiguity  takes
  Ex: fxe5 (pawn(file only if no ambiguity) takes e5)

 kingside castling: O-O
queenside castling: O-O-O
    Pawn promotion: e8=Q [+ check, # checkmate]

Comments: ; or {...}

Numeric Annotation Glyphs
-~-~-~-~-~-~-~-~-~-~-~-~-

!  good move
?  poor move or mistake
!! very good or brilliant move
?? very poor move or blunder
!? speculative or interesting move
?! questionable or dubious move
...
----------------------- File -----------------------
help/misc/music.txt
musical scale:

  a set of ordered notes grouped together for a musical reason.
  notes in a particular scale always sound good played together =>
    chords derive from scales

dot above note: staccato (detached)

  pause before next note + shorter duration

Musical staff & piano keyboard:

  --do--                      --C--
        si                         B        │    │     │ │     │    │    │     │ │     │ │     │    │
  --la--                      --A--         │    │     │ │     │    │    │     │ │     │ │     │    │
        sol                        G        │    │ do♯ │ │ re♯ │    │    │ fa♯ │ │ sol♯│ │ la♯ │    │
――――fa――――――――――――――――――――――――――F―――――――    │    │ re♭ │ │ mi♭ │    │    │ sol♭│ │ la♭ │ │ si♭ │    │
        mi                         E        │    │     │ │     │    │    │     │ │     │ │     │    │
――――re――――――――――――――――――――――――――D―――――――    │    └──┬──┘ └──┬──┘    │    └──┬──┘ └──┬──┘ └──┬──┘    │
        do                         C        │       │       │       │       │       │       │       │
――――si――――――――――――――――――――――――――B―――――――    │       │       │       │       │       │       │       │
        la                         A        │  do   │  re   │  mi   │  fa   │  sol  │  la   │  si   │
―𝄞――sol―――――――――――――――――――――――――G―――――――    │       │       │       │       │       │       │       │
        fa                         F        │       │       │       │       │       │       │       │
――――mi――――――――――――――――――――――――――E―――――――    └───────┴───────┴───────┴───────┴───────┴───────┴───────┘
        re                         D
  --do--                      --C--
        si                         B
――――la――――――――――――――――――――――――――A―――――――
        sol                        G        │    │     │ │     │    │    │     │ │     │ │     │    │
―𝄢――fa――――――――――――――――――――――――――F―――――――    │    │     │ │     │    │    │     │ │     │ │     │    │
        mi                         E        │    │  C♯ │ │  D♯ │    │    │  F♯ │ │  G♯ │ │  A♯ │    │
――――re――――――――――――――――――――――――――D―――――――    │    │  D♭ │ │  E♭ │    │    │  G♭ │ │  A♭ │ │  B♭ │    │
        do                         C        │    │     │ │     │    │    │     │ │     │ │     │    │
――――si――――――――――――――――――――――――――B―――――――    │    └──┬──┘ └──┬──┘    │    └──┬──┘ └──┬──┘ └──┬──┘    │
        la                         A        │       │       │       │       │       │       │       │
――――sol―――――――――――――――――――――――――G―――――――    │       │       │       │       │       │       │       │
        fa                         F        │   C   │   D   │   E   │   F   │   G   │   A   │   B   │
  --mi--                      --E--         │       │       │       │       │       │       │       │
        re                         D        │       │       │       │       │       │       │       │
  --do--                      --C--         └───────┴───────┴───────┴───────┴───────┴───────┴───────┘

Sharps, flats and naturals:

* If a ♯ or a ♭ sits in the header, it affects all similar notes (ex: do1, do2,
  do3...) till the end of the composition.

  If it sits on a measure, it only affects that particular note (ex: do1 but
  not do2...) till the next bar.

* A ♮ cancels the effects of both ♯ and ♭ till the end of the current measure.
----------------------- File -----------------------
help/misc/rubik.txt
                                  Solve the Rubik's cube

1. Top layer

   .---.---.---.
   |   |   |   |
   +---+---+---+
   |   |   |   |
   +---+---+---+
   |   |   | O |
   *---*---*---*
   .---.---.---.
   |   |   |   |
   +---+---+---+
   |  facing   |
   +---+---+---+
   |   |   | X | move X to O
   *---*---*---*

      ·
   ⟳  · ⟲  (3)
      ⇒

2. Middle row

                 .---.---.---.            .---.---.---.
                 | O | O | O |            | O | O | O |
                 +---+---+---+            +---+---+---+
                 |   | O |   |            |   | O |   |
                 +---+---+---+            +---+---+---+
                 |   |   |   |            |   |   |   |
                 *---*---*---*            *---*---*---*
   .---.---.---. .---.---.---.            .---.---.---. .---.---.---.
   | X |   |   | |   |   |   |            |   |   |   | |   |   | X |
   +---+---+---+ +---+---+---+            +---+---+---+ +---+---+---+
   | X | X |   | |  facing   |            |  facing   | |   | X | X |
   +---+---+---+ +---+---+---+            +---+---+---+ +---+---+---+
   | X |   |   | |   | O |   |            |   | O |   | |   |   | X |
   *---*---*---* *---*---*---*            *---*---*---* *---*---*---*
                       X                        X

                  place O/X between the 2 other faces

   ⇒     ⇐                                ⇐     ⇒
   ·  ⟳  ·  ⟳  ⇓··  ⟲  ⇑·· (7)            ·  ⟲  ·  ⟲  ··⇓  ⟳  ··⇑ (7)
   ·     ·                                ·     ·

3. Bottom corners

   3.1. Permute (put in correct location)

        .---.---.---.
        | 3 |   | 4 |
        +---+---+---+
        |  upwards  |
        +---+---+---+
        | 1 |   | 2 |
        *---*---*---*

        * swap 1 with 2

               ⇒          ⇐          ⇐       ⇐  ⇐
          ⇑··  ·  ⇓··  ⟳  ·  ⟲  ⇑··  ·  ⇓··  ·  ·  (11)
               ·          ·          ·       ·  ·
          =====>=====           =====<=====  -  -

        * swap 1 with 2 and 3 with 4

               ⇒          ⇐  ⇐          ⇐
          ⇑··  ·  ⇓··  ⟳  ·  ·  ⟲  ⇑··  ·  ⇓··  (10)
               ·          ·  ·          ·
          =====>=====        +     =====<=====

   3.2. Orient (align colors)

        Possible bottom (upwards) layouts and their solutions. Perform L and/or R from the indicated face(s).

                    o                                 o
                  .---.---.---.             .---.---.---.
                  |   |   |   | o         o |   |   |   |
                  +---+---+---+             +---+---+---+
              R:  |   | O |   |       L:    |   | O |   |
                  +---+---+---+             +---+---+---+
                  | O |   |   |             |   |   | O |
                  *---*---*---*             *---*---*---*
                            o                 o
                        ⇧                         ⇧
                        R                         L

                        L
                        ⇩
                    o
                  .---.---.---.     .---.---.---.     .---.---.---.
                  |   |   | O |     | O |   | O |     | O |   |   | o
                  +---+---+---+     +---+---+---+     +---+---+---+
        L and R:  |   | O |   |     |   | O |   |     |   | O |   |  ⇦ L
                  +---+---+---+     +---+---+---+     +---+---+---+
                  |   |   | O |     |   |   |   |     |   |   | O |
                  *---*---*---*     *---*---*---*     *---*---*---*
                    o                 o       o         o
                        ⇧                 ⇧                 ⇧
                        R               L & R               R

                          R
                          ⇩
                                                    o
                    .---.---.---.         .---.---.---.
                  o |   |   |   | o     o |   |   |   |
                    +---+---+---+         +---+---+---+
        R and R:    |   | O |   |         |   | O |   |  ⇦ R
                    +---+---+---+         +---+---+---+
                  o |   |   |   | o     o |   |   |   |
                    *---*---*---*         *---*---*---*
                                                    o
                          ⇧                     ⇧
                          R                     R

        R. Rotate corners clockwise

                ⇐       ⇐       ⇐  ⇐       ⇐  ⇐
           ··⇑  ·  ··⇓  ·  ··⇑  ·  ·  ··⇓  ·  ·  (10)
                ·       ·       ·  ·       ·  ·

        L. Rotate anti clockwise

                ⇒       ⇒       ⇒  ⇒       ⇒  ⇒
           ⇑··  ·  ⇓··  ·  ⇑··  ·  ·  ⇓··  ·  ·  (10)
                ·       ·       ·  ·       ·  ·

4. Bottom edges

   4.1. Permute (put in correct location)

        .---.---.---.
        |   | X |   |
        +---+---+---+
        | X |   | X | upwards
        +---+---+---+
        |   |   |   |
        *---*---*---*

        * Anti clockwise permutation [ ⟲ ]

               ⇒       ⇒  ⇒       ⇒
          ·⇑·  ·  ·⇓·  ·  ·  ·⇑·  ·  ·⇓·  (8)
               ·       ·  ·       ·
          ===========        ===========

        * Clockwise permutation [ ⟳ ]

               ⇐       ⇐  ⇐       ⇐
          ·⇑·  ·  ·⇓·  ·  ·  ·⇑·  ·  ·⇓·  (8)
               ·       ·  ·       ·
          ===========        ===========

   4.2. Orient (align colors)

        * Dedmore 'H' Pattern

          .---.---.---.
          |   |   |   |
          +---+---+---+
          | X |   | X | upwards
          +---+---+---+
          |   |   |   |
          *---*---*---*

               ·            ·  ·       ⇐  ⇐       ·  ·            ·       ⇐  ⇐
          ··⇓  ⇐  ··⇑  ··⇑  ⇒  ⇒  ··⇓  ·  ·  ··⇑  ⇒  ⇒  ··⇓  ··⇓  ⇒  ··⇑  ·  ·  (18)
               ·            ·  ·       ·  ·       ·  ·            ·       ·  ·
          =====<=====  ==============        ==============  =====>=====

        * Dedmore 'Fish' Pattern

          .---.---.---.
          |   |   |   |
          +---+---+---+
          | X |   |   | upwards
          +---+---+---+
          |   | X |   |
          *---*---*---*

          ⟳      'H' bar 1st ··⇓  ⟲  (20) ⇔
          ⟳  ··⇑ 'H'         ··⇓  ⟲  (22)

5. Summary

   bottom corners positions:
        ⇒          ⇐          ⇐       ⇐  ⇐
   ⇑··  ·  ⇓··  ⟳  ·  ⟲  ⇑··  ·  ⇓··  ·  ·  (11) 1 <-> 2
        ·          ·          ·       ·  ·
   =====>=====           =====<=====  -  -

        ⇒          ⇐  ⇐          ⇐
   ⇑··  ·  ⇓··  ⟳  ·  ·  ⟲  ⇑··  ·  ⇓··  (10) 1 <-> 2 & 3 <-> 4
        ·          ·  ·          ·
   =====>=====        +     =====<=====

   bottom corners colors:
        ⇐       ⇐       ⇐  ⇐       ⇐  ⇐
   ··⇑  ·  ··⇓  ·  ··⇑  ·  ·  ··⇓  ·  ·  (10) R. clockwise
        ·       ·       ·  ·       ·  ·

        ⇒       ⇒       ⇒  ⇒       ⇒  ⇒
   ⇑··  ·  ⇓··  ·  ⇑··  ·  ·  ⇓··  ·  ·  (10) L. anti clockwise
        ·       ·       ·  ·       ·  ·

   bottom edges positions:
        ⇒       ⇒  ⇒       ⇒
   ·⇑·  ·  ·⇓·  ·  ·  ·⇑·  ·  ·⇓·  (8) anti clockwise
        ·       ·  ·       ·
   ===========        ===========

        ⇐       ⇐  ⇐       ⇐
   ·⇑·  ·  ·⇓·  ·  ·  ·⇑·  ·  ·⇓·  (8) clockwise
        ·       ·  ·       ·
   ===========        ===========

   bottom edges colors:

   Dedmore H
        ·            ·  ·       ⇐  ⇐       ·  ·            ·       ⇐  ⇐
   ··⇓  ⇐  ··⇑  ··⇑  ⇒  ⇒  ··⇓  ·  ·  ··⇑  ⇒  ⇒  ··⇓  ··⇓  ⇒  ··⇑  ·  ·  (18)
        ·            ·  ·       ·  ·       ·  ·            ·       ·  ·
   =====<=====  ==============        ==============  =====>=====

   Dedmore Fish
   ⟳      'H' bar 1st ··⇓  ⟲  (20) ⇔
   ⟳  ··⇑ 'H'         ··⇓  ⟲  (22)

6. Patterns, ⟲` means the back row

   6.1 Nested cube

                  ⇒       ⇐                  ⇒           ·                ⇐
       ⟳  ⇓··  ⟳  ·  ··⇑  ·  ⟳  ⟳  ⇓··  ⇓··  ·  ⇑··  ⟲`  ·  ⟳`  ⇓··  ⇓··  ·  (18)
                  ·       ·                  ·           ⇐                ·

   6.2 Two nested cubes

          ·     ·  ·           ⇐       ·       ⇐          ⇐       ⇐  ⇐
       ⟳  ·  ⟲  ·  ·  ⇑··  ⟳`  ·  ⇓··  ·  ··⇑  ·  ⇑··  ⟲  ·  ⇓··  ·  ·  (18)
          ⇒     ⇒  ⇒           ·       ⇒       ·          ·       ·  ·

   6.3 Six Ts

                       ⇐  ⇐         ·  ·
       ⟳  ⟳  ··⇑  ··⇑  ·  ·  ⟲  ⟲`  ·  ·  ⇓··  ⇓··  ⟳  ⟲`  (14)
                       ·  ·         ⇒  ⇒

   6.4 Two Twisted Peaks

              ⇐     ⇐     ⇐                         ⇐          ⇐
       ⟳  ⟳`  ·  ⟳  ·  ⟳  ·  ⇓··  ⟲`  ⇓··  ⇓··  ⟳`  ·  ⟲  ⇓··  ·  ⇑··  ⟲`  (18)
              ·     ·     ·                         ·          ·

       OR

          ·  ·                       ·
       ⟳  ·  ·  ⟲`  ··⇑  ⟳`  ⇑··  ⟳  ·  ⇓··  ⇓··  ⟳  ⟳  ··⇑  ⟲  ··⇓  ⟳  ⟳  ⇑··  ⟲  (20)
          ⇒  ⇒                       ⇐

   6.5 Stripes

          ⇐                   ·       ·  ·       ·                   ⇐
       ⟳  ·  ⟳  ⇓·⇑  ⇓··  ⟲`  ·  ··⇑  ·  ·  ⇓··  ·  ⟲`  ··⇑  ⇓·⇑  ⟳  ·  ⟳  (20)
          ·                   ⇐       ⇒  ⇒       ⇐                   ·
       =======  ===                                          ===  =======

   6.6 Zigzag

       ⇓·⇑  ⟲`  ⟳  ⇓·⇑  ⟲`  ⟳  ⇓·⇑  ⟲`  ⟳  (12)
       ==========  ==========  ==========

   6.7 Four holes

                     ⇐            ⇐
       ⟳  ⟳`  ⟳  ⟳`  ·  ⇓·⇑  ⇓·⇑  ·  (12)
                     ⇐            ⇐

# vim: fdm=expr fde=getline(v\:lnum)=~'\\d\\.'?'>'.(len(matchstr(getline(v\:lnum),'\\%(\\d\\.\\d\\?\\)'))-1)\:'='
----------------------- File -----------------------
help/misc/russian_cases.txt
Prepositional case {{{1
==================

в, на, о, при:
--------------
      : е  |     Москва -> в Москве
ь (f) : и  |     Сибирь -> в Сибири
ие    : ии | упражнение -> в упражнении
ия    : ии |     Англия -> в Англии
ий    : ии |       Юрий -> о Юрии
             ---------------------------------
                   сад  -> в, на саду (о саде)
                   лес  -> в, на лесу
    Exceptions     мать -> о     матери
                   дочь -> о     дочери

Personal pronouns:
------------------
обо мне            о нас
  о тебе           о вас
  о нём, ней, нём  о них

Possessive pronouns:
--------------------
m and n  f
моём     моей
твоём    твоей        Note: его, её, его, их are indeclinable
нашем    нашей
вашем    вашей
---
этом     этой
том      той

Adjectives:
-----------
новый -> новом | зимнии -> зимнем | третии -> третьем
новая -> новой | зимняя -> зимней | третья -> третьей
новое -> новом | зимнее -> зимнем | третье -> третьем

Accusative case {{{1
===============

   а : у |    Москва -> Я люблю Москву
   я : ю | Дядя Ваня -> Она любит дядю Ваню

Animate masculine nouns:
------------------------
cons : а |   Иван -> Вы знаете Ивана ?
   ь : я |  Игорь -> Вы знаете Игоря ?
   й : я | Андрей -> Вы знаете Андрея ?

Additional usages:
------------------
    в, на (motion) -> Мы едем в Москву
через, за          -> через улицу

Personal pronouns:
------------------
меня          нас
тебя          вас
его, её, его  их

Possessive pronouns:
--------------------
m animate  f
моего      мою
твоего     твою       Note: его, её, его, их are indeclinable
нашего     нашу
вашего     вашу
---
этого      эту
того       ту

Adjectives:
-----------
новый -> нового | зимнии -> зимнего | третии -> третьего
новая -> новую  | зимняя -> зимнюю  | третья -> третью

# vim: keymap=ru spelllang=ru foldmethod=marker foldmarker={{{,}}}
----------------------- File -----------------------
help/mount.md
# Synopsis
`mount -t type device mount_point`

mount writes what it has done to `/etc/mtab` (unless using `-n`), thus the contents
of this file are very similar to the output of `mount` without options.

# CD/DVD mounting
1. `mount -t iso9660 -o ro /dev/cdrom /mnt`
2. `mount -t iso9660 -o loop,ro matlab.iso /media/cdrom0`
A loop device in Linux is an abstraction that lets you treat a file (our iso
image which is not an actual CD) like a block device

# /etc/fstab
mount `/mydata` looks in `/etc/fstab` (first for mount points then for devices)
```
#                                           0 for swap/remote
#                                           1 for /
#                                           2 otherwise
# <device> <mount> <type> <options> <dump> <fsck>
# UUID=... /home   ext4   rw...     0      2
```

# View all mounted partitions of specific type
`mount -l -t ext4`

# Bind mount points to a new directory (like a temporary symlink)
`mount -B /mydata /mnt # or --bind`

## move a mount point
`mount -M /mydata /mnt/ # or --move`

# Remount the mounted filesystem (in order to change some options)
`mount -o remount,rw /mydata`

# Lazy unmount of a filesystem (after disk operations are done)
`umount /mydata -l`

# List filesystems
- `findmnt`
- `mount`

# See if a directory or file is a mountpoint
`mountpoint directory|file`
----------------------- File -----------------------
help/mutt.txt
Mutt

Get help with ?

T.  - tag all messages
;W  - apply to tagged messages (;), clear a status flag (W)
^T. - untag all (.) messages

So:
T.;WN^T. - mark all messages as read

d$ - delete a message (d) and save changes to the mailbox ($)
----------------------- File -----------------------
help/nagios.txt
NRPE - Nagios Remote Plugin Executor
====================================

The NRPE addon consists of two pieces:
--------------------------------------
- The check_nrpe plugin, which resides on the local monitoring machine
- The NRPE daemon, which runs on the remote Linux/Unix machine

Note: nrpe runs on port 5666 (define in /etc/services)

1. Nagios will execute the check_nrpe plugin and tell it what service needs to
   be checked

   Example:
   /etc/nagios/objects/hosts.cfg
   define host {
      host_name ...bla.com

   define hostgroup {
      hostgroup_name all_bla
      members ^.*bla.com$ # the above host will be part of this group

   /etc/nagios/objects/services.cfg
   define service {
      hostgroup_name all_bla # use this group...
      host_name !host1       # ...but exclude that host
      check_command check_init_service!sinatra
      servicegroups s_bla # group for display purposes

   define servicegroup {
      servicegroup_name s_bla

   Notes:
   * command is the same as defined in nrpe.cfg below
   * Verify your Nagios configuration files: nagios -v /etc/nagios/nagios.cfg
   * service nagios restart

2. The NRPE daemon runs the appropriate Nagios plugin to check the service or
   resource
   * The NRPE daemon is started by xinetd
   * nrpe.cfg is used to configure the plugins
     Example: command[check_init_service]=/usr/lib64/nagios/plugins/check_init_service $ARG1$
   * Get help on a plugin: /usr/lib64/nagios/plugins/check_procs -h

Note: The NRPE daemon requires that Nagios plugins package be installed on the
      remote Linux/Unix host. Without these, the daemon wouldn't be able to
      monitor anything.

Tests:   on remote: /usr/lib64/nagios/plugins/check_nrpe -H localhost
                    /usr/lib64/nagios/plugins/check_init_service sinatra
      from monitor: /usr/lib64/nagios/plugins/check_nrpe -H remote_machine
      Note: Adapt paths to your plugin location

NRPE.pdf:
http://www.google.co.uk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CC8QFjAA&url=http%3A%2F%2Fnagios.sourceforge.net%2Fdocs%2Fnrpe%2FNRPE.pdf&ei=q7M5UvfQGKXW0QXqooHIBA&usg=AFQjCNGYpmjIaMBRWQWzjMSmZZ_Zl_BgLA&sig2=sDp_95AVMs2amDitMTplgQ&bvm=bv.52288139,d.d2k
----------------------- File -----------------------
help/network/net.txt
OSI

2. link layer: frame  host   A - MAC addresses - frame  host   B
3. inet layer: packet router A - IP  addresses - packet router B
4. transport layer: TCP
5. app layer: SSH, HTTP, ...

|o|o| - console (serial) port

uplink port, switch, router

A route has host scope when it leads to the local host.
A route has link scope when it leads to the local network.
A route has universe scope when it leads to addresses more than one hop away.

managed switches support VLANs

Routing

The kernel takes care of routing automagically, this can be checked with route
-n... auto flag. The only time we need explicit routing is when it isn't
obvious.

Two ports being on the same NIC, doesn't mean that the 2 associated nets will
auto communicate. If the ports are on the same net yes, if on different nets,
IP forwarding is enabled and there is an allowing firewall rule then yes, else
not.

Notes:
* the admin decides what nets they will be on when configuring
  /etc/sysconfig/network-scripts/ifcfg-...

* Once IP forwarding is enabled, all nets can communicate thus you need proper
  firewall rules to prevent this

Gateways

route -n
In the Destination column, 0.0.0.0 (default) matches all IPv4 addresses
In the Gateway     column, 0.0.0.0 (*)       indicates that there is no gateway
                                             and the packet should not be routed (sent locally?)

Link Aggregation aka LAG,
aka bond/team (NIC), bundle, trunk, EtherChannel, Port Channel...

Multiple physical links are combined into one logical link. Benefits:
* Increased throughput: traffic load balanced between links
* Redundancy: can survive the loss of links/devices

                                               +-- --+
ex: switches linked by more than 1 cable => sw +-- --+ sw
                                               +-- --+

Firewall

Multi homed means that our firewalls are connected to several ISPs for high availability

A connection in one direction creates a state and all packets going in the
other direction will belong to this connection.

ex: for an incoming connection there is no need to create outgoing nat rules.
    it's only necessary if we initiate the connection.

  Linux: source NAT and destination NAT
OpenBSD: NAT and redirection
----------------------- File -----------------------
help/network/troubleshoot.txt
Layer 5: Application
Layer 4: Transport
Layer 3: Network/Internet # ping, traceroute, firewalls
Layer 2: Data Link        # ip n; LAN only: ARP, IP <-> MAC; can't reach GW?
Layer 1: Physical

1.
ethtool eth0
ifstat
ip -s -c l
ip -br -c l         # DOWN could mean no cable or disabled interface
ip link set eth0 up #   UP would mean it was simply disabled

3.
ip -br -c -4 a
ping: can be rate limited by intermediate network gear => inaccurate application latency

traceroute:
 - TTL 1, 1st router sends back an ICMP Time-to-Live Exceeded packet
 - TTL++, 2nd router ...

DNS (not level 3), if it fails, check if we can connect via IP.

4.
ss -tunlp4
telnet db 3306
nc 192.168.0.100 -u 80
nmap
----------------------- File -----------------------
help/nginx.txt
nginx # start
nginx -s quit
nginx -t

Configuration

events {}

worker_processes 2; # directive in main context

http {
 # vhosts
 server {
    listen 80 default_server; # *:80
    root /home/goldorak;
    location / {
       # avoid in server context as then it's the least specific location
       try_files /prout /index.html =500;
    }
  }
 server { # server context
    listen 80;
    server_name caca; # curl -Hhost:caca localhost:80
    return 200 'Hi caca';
  }
}
----------------------- File -----------------------
help/nm.txt
nmcli

g[eneral]    - status | hostname | permissions | logging
n[etworking] - [on | off | connectivity]
r[adio]      - all | wifi | wwan
c[onnection] - show | up | down | add | modify | clone | edit | delete | monitor | reload | load | import | export
d[evice]     - status | show | set | connect | reapply | modify | disconnect | delete | monitor | wifi | lldp
a[gent]      - secret | polkit | all
m[onitor] - prints a line whenever a change occurs in NetworkManager

nmcli d h # get help on device command
nmcli g (st)
nmcli d (st) # interfaces status
nmcli c (show)
----------------------- File -----------------------
help/nodejs.txt
Node.js - a V8 js platform which allows to build server/client apps all in js
          ex of such an app: statsd
----------------------- File -----------------------
help/ntp.txt
                                      ntp
ntp strata
----------
stratum 0: time keeping device
stratum 1: server directly attached to a stratum 0
stratum 2: server which queries a        stratum 1
...

Recommendations:
* sync from a stratum 2 up so not to overload stratum 1s
* Use a ntp pool, ex:

    - 0.uk.pool.ntp.org
    - 1.uk.pool.ntp.org
    - 2.uk.pool.ntp.org
    - 3.uk.pool.ntp.org

  (http://www.pool.ntp.org/en/use.html)
----------------------- File -----------------------
help/oo.txt
Benefits of encapsulation

   public name

   file1: obj.name = bla
   file2: obj.name = hey
   file3: obj.name = ole
   file4: obj.name = wow

Problem: now we also need to record the name in a db!

Solution:

   private name

   def setName(self, name):
      self.name = name
      record name in db

   file1: obj.setName(bla)
   file2: obj.setName(hey)
   file3: obj.setName(ole)
   file4: obj.setName(wow)

Conclusion:

   By using a private name, we now only have to change the setter instead of
   all the places where we were directly modifying the property!
----------------------- File -----------------------
help/options.txt
Perl Getopt::Long GetOptions

!   --foo    -> 1
    --no-foo -> 0

+   --more --more --more -> 3

=   s string
    i integer
    o extended integer
    f real number
    @ list
    % hash

:   same as = above but arg is optional
:5  similar to :i but --age -> 5
:+  similar to :i but --age -> 3 if age was 2

Bash getopts

http://wiki.bash-hackers.org/howto/getopts_tutorial

getopts :f:b:su var $@ (default)
         \ \     \
          \ \     where the option is stored (without the -)
           \ -f expects an argument (available in $OPTARG)
            silent error mode
----------------------- File -----------------------
help/package_managers.adoc
== List package contents

:table-caption: package manager
.yum
|===
|installed |non-installed

|`rpm -ql pkg`
|`repoquery -l pkg`
|===

== Find which package contains a given file/binary (reverse of 'List package contents')

[source,bash]
....
rpm -qf /usr/bin/certutil
dpkg -S =pgrep
yum provides sar
pacman -Qo clear
....

== Show all available versions for a package

|===
|yum | apt

|`yum list --showduplicates httpd`
|`apt-cache policy httpd`
|===
----------------------- File -----------------------
help/parallel.txt
Single input source

parallel cmd ::: arg1 arg2 arg3
parallel cmd :::: args_from_file

ex: parallel --tag git -C {} stash list ::: *(/)

Multiple input sources

parallel cmd ::: args... ::: args...
parallel cmd :::: file1 file2
parallel -afile1 -afile2 cmd

STDIN

cmd | parallel echo
cat file | parallel echo :::: - file2 ::: arg1 arg2
parallel --xargs rm < file # safe on wordsplitting

Combinations

parallel echo ::: A B ::: D E
A D
A E
B D
B E

parallel echo ::: A B C :::+ D E F # or use --link
A D
B E
C F

Replacement strings (-I to change {})

parallel echo {.}  ::: A/B.C -> A/B   # rm ext,                 also: {1.}  and: {=1 s/// =}
parallel echo {/}  ::: A/B.C ->   B.C # rm path   (basename),   also: {1/}
parallel echo {//} ::: A/B.C -> A     # keep path (parent dir), also: {1//}
parallel echo {/.} ::: A/B.C ->   B   # rm ext & path,          also: {1/.}

--plus

      A/B  C
 {} = {.}.{+.} # the 'plus' variant is the opposite of {.}

{..} and {...} # remove 2/3 extensions

     {%string} # remove str from end # see bash counterparts

Positional replacement strings

parallel echo {1} and {2} ::: A B ::: C D
A and C
A and D
B and C
B and D

Perl expression replacement string

parallel echo '{= s/\.gz$//; s/\.tar$// =}' ::: foo.tar.gz -> foo
parallel echo '{= skip if /3/ =}' ::: {1..5}

args are inside @arg, so '{= ... $arg[0] ... =}'

'{= $_=Q($_) =}' # shell quote

Other options

--tag: prefix output with the arguments

parallel --tagstring {/}: echo ::: A B
A:      A
B:      B

--xargs, -m, -X: process args in bulk

parallel --xargs echo hello_{} ::: a b c ::: c d f
hello_a c a d a f b c b d b f c c c d c f

parallel -m echo hello_{} ::: a b c ::: c d f
hello_a c a d a f
hello_b c b d b f
hello_c c c d c f

parallel -X echo hello_{} ::: a b c ::: c d f
hello_a hello_c hello_a hello_d hello_a hello_f
hello_b hello_c hello_b hello_d hello_b hello_f
hello_c hello_c hello_c hello_d hello_c hello_f

-d: arg delim

find ... -print0 | parallel -0 echo # short for -d'\0'

--no-run-if-empty: skip empty lines

Notes

No cmd (implies bash) => treat args as cmds
parallel ::: ls 'echo hi' pwd

SSH

parallel --nonall --tag -S host1,host2 uname -a
----------------------- File -----------------------
help/parted.txt
The legacy MBR partition scheme allows 4 "primary" partitions at most. To
bypass this limitation, one of the partitions must be "extended" instead of
"primary" so it can contain several "logical" partitions.

Windows is unable to boot from logical partitions.

The newer partition scheme, GPT, doesn't have such a restriction, and does not
need to use extended/logical partitions.

parted -l
parted /dev/sdb mklabel gpt
parted /dev/sdb mkpart primary xfs 0% 100%
parted /dev/sdb set 1 lvm on

or

parted /dev/sdb
print
mklabel
...
quit

pvcreate /dev/sdb1
vgcreate vg /dev/sdb1
lvcreate -l100%FREE -nlv vg

ll /dev/mapper
vg-lv -> ../dm-2

mkfs.xfs -L data /dev/mapper/vg-lv

fstab
LABEL=data /data xfs defaults 0 0
----------------------- File -----------------------
help/partitions.adoc
== Optimal sizes?

 /     20GiB
 /boot  1GiB EFI System Partition (ESP)
             +- Extensible Firmware Interface

linuxswap:

* < 4GiB RAM - twice the swap
* > 4GiB RAM - same swap

== Create new partition

 fdisk /dev/sda
 n - accept default sectors to use all space
 t - 31 Linux LVM
 w
 pvcreate /dev/sda3
 vgcreate vgroup /dev/sda3
 lvcreate -l100%FREE -nsync vgroup
 mkfs.xfs /dev/vgroup/sync
 mkdir /sync
 mount /dev/sda3 /sync

== LVM list

 pvs[can] # physical volumes
 vgs[can] # volume groups
 lvs[can] # logical volumes

== LVM operatios

pv:

* `/var`  25G
* `/home` remaining

[source,bash]
....
lvcreate -L25gib -nvar lv
lvcreate -l100%FREE -nhome lv

# extend an lvm volume + resize the ext4 partition on it
lvextend -L+2gib /dev/lv/lv-var /dev/sda4
# alternatively use all remaining space
# lvextend -l100%FREE /dev/lv/lv-var /dev/sda4

resize2fs /dev/lv/lv-var
....
----------------------- File -----------------------
help/paste.txt
% cat list
a
aa
b
bb
c
cc

# serial
% paste -s list
a       aa      b       bb      c       cc

# Delimit 1st el with :, 2nd with \n, loop
% paste -s -d':\n' list
a:aa
b:bb
c:cc

% paste - - < list
a       aa
b       bb
c       cc
----------------------- File -----------------------
help/patch.md
# Hunks are read from STDIN or a patch file

## Patch one or many files

The filename can be omitted if it can be deduced from within the hunks.  
It must be omitted when patching multiple files.

_These 3 commands are equivalent:_
```sh
patch [pf.conf] -i pf.patch # --input
```
```sh
patch [pf.conf] < pf.patch
```
```sh
patch [pf.conf]
   paste on STDIN
^d
```

## Patch a single file
```sh
patch pf.conf pf.patch
```

# Paths in hunks must correspond to real files

```diff
--- /home/user/project.pl
+++ /home/user/project.pl
```
```sh
# if user is unknown locally, remove /home/user by stripping 3 leading /s
patch -p3 < project.patch
```

*_NB_*: relative paths in hunks need `-p0`

# Create a patch

```sh
diff -u orig new > orig.patch
        git diff > orig.patch
    svn di -x -u > orig.patch
```
----------------------- File -----------------------
help/perl/golf/fizzbuzz.pl
for(1..100){$d=$_;$_%3||s/.+/Fizz/;$d%5||s/\d+|$/Buzz/;say}
say'Fizz'x!($_%3).Buzz x!($_%5)||$_ for 1..100
say+[Fizz]->[$_%3].(Buzz)[$_%5]||$_ for+1..1e2
say+(Fizz)[++$_%3].Buzz x/.?[05]$/||$_ until$`
say+(Fizz)[++$_%3].(Buzz)[$_%5]||$_ until/00/
map{say+(Fizz)[$_%3].(Buzz)[$_%5]||$_}1..1e2
say+(Fizz)[$_%3].(Buzz)[$_%5]||$_ for+1..1e2
say+(Fizz)[$_%3].Buzz x/0|5$/||$_ for 1..1e2
----------------------- File -----------------------
help/perl/golf/history.md
* [Andrew’s Santa Claus Golf Apocalypse](#santa)
    * [head](#head)
    * [tail](#tail)
    * [rev](#rev)
    * [mid](#mid)
    * [wc](#wc)

# Andrew’s Santa Claus Golf Apocalypse <a name="santa"></a>

## Hole 1 (head) <a name="head"></a>
```perl
-p 11..& # call to undefined function
```

## Hole 2 (tail) <a name="tail"></a>
```perl
print+(<>)[~9..-1] # ~9 is -10
```

## Hole 3 (rev) <a name="rev"></a>
```perl
# while (<>) {
#   # prefill $\ with all the lines in reverse order
# }
# {
#   ();
# }
# continue {
#   print $_; # then do a single print
# }
-p $\=$_.$\}{

print reverse<>
```

## Hole 4 (mid) <a name="mid"></a>
```perl
-p0 $_=$1while/.(^.+)^/ms
@n=<>;print@n[$#n/2..@n/2]
```

## Hole 5 (wc) <a name="wc"></a>
```perl
-p }{$_=$.+1e9.$/^v1 # binary xor ^
printf"%010d\n",$.,<>
```
----------------------- File -----------------------
help/perl/golf/sierpinski.pl
#! /usr/bin/env perl

use v5.22;
use warnings;
use open qw/:std :encoding(UTF-8)/;
use Getopt::Long qw/GetOptions :config bundling/;
use File::Basename 'basename';
use utf8;

my $script = basename $0;
$script = -f $script ? "./$script" : $0 =~ s/$ENV{HOME}/~/r unless -l $0;

# Help
my $help = << "";
$script [options]
--char,  -c : default ▲
--order, -o : default 4

my $char = '▲';
my $order = 4;

# Options
GetOptions (
   'c|char=s'  => \$char,
   'o|order=i' => \$order,
   'h|help'    => sub { print $help; exit }
) or die "Error in command line arguments\n";

my @triangle = $char;

while ($order--)
{
   my $limit = $#triangle;
   for (0..$limit)
   {
      push @triangle, $triangle[$_] . ' ' x length ($triangle[$limit - $_]) . $triangle[$_]
   }
}

for (0..$#triangle)
{
   say ' ' x length($char) x ($#triangle - $_) . $triangle[$_]
}
----------------------- File -----------------------
help/perl/maxclients.pl
#! /usr/bin/env perl

# List files in which a given pattern occurs after another pattern
#
# Ex:
# in httpd.conf find MaxClients when occuring after worker.c

use v5.12;
use warnings;

open my $FIND, '-|', qw/fd -tf -E*~ ^httpd\.conf/ or die "$!\n";

while (my $file = <$FIND>)
{
   chomp $file;
   open (my $fh, $file) or die "$!\n";
   while (<$fh>)
   {
      next if 1 .. /worker\.c/;
      next unless /^maxclients\s+(.+)/i;
      say "$file: $1";
      last;
   }
   close $fh;
}
----------------------- File -----------------------
help/perl/oneliners.adoc
== List valid `/home` directories
`perl -F: -lanE 'next if $F[0] =~ /halt|sync|shutdown|nfsnobody/; next if $F[6] =~ m#^(?:/usr)?/sbin/nologin/?$# or $F[6] =~ m#(?:/usr)?/bin/false/?$#; say $F[5]' /etc/passwd | xargs ls -Fld --color`
----------------------- File -----------------------
help/perl/pack.pl
#! /usr/bin/env perl

# example from:
# https://perldoc.perl.org/perlpacktut.html

use v5.12;
use warnings;
use POSIX 'strftime';

my $tot_income;
my $tot_expend;

while (<DATA>)
{
   next if /^$/ or /^Date/;
   print;

   my ($date, $desc, $income, $expend) = unpack('A10xA27xA7xA*', $_);

   $tot_income += $income ||= 0;
   $tot_expend += $expend ||= 0;
}

$tot_income = sprintf('%.2f',   $tot_income);
$tot_expend = sprintf('%11.2f', $tot_expend); # right-align expenditure

my $date = POSIX::strftime('%m/%d/%Y', localtime);

say '-' x 58;
say pack('A11 A28 A8 A*', $date, 'Totals', $tot_income, $tot_expend);

__DATA__

Date      |Description                |Income |Expenditure
01/24/2001 Zed's Camel Emporium                    1147.99
01/28/2001 Flea spray                                24.99
01/29/2001 Camel rides to tourists     1235.00
----------------------- File -----------------------
help/perl/perl.md
* [use](#use)
* [Statement, expression](#statement)
* [Arrays and Lists](#arrays_lists)
    * [merge two arrays and keep elements unique](#arr_merge)
* [Hashes](#Hashes)
* [References](#References)
* [Scope](#Scope)
    * [environment variables](#scope_env)
* [Regex](#Regex)
    * [backreferences](#backreferences)
    * [captures in list context](#reg_captures)
    * [lookaround](#lookaround)
    * [match multilines and newlines in s///ms](#reg_newlines)
    * [possessive quantifiers](#reg_possessive)
* [Subroutines](#Subroutines)
    * [ternary operator - cond ? true : false](#sub_ternary)
    * [printf is sometimes more readable than print](#sub_printf)
    * [sprintf is like printf but a string is returned instead of printed](#sub_sprintf)
    * [date with format](#sub_date)
    * [pack](pack.pl)
    * [evaluation in s//$1/](#sub_s_eval)
    * [return values](#sub_return)
* [Command line](#cmd_line)
    * [sed](#sed)
    * [use a module](#use_module)
    * [one liners](#one_l)
        * [delete lines](#one_l_delete)
        * [search and replace in multiple files in parallel](#one_l_replace)
        * [print from field $3 to last](#one_l_fields)
        * [regex based sort](#one_l_sort)
        * [replace line(s) with contents of file](#authorized_keys)
        * [namei -l](#one_l_namei)
        * [disk usage pretty](#one_l_du)
        * [find files older than a day](#one_l_find)
        * [PerlIO: convert from dos/cyrillic to unix/utf8](#perlio)
* [Precedence](#Precedence)
* [Unicode](#Unicode)
* [Exceptions](#Exceptions)
* [Traps](#Traps)
* [Documentation](#Documentation)
* [Modules](#Modules)
* [End of the program](#end)

# use

```perl
use v5.10 # say, state, //, -r -w (-r $file && -w _)
use v5.12 # use strict, ...
use v5.14 # s///r
use v5.16 # fc
use v5.20 # postderef
use v5.22 # regex modifier /n
use v5.26 # <<~
use v5.32 # chained comparisons
```

# Statement, expression <a name="statement"></a>

```perl
 statement -> code
expression -> code that returns a value
```

# Arrays and Lists <a name="arrays_lists"></a>

`,` creates lists. `()` is only necessary if precedence is ambiguous.

```perl
=> (fat coma) is the same as ,
      foreach is the same as for
```

```perl
@items = qw/one two three four five/ # quote words into a list ('', '', ...)

if (@items) # number of elements

@items[1..$#items] # slice: all bar 1st

          my $a = @array # last to $a in scalar context, see coma operator
        my ($a) = @array #  1st to $a in list   context
my ($a, $b, $c) = @array # multiple assignments

splice @items,  0,  2            # remove beginning  :         three four five | ~shift
splice @items,  1, -1            # remove   middle   : one                five |
splice @items, -2                # remove       end  : one two three           | ~pop
splice @items,  1,  3, qw/2 3 4/ # remove & REPLACE  : one 2   3     4    five |
splice @items,  2,  0, qw/2 3/   # remove 0 (INSERT) : one two three four five |
#                                                             '- 2 3

shift, unshift, pop and push # special cases of splice

 map expr, @items # modify - comprehension
grep expr, @items # filter - like @list =~ /match/ but check ~~ for this

print @items   # $, - print's field separator; $\ is the record separator only printed after print's last argument
print "@items" # $" - list separator for interpolation
```

## merge two arrays and keep elements unique <a name="arr_merge"></a>

```perl
my @unique = uniq(@array1, @array2);        # 1. use List::Util 'uniq';
my @merged{@array1, @array2} = ();          # 2.
my %merged = map { $_, 1 } @array1, @rray2; # 3. create (key:$_, val:1) list for each item
```

methods 2. and 3. need this:
```perl
my @unique = keys %merged;
```

# Hashes

```perl
%items = @pairs;

%items = (
   key1 => 'val1', # keys auto quoted (same for $items{key1})
   key2 => 'val2'
);

while (my ($key, $val) = each %items)
```

# References

```perl
$ref = \$named_variable;      # \@, \%
$ref = [qw/anonymous array/]; # mnemo: []s access array elements
$ref = {anonymous => 'hash'}; #        {}s access hash elements
```

```perl
                use {$ref} anywhere an array/hash would be used ({}s are optional)
               /
     %hash | %$ref      | $ref->%*
    @array | @$ref      | $ref->@*
$hash{key} | $$ref{key} | $ref->{key}
 $array[3] | $$ref[3]   | $ref->[3]
                              /
                 optional between 2 subscripts:

([...], [..x], [...]) - $$array[1][2]   <=>
                         $array[1]->[2] <=>
                         $array[1][2]       {}{} for hashes
```

# Scope

```perl
   my - lexical scope
  our - same but alias for package var so can be accessed from outside
local - local copy of a global variable
```

_example_: input record separator aka IFS
```perl
local $/ = "\0"; # read null separated records
local $/;        # slurp file mode
local $/ = '';   # paragraph mode
```

## environment variables <a name="scope_env"></a>
```perl
         private | public (inherited by children)
      -----------+--------------
Perl: my $EDITOR | $ENV{EDITOR}
Bash:     EDITOR | export EDITOR -> $ env # see public
                                    $ set # see all
```

# Regex

[Perl regex REPL](https://github.com/kurkale6ka/scripts/blob/master/rrepl.pl)

zero-width assertions don't consume chars => they are **AND**ed
```perl
hello(?=\d)(?!123) # followed by a number AND not followed by 123
```

`/$var/o` - check `$var` only once since we know it's not going to change

```perl
  pre - $`
match - $&
 post - $'
```

## backreferences
```perl
s/(\d+).\1/...$1/; # \1 and $1 represent the actual match, not \d+
```

## captures in list context <a name="reg_captures"></a>
```perl
my ($ext) = $file =~ /\.(\w{3})/;
my @numbers = $version =~ /\d+/g; # progressive matching
```

## lookaround
```perl
        \K
(?<= ... ) --- (?= ... )
(?<! ... ) --- (?! ... )
```

:warning: `.+(?=bla)` (variable length pattern) is a bad idea as lookahead is of zero-width so `.+` consumes everything!

## multilines and newlines in s///ms <a name="reg_newlines"></a>
```perl
# ^ and $ positions: hello$\n^alien$\n^world, beware of $\ or $. which are special variables!
$_ = qq/hello\nalien\nworld\n/;
s/^.+$/---/m;  # multilines: match ^ and $ many times
s/lo.+wo/@@/s; # pretend $msg is a single line => . matches anything, including \ns
```

## possessive quantifiers <a name="reg_possessive"></a>
no backtracking <=> don't give up characters

`A++` is syntactic sugar for atomic group notation: `(?>A+)`

_example_:  
`"abcd =~ "[^"]+"`  
after matching `"abcd`, it's clear that no backtracking will change the fact that  
a final `"` cannot be matched. Thus, in order to speedup failure, the pattern is  
better rewritten as `"[^"]++"`  

_notes_:
* `"abcd" =~ "[^"]++"` still matches.
* the optimizer would've automatically turned the regex possessive in this simple case.

# Subroutines

[FMTEYEWTK on Prototypes in Perl](https://groups.google.com/g/comp.lang.perl.modules/c/SVhwH2tRVaM/m/WwgB6-VNSIQJ)

```perl
# takes a scalar, and a 2nd optional one
sub get ($;$) {
   my $var = shift; # or my ($var1, $var2) = @_;
   wantarray ? @res : $res;
}
```

## ternary operator - cond ? true : false <a name="sub_ternary"></a>
```perl
printf "I've got %d camel%s", $ARGV[0], $ARGV[0] == 1 ? '' : 's';
```

## printf is sometimes more readable than print <a name="sub_printf"></a>
```perl
print 'Found a ', pos($i), "at\n";
printf "Found a %d at\n", pos($i);
```

## sprintf is like printf but a string is returned instead of printed <a name="sub_sprintf"></a>
it can then be passed to functions such as `say` which lack formatting capabilities.

## date with format <a name="sub_date"></a>
```perl
strftime '%d-%b-%Y_%Hh%M:%S', localtime; # POSIX module
$now->strftime($format);                 # Time::Piece->new
```

## pack
[pack](pack.pl)

## evaluation in s//$1/ <a name="sub_s_eval"></a>
```perl
$add = 4 + 3;
$_ = 'Sum: $add';
s/(\$\w+)/$1/ee;
```
```perl
without /e -> "" interpolation
   with /e -> normal code:
              $1 gets 'interpolated' by the first /e,
              it's value (4 + 3) gets evaluated by the second /e!
```

## return values <a name="sub_return"></a>

```perl
              s/// - number of substitutions
             chomp - number of chars
              grep - list
               map - list
if (my $var = ...) - lvalue, not boolean
   each %hash, //g - boolean
        shift, pop - element
     unshift, push - number of elements
```

# Command line <a name="cmd_line"></a>

```perl
perl -n # while (<>) { ...        } read           lines from files, add -l for chomp
perl -p # while (<>) { ...; print } read and print lines from files

-a implies -n
-F implies -an

perl -00   # paragraph  mode
perl -0777 # file slurp mode

# to match newlines we need 'slurp' and //s
# use print $1 because $& would be the whole file
# 'while //sg' could be replaced with 'if //ms' when matching with ^ and/or $
perl -0777 -lne 'print $1 while /(---)/sg' file
```

## sed

```perl
perl -lpe 's///' file

perl [-i(suffix)] -lp[0]e 's///' file
        \              \
         \              read null separated data
          edit in place
```

## use a module <a name="use_module"></a>
```perl
perl -M'Term::ANSIColor ":constants"' -E 'say YELLOW.Hello'
perl -mTerm::ANSIColor=:constants -E 'say YELLOW.Hello'
```

## one liners <a name="one_l"></a>

### delete lines <a name="one_l_delete"></a>
```perl
perl -i -lnE 'say unless /.../ or /.../' file
```

### search and replace in multiple files in parallel <a name="one_l_replace"></a>
```perl
rg -il mem | parallel -q perl -i -lpe 's/mem/Memory/ig'
```

### print from field $3 to last <a name="one_l_fields"></a>
```perl
perl -lane 'print "@F[2..$#F]"' /my/file
```

### regex based sort <a name="one_l_sort"></a>
```perl
perl -e 'print for sort {@m=map/(\d\.\d)/,$b,$a; pop@m<=>pop@m} <>' /my/file
perl -e 'print for sort {(split" ",$a)[1]<=>(split" ",$b)[1]} <>' /my/file # sort on 2nd field
```

### replace line(s) with contents of file <a name="authorized_keys"></a>
```perl
perl -i -lnE '$name=...; $_=`cat ~/keys/$name` if /$name/; chomp; say' authorized_keys
```

### namei -l <a name="one_l_namei"></a>
```perl
perl -e '$_=shift; push @paths, $`.$& while m{.*?/(?!$)}g; exec qw/ls -lhd/, @paths, $_' /my/file
```

### disk usage pretty <a name="one_l_du"></a>
```perl
du -ah0 -t100m -d1 | sort -hrz | perl -0lane 's:^\./:: for @F; print shift @F, " ", `ls -d --color "@F"`'
```

### find files older than a day <a name="one_l_find"></a>
```perl
perl -E 'for(<*>){say if-M>1}'
```

### PerlIO: convert from dos/cyrillic to unix/utf8 <a name="perlio"></a>
```perl
perl -Mopen=':std,IN,crlf:encoding(cp1251),OUT,unix:encoding(utf8)' -lpe '' star_wars.sub > star_wars.srt
```

# Precedence

`or`, `and` are the same as  
`||`, `&&` but with lower precedence

# Unicode
```perl
use utf8;                           # write Unicode characters in your source code
use Encode 'decode';                # process @ARGV in utf8 (perl -CA can also be used)
use open qw/:std :encoding(UTF-8)/; # Unicode with IO filehandles, e.g `say '零'`
```

# Exceptions

try, catch is:
```perl
   eval BLOCK;
if ($@) BLOCK
```

# Traps

always `chomp` after: `system`, backticks, `open`, `<STDIN>`

---

`glob`, `<*>` is safe for word splitting,  
it's arguments only split on whitespace, not the returned files!  
solutions: `<"">`, `glob '""'`, or **best** to completely avoid the shell:

```perl
opendir my $DIR, '.' or die "$!\n";
my @dotfiles = grep { -f and /^\./ } readdir $DIR;
```

---

use `open`, `system`, ... with 3 args `'-|', ...` to:
- be protected against clobbering, code exe, ... (`>`, `|`, ... in `$filename`)
- avoid spawning a shell

---

```perl
die "exception"; # without a newline, the script line number is appended
```

---

```perl
if (`lsof ...`)
vs
if (system('lsof', ...) == 0)
```
because  
`% lsof +D folder` 'always' sets `$?` to 1, `man lsof` (DIAGNOSTICS)

---

do not use `-X` file tests because of race conditions  

_example_: just use
```perl
`cat $file`
vs
`cat $file` if -f $file;
```

---

`each` and `//g` return boolean so use `while` instead of `for`:  
```perl
while (each %hash)
while (//g)
```

---

use
```perl
@backups[0 .. $#backups - 3]
vs
@backups[0 .. -3]
```
because `..` counts up only

---

```perl
100 %  3 =  1 (100 - 3 * 33 = 99)
100 % -3 = -2 (100 - 3 * 34 = 102 <=> 100 % 3 - 3) # => it's either 0 or negative
```

# Documentation

[Easier access to Perl help topics](https://github.com/kurkale6ka/scripts/blob/master/man.pl)

```perl
perldoc perl
perldoc perldoc
perldoc perlop
perldoc perlrun # command line options
perldoc File::Basename
perldoc -f split
perldoc -f -x # file test operators
```

* [Perl Training Australia - Perl Tips](http://perltraining.com.au/tips)
* [Learn Perl in about 2 hours 30 minutes](https://qntm.org/perl_en)
* [perlsecret](https://metacpan.org/dist/perlsecret/view/lib/perlsecret.pod)
* [Have fun with code golf](https://code.golf/)

# Modules

```perl
use warnings;
use diagnostics;
use Getopt::Long 'GetOptions';
use File::Basename 'basename';
use File::Path 'make_path';
use Term::ANSIColor ':constants';
use List::Util 'any';
```

# End of the program <a name="end"></a>

`__END__` or `__DATA__`

* POD
* _comments_
* data that we want to process with `while (<DATA>)`
----------------------- File -----------------------
help/perl/raku.txt
say %capitals
say keys %capitals
say %capitals.keys
.say for keys %capitals
for keys %capitals {.say}
----------------------- File -----------------------
help/perl/subtotals.pl
#! /usr/bin/env perl

use strict;
use warnings;

# hash of hashes:
#
# apples
#   +- blue => 3
#   +- red  => 1

my %db;

while (<DATA>)
{
   my ($col1, $col2) = split;
   $db{$col1}{$col2}++;
}

while (my ($key, $val_ref) = each %db)
{
   while (my ($key2, $val2) = each %$val_ref)
   {
      printf "%8s: %d %s\n", $key, $val2, $key2;
   }
}

__DATA__
apples   blue
berries  red
tomatoes green
apples   red
apples   blue
berries  red
tomatoes green
berries  red
apples   blue
berries  green
----------------------- File -----------------------
help/perl/versions.pl
#! /usr/bin/env perl

# Find max release
# by comparing maj, min and patch components in versions

use v5.12;
use warnings;

my @release = (0) x 3;

RELEASE: while (<DATA>)
{
   next unless /^\d+\.\d+\.\d+$/;

   chomp;
   my @rel = split /\./;

   # - maj (rel) = maj (release): continue with min component
   # - maj (rel) > maj (release): upgrade release, test next rel
   # - maj (rel) < maj (release): test next rel
   for my $i (0..$#release)
   {
      unless ($rel[$i] == $release[$i])
      {
         @release = @rel if $rel[$i] > $release[$i];
         next RELEASE;
      }
   }
}

my $release = join '.', @release;

say $release;

__DATA__

11.3.2
12.4.8
10.0.1
13.0.10
13.0.11
13.1.11
1.1.1
0.0.0
11.3.3
----------------------- File -----------------------
help/perl/wifi-labels.pl
#! /usr/bin/env perl

# print wifi password labels!

use strict;
use warnings;

my ($network1, $password1) = ('guest 1:', '-%-%-%-%-%-%-%-%');
my ($network2, $password2) = ('guest 2:', '%-%-%-%-%-%-');

write for 1..8;

format STDOUT =
@<<<<<<<<<<<<<<< │ @<<<<<<<<<<<<<<< │ @<<<<<<<<<<<<<<< │ @<<<<<<<<<<<<<<<
($network1) x 4
@<<<<<<<<<<<<... │ @<<<<<<<<<<<<... │ @<<<<<<<<<<<<... │ @<<<<<<<<<<<<...
($password1) x 4
                 │                  │                  │
@<<<<<<<<<<<<<<< │ @<<<<<<<<<<<<<<< │ @<<<<<<<<<<<<<<< │ @<<<<<<<<<<<<<<<
($network2) x 4
@<<<<<<<<<<<<... │ @<<<<<<<<<<<<... │ @<<<<<<<<<<<<... │ @<<<<<<<<<<<<...
($password2) x 4
─────────────────┼──────────────────┼──────────────────┼─────────────────
.
----------------------- File -----------------------
help/permissions.txt
  u   g   o    base               umask 022
 421 421 421 | File / Directory | File / Directory
-rwx rwx rwx | 666    777       | 644    755

file      default: 666 (-rw-rw-rw-) => 644 (-rw-r--r--)
directory default: 777 (drwxrwxrwx) => 755 (drwxr-xr-x)

dir: rwx
      \\\
       \\*- cd + use
        \*-- create stuff (requires x)
         *--- ll

x is needed everywhere!
we must be able to traverse (cd) the whole path

Example:

% namei -l /etc/yum/vars/test
dr-xr-xr-[x] root  root  /
drwxr-xr-[x] root  root  etc/
drwxr-xr-[x] root  root  yum/
drwxr-xr-[x] root  root  vars/
drwxr-xr-[x] mitko mitko test/

here mitko would be able to:
touch /etc/yum/vars/test/bla

because only the final (leaf) permissions count as long as x is everywhere.

NB: no w on any of the intermediary folders is needed by that user!

Special bits

  suid
setuid: allow users to run a program as the owner
        -rwsr-xr-x root root /usr/bin/sudo

setgid: allow users to run a program as the group
        -rwxr-sr-x root crontab /usr/bin/crontab

        directory with setgid:
        new files will inherit the directory's group

sticky bit:
when set on a directory, files inside can only be removed by their owners

 421
 ||+ sticky (t)
 |+ setgid (s if x also set, else S)
 + setuid (s)

ex: chmod 2755 is equivalent to
    chmod g+s

ACL

setfacl
getfacl -t
----------------------- File -----------------------
help/php_documentor.md
﻿DocBlock
========

```php
/**
 * Short Description
 *
 * Long Description: this line
 * and this one are part of the same paragraph
 * use a dot or an empty line if you want to start on a new line
 *
 * @author  this is a valid tag
 * @author: this is NOT (because of the colon)
 *
 * This {@author} is a valid tag
 * This  @author  is NOT (you need braces)
 */
```
_Warning:_ `/** This explanation is NOT parsed */` (2 lines is the minimum)

A DocBlock is valid only when the following element is:
-------------------------------------------------------

* class
* class method
* class variable (A variable local to a function can _NOT_ be documented)
* global variable
* function
* define statement
* include statement

_See:_ [Documentable PHP Elements](http://manual.phpdoc.org/HTMLSmartyConverter/HandS/phpDocumentor/tutorial_elements.pkg.html)

Package DocBlock:
-----------------

A `@package` tag should reside in it's own **DocBlock** and that **DocBlock**
should be the very first in the file:
```php
/**
 * @package Controllers
 */
```

Links
=====

[PHP Documentor  Documentation](http://manual.phpdoc.org/)

Tags
====

```php
 /**
  * The short description
  *
  * As many lines of extendend description as you want {@link element}
  * links to an element
  * {@link http://www.example.com Example hyperlink inline link} links to
  * a website. The inline
  * source tag displays function source code in the description:
  * {@source }
  *
  * In addition, in version 1.2+ one can link to extended documentation like this
  * documentation using {@tutorial phpDocumentor/phpDocumentor.howto.pkg}
  * In a method/class var, {@inheritdoc may be used to copy documentation from}
  * the parent method
  * {@internal
  * This paragraph explains very detailed information that will only
  * be of use to advanced developers, and can contain
  * {@link http://www.example.com Other inline links!} as well as text}}}
  *
  * Here are the tags:
  *
  * @abstract
  * @access       public or private
  * @author       author name <author@email>
  * @copyright    name date
  * @deprecated   description
  * @deprec       alias for deprecated
  * @example      /path/to/example
  * @exception    Javadoc-compatible, use as needed
  * @global       type $globalvarname
    or
  * @global       type description of global variable usage in a function
  * @ignore
  * @internal     private information for advanced developers only
  * @param        type [$varname] description
  * @return       type description
  * @link         URL
  * @name         procpagealias
    or
  * @name         $globalvaralias
  * @magic        phpdoc.de compatibility
  * @package      package name
  * @see          name of another element that can be documented,
  *                produces a link to it in the documentation
  * @since        a version or a date
  * @static
  * @staticvar    type description of static variable usage in a function
  * @subpackage    sub package name, groupings inside of a project
  * @throws       Javadoc-compatible, use as needed
  * @todo         phpdoc.de compatibility
  * @var        type    a data type for a class variable
  * @version    version
  */
 function if_there_is_an_inline_source_tag_this_must_be_a_function()
 {
 // ...
 }
```
----------------------- File -----------------------
help/postfix.txt
                                    postfix

Makefile for Berkeley DB files:
https://github.com/kurkale6ka/scripts/blob/master/mail/makefile.db.sh

Forwarding:
mynetworks -> anywhere
 strangers -> relay_domains

local_recipients:
for local(8) delivery, all mydestination domains are equal, with one shared namespace.
fname.lname(@domain not needed) ok

virtual_alias_maps:
Tables will be searched in the specified order until a match is found. Yes but:

VIRTUAL(5):
user@domain address, address, ... <- 1st this form is tried against all tables
user address, address, ...        <- then this one
@domain address, address, ...     <- and finally this one

Address rewriting:
          cleanup(8)                                  local(8)
canonical (envelope + headers)    -    queues    -    aliases
  virtual (envelope)

NB: in transport_maps we can overwrite the local transport with something else
    so aliases will not be reached.

virtual aliasing solves one problem:
it allows each domain to have its own info mail address

/etc/postfix/virtual:
info@example.com joe

/etc/postfix/aliases:
info: info

Certificates:
Domain validity in mail world is established using DKIM and SPF vs CA.
So if our DKIM/SPF checks pass, clients will be safe and they will accept our self signed certificate.

smtpd_tls_security_level = may # for incoming emails, encrypt with our cert if client wants it
 smtp_tls_security_level = may # for outgoing emails, encrypt with remote cert if available

smtpd_tls_cert_file = # the .chn is actually expected as there is no smtpd_tls_ca_file

Monitoring:
http://www.postfix.org/DEBUG_README.html
# postfix/smtpd[22222]: fatal:
tail -n10000 /var/log/maillog | egrep 'postfix/[^[:space:]]+\[[[:digit:]]+\]: (error|fatal|panic):'

Queues:
mailq # see all queues
postcat -q ID
qshape deferred # queue shape

Commands:
postconf -m # supported lookup tables
postconf -xf alias_maps # expand value
postconf -nf alias_maps # changed value
postconf -df alias_maps # default value
postconf -d mail_version
postmap transport # create transport.db
postalias aliases # postmap works too
postmap -s # show all
postmap -q key # show key
postfix reload # main.cf + master.cf
postfix check  # permissions
----------------------- File -----------------------
help/powershell.adoc
== Check

`$PROFILE`

== Create

 mkdir $PROFILE\WindowsPowerShell # if needed
 echo '' > $PROFILE               # touch equivalent

== Edit

 ise $PROFILE
 Set-PSReadLineOption -EditMode Emacs
 Set-PSReadLineOption -BellStyle Visual
----------------------- File -----------------------
help/printf.pl
#! /usr/bin/env perl

use v5.12;
use warnings;
use List::Util 'pairs';
use Term::ANSIColor qw/:constants color/;

my $BLUE = color 'ansi69';

# figlet -f smslant
say << '';
             _      __  ___
   ___  ____(_)__  / /_/ _/
  / _ \/ __/ / _ \/ __/ _/
 / .__/_/ /_/_//_/\__/_/
/_/

# text used in the examples
my @lorem = split /\n/, << '';
In addition to the standard
format specifications
described in printf(1)
and printf(3)

select STDOUT;

# Right align
say GREEN, '# Min width 35 right align, pad with spaces';
say $BLUE, q/printf '%35s\n' "${arr[@]}"/, RESET;

$~ = 'RIGHT';
format RIGHT =
@>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>...
$_
.

write foreach @lorem;
print "\n";

# Left align
say GREEN, '# Min width 35 left align, left align (no width means left aligned)';
say $BLUE, q/printf '%-35s%s\n' "${arr[@]}"/, RESET;

$~ = 'LEFT';
format LEFT =
@<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<...@<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<...
$$_[0],                            $$_[1]
.

write foreach pairs @lorem;
print "\n";

# Pad numbers with zeros
say GREEN, '# Pad numbers with zeros';
say $BLUE, q/printf '%03d\n' 1 22 333/, RESET;

printf "%03d\n", $_ foreach qw/1 22 333/;
print "\n";

# Write quotes in octal
say GREEN, q/# \047 is the octal value of '/;
say $BLUE, q/printf '\047Quotes\047\n'/, RESET;

say "\047Quotes\047\n";

# Use * as a placeholder for the length
say GREEN, '# * refers to 31';
say $BLUE, q/printf '%*s\n' 31 'Asus z87-pro'/, RESET;

printf "%31s\n\n", 'Asus z87-pro';

# Formats
print << '';
%q - print the associated argument shell-quoted, reusable as input
%d - print the associated argument as signed decimal number
%s - interprets the associated argument literally as string
----------------------- File -----------------------
help/prompt.adoc
== short
`PS1='[\u@\h \w]\$ '`

== with day and time

 PS1='[\u@\h \w] \d \t \$ '
 PS1='\D{%d %b} [\u@\h \w] \A \$ '
          │  │
          │  └─ short month name _# strftime_
          └─ day 01 - 31

 man bash (search for PS1)

 \d     date in "Weekday Month Date" format (e.g., "Tue May 26")
 \h     hostname up to the first .
 \t     time in 24-hour HH:MM:SS format
 \A     time in 24-hour HH:MM    format
 \u     user
 \w     CWD full
 \W     CWD basename
 \$     root # else $
----------------------- File -----------------------
help/ps.txt
                                  ps BSD-style
                                  ============

export PS_PERSONALITY=bsd
export PS_FORMAT=pid,ppid,pgid,sid,tname,tpgid,stat,euser,egroup,start_time,cmd

       .- a: ! only yourself
      /.-- u: user-oriented format
     //.--- x: ! must have a tty
    ///
ps auxfwh
       \\\
        \\*--- h: display headers (if PS_PERSONALITY)
         \*-- w: wide output (ww for unlimited)
          *- f: ASCII art hierarchical forest

Formats
-------
u: user oriented | USER PID %CPU %MEM VSZ RSS TT STAT STARTED TIME COMMAND
j: bsd job ctrl  | USER PID PPID PGID SESS JOBC STAT TT TIME COMMAND
l: bsd long      | UID PID PPID CPU PRI NI VSZ RSS WCHAN STAT TT TIME COMMAND
s: signal        | UID PID PENDING BLOCKED IGNORED CAUGHT STAT TTY TIME COMMAND
v: virtual mem   | PID STAT TIME SL RE PAGEIN VSZ RSS LIM TSIZ %CPU %MEM COMMAND
X: registers     | PID STACKP ESP EIP TMOUT ALARM STAT TTY TIME COMMAND

Custom: o pid=PROCESS_ID,stat,euser,egroup,start_time,cmd
                   \
                    *- new column name (no headers, use: ps ax o ppid=)

Less used options
-----------------
U httpd # userlist: id or name
Z       # SELinux column

Examples
--------

ps up 111408 # list for PID
ps jp 111408
ps lp 111408

ps uU httpd # list for user
ps jU httpd
ps lU httpd

# List parents of defunct processes
ps ax o ppid,cmd | grep [d]efunct | awk '{print $1}' | xargs ps up

NB: iptables processes cannot be seen with ps as iptables is kernel based!
    use iptables -nvL instead.
----------------------- File -----------------------
help/puppet.txt
Hiera

Hiera is a key/value lookup tool for configuration data, built to make Puppet
better and let you set node-specific data without repeating yourself.

hiera.yaml:

---
:backends: yaml
:yaml:
  :datadir: "/etc/puppetlabs/code/environments/%{environment}/hieradata"
:hierarchy:
  - "nodes/%{::trusted.certname}"
  - "nodes/%{::fqdn}"
  - "virtual/%{::is_virtual}"
  - common

Example with a node name of web01.example.com:
$trusted[certname] = web01.example
agent facts fqdn = web01.example.com
environment = production (set from ENC so not a fact)
$::is_virtual = true

hiera would check the following data sources, in order:

/etc/puppetlabs/code/environments/%{environment}/hieradata/nodes/web01.example.yaml
/etc/puppetlabs/code/environments/%{environment}/hieradata/nodes/web01.example.com.yaml
/etc/puppetlabs/code/environments/%{environment}/hieradata/virtual/true.yaml
/etc/puppetlabs/code/environments/%{environment}/hieradata/common.yaml

In a normal (priority) lookup, Hiera will stop at the first data source with
the requested data and return that value.

command line check:
hiera key environment=production ::fqdn=localhost.local
   -> value

Debugging

puppet master -d --codedir=/mnt/nas/puppet --compile localhost.local
----------------------- File -----------------------
help/python/argparse.md
# nargs
```sh
parser.add_argument("--foo", nargs="?", const="c", default="d")
parser.parse_args(["--foo", "YY"]) # Namespace(foo="YY")
parser.parse_args(["--foo"])       # Namespace(foo="c")
parser.parse_args([])              # Namespace(foo="d")
```
----------------------- File -----------------------
help/python/pdb.md
# Start the debugger
- `python -m pdb main.py`
- `breakpoint()` in code, then `python main.py`

_Set a breakpoint_:
```python
(Pdb) b line, condition
(Pdb) b                 # list all
```

# Pdb repl
```python
  example.py(2)<module>()
-> print("one")

  example_1.py(4)get_abs_path()
-> print("two")

> example_2.py(5)get_path()    # file(line)function()
-> print(f"path = {filename}") # code where we paused (to be executed)
(Pdb)
```
- `w` will print the stack trace
- `u` will move a frame up in the stack trace ( `>` will move between `example|example_1|example_2`)
- `d` will move a frame down in the stack trace

# Move through code
```python
> example.py(7)<module>()
   for dir in dirs:
->     path = get_path()
   print("done")
(Pdb)
```
- `n` will continue to the _next_ **logical** line (next iteration in this example)
- `unt` will continue to a line greater than the current one (it will exit the loop here)
- `s` will _step_ inside `get_path()`
- `c` will _continue_ to the next breakpoint

# Show code
- print: `p`, `pp`
- list: `l`, `ll`, `l.` (recentre to current)
- monitor `<expr>` for changes: `display <expr>`, list all: `display`
----------------------- File -----------------------
help/python/python.md
# Variables
Variables are pointers to objects and objects can be of arbitrary data type,  
thus variables cannot have types associated with them

```python
x = 11, y = x
id(x) == id(y) # True: y is x
```

# Quotes
`'` and `"` are equivalent

# Tips
In interactive mode, the last printed expression is assigned to `_`

## comprehensions
whenever a list needs to be modified in a loop (e.g with `.append()`/`.remove()`),  
assess if using a comprehension wouldn't be easier/clearer

## misc
```python
# 1.
str.index # raises  ValueError ~
str.find  # returns -1

# 2.
if 'yellow' in colors: # shorter/easier than
any(c == 'yellow' for c in colors)

# 3. check map/filter
filter(str.isalpha, text)

# 4.
return True if a > b else False # True if True False if False :-)
return a > b                    # just return the boolean expression
```

## sequences
```python
zip('abs', '@&$') # strings are sequences
<=>
zip(['a', 'b', 's'], ['@', '&', '$'])
-------------------------------------
list(...) => [('a', '@'), ('b', '&'), ('s', '$')]
```

## packing
```
  packing: assign to *t, we get a single tuple
unpacking:       use *t, we get all items
```

# Decorators
Pass my function to a _decorator_ function, so it can apply it AND add extra  
functionality, then return a new function to replace my initial one:

```python
my_func = decorator(my_func) # which is what happens with:

@decorator
my_func(...)
```

so now `my_func` executes:
* own code
* _decorator_'s code, which is part of `my_func` now

# Iterators
- iterables implement `__iter__` to return an _iterator_
- an _iterator_ implements `__next__` to get elements + raises `StopIteration`
- a _generator_ helps to create an _iterator_ thanks to the `yield` statement

## generator expression
`(n**2 for n in seq)`:
* create items as needed => little RAM, infinite sequences
* iterate once, no access to list methods

## list comprehension
`[n**2 for n in seq]`:
* create whole list in RAM

## itertools
`islice()` parameters act similarly to `range()`'s ones,  
the difference is we get an _iterator_, not a range.

```python
groupby(iterable, key=None) # define a key per item (identity by default)
                            # -> (key1, iterator1), ..., (keyN, iteratorN)

operator.itemgetter(0) # same as lambda g: g[0]
```

# Regex
| Example                               | Details                        |
| ---                                   | ---                            |
| `match = re.match(pattern, str)`      | `^match`                       |
| `match = re.fullmatch(pattern, str)`  | `^fullmatch$`                  |
| `match = re.search(pattern, str)`     | `/search/`                     |
| `fst, snd = re.findall(pattern, str)` | `findall` -> _list_ of matches |

_Note_: Get groups/`(.+)` with `match.groups()`

# Resource vs Client in AWS SDK boto3
https://stackoverflow.com/questions/42809096/difference-in-boto3-between-resource-client-and-session

# Virtual environments
Create isolated working copies of python with their own site packages

```bash
python3 -mvenv .venv
. .venv/bin/activate
# pip installs
deactivate
```

**NB**: else use: `pip install --user ...`
----------------------- File -----------------------
help/raid.txt
# RAID  0 - 2d - Striping
r/w fast but data loss

# RAID  1 - 2d - Mirroring
    R/w fast but ½ storage
rebuild fast but no hot swap (poweroff needed)

# RAID 10 - 4,6,...d - stripe across RAID 1 pairs/legs (0 & 1 combined)
    r/w fast but ½ storage
rebuild fast
hot swap
can withstand 2 fails (depending upon which 2)

# RAID  5 - 3d - Blocks striped across n-1 drives + block parity checksum on the last
r fast but w (+rebuild) slow because of checksum
hot swap

# RAID  6 - 4d - Striping with double parity
r fast but w even slower
withstands 2 fails
----------------------- File -----------------------
help/regex.txt
                                   Regex

              .- a single char
             /  .- a string of length 0 or more
            /  /
    Globs: ?  * [..]                                  bash,    find
POSIX BRE: . .* [..] ^ $ \{,\} \(\) \1               (s)ed,    grep
  GNU BRE: --------------------------- \? \+ \|      (s)ed,    grep
  GNU ERE: ------------------------------------ no \ (s)ed -r, grep -E, awk
           \w \W \s \S \b \B \< \>
           Multiline: \` ^ $ \'

Notes:

* POSIX's [[:upper:]] will match according to the locale
  perl's one will match unicode chars

* \d (not in BRE/ERE) is a shortcut for [[:digit:]] and is equivalent to [0-9]
  only faster

* POSIX: \ cannot escape stuff in [...]

* perl: The \d\s\w\D\S\W abbreviations can be used both inside and outside of
  character classes

Extended Globs: (shopt -s extglob in bash)

    @(item1|item2): 1      <=> item1|item2
    ?               0 or 1 occurrence of the given patterns
    *               0 or >
    +               1 or >
    !               none   <~> [^...]

Test in irb with: 'your_string'.match(%r@...@)
----------------------- File -----------------------
help/ripgrep.txt
-s :        case - default
-S :  smart case
-i : ignore case

man gitignore
     -g '!.git'  : exclude
--iglob 'foo/**' : search foo, case insensitively

--no-heading - disable:
file1
  res1
  res2

-I (no filenames) - disable:
file1: res1
file1: res2

-N : no line numbers

--hidden
----------------------- File -----------------------
help/rrd.txt
                      Round Robin Database tool (RRDtool)

RRDtool lets you create a database, store data in it, retrieve that data and
create graphs in PNG format for display on a web browser.

An RRD is composed of RRAs (db -> tables)

rrdtool create test.rrd              \ #                          ,any data out of range is saved as UNKNOWN
            --start 920804400        \ #                         /
            DS:speed:COUNTER:600:U:U \ # DS:var:DST:heartbeat:min:max
            RRA:AVERAGE:0.5:1:24     \ #               \
            RRA:AVERAGE:0.5:6:10       #                `time in seconds before saving UNKNOWN
                               \
                                keep 10 samples (an average of 6 values in here)

rrdtool update test.rrd 920804700:12345 920805000:12357 920805300:12363
rrdtool fetch test.rrd AVERAGE --start 920804400 --end 920809200
rrdtool graph speed.png                     \
         --start 920804400 --end 920808000  \
         DEF:myspeed=test.rrd:speed:AVERAGE \
         LINE2:myspeed#FF0000
             \
              pixels

The database holds one data source (DS) named "speed" that represents a counter
that is read every 5m (--step 300) which is the default.

Several round robin archives (RRA) can be kept in the same database.

Data source types (DST):
COUNTER  - saves the rate of change
DERIVE   - same as COUNTER, but it allows negative values as well
ABSOLUTE - also saves the rate of change, but it assumes that the previous value is set to 0
GAUGE    - does not save the rate of change. It saves the actual value itself

RRA:CF:xff:step:rows
--------------------

Consolidation function (CF) - AVERAGE, MINIMUM, MAXIMUM, and LAST
   Uses step number of PDPs per row (primary data points: one PDP per --step: see create below)

xff - XFiles Factor (XFF), this is the percentage of PDPs that can be unknown
      without making the recorded value unknown

rrdtool create target.rrd         \
        --start 1023654125        \
        --step 300                \ # 5 minutes
        DS:mem:GAUGE:600:0:671744 \
        RRA:AVERAGE:0.5:12:24     \ # (resolution: 12  * 300 = 3600 , one value per 1h,  24 => a day)
        RRA:AVERAGE:0.5:288:31      # (resolution: 288 * 300 = 86400, one value per 24H, 31 => a month)
----------------------- File -----------------------
help/rsync.txt
Usage:
rsync src... dst
        \
         local or host:'file1 file2 ...'

rsync -ain src  dst # copies src            to dst
rsync -ain src/ dst # copies src's contents to dst

Filters: (first match wins)
-f'+ foo'  # include files and directories named foo
-f'- foo/' # exclude directories named foo (but not files)
-f'+ */' -f'+ *.vim' -f'- *' # transfer vim files only
       \
        the / means directory objects

-f".- $HOME/.gitignore" -f':- .gitignore' -f'- .git'
             \                   \
              \                   ignore patterns in .gitignore files located anywhere
               exclude patterns in this file
               the - is needed because the patterns don't start with -/+/...
               otherwise ". $HOME/.gitignore" would be enough

-f':e- .gitignore' # --delete isn't safe with :(with . yes); use :- and --delete-after

rsync -ain -f'+ /*/.bashrc' -f'- *' ~/github/bash dst:
                \
                 this is like a regex ^, it's not an absolute path. it means
                 .bashrc in a subfolder of the transfer root (~/github/bash) so /*/.bashrc ~ /bash/.bashrc

ex: rsync -ai -f'+ */' -f'- *' ~/foo /tmp # copy foo's directory structure to /tmp

Archive mode -a:
-r: recursive
-l: copy symlinks as symlinks
-p: preserve permissions
-t: preserve modification times
-g: preserve group
-o: preserve owner - remote super-user only (i.e. ssh as root)
-D: --devices:  preserve device files (super-user only)
    --specials: preserve special files

Notes about -d and -a:
* without -a or -d, no dirs would be transfered
* -d allows to copy dirs but isn't recursive

rsync -a ~/foo  /tmp # copy ~/foo and its contents to /tmp
rsync -d ~/foo  /tmp # copy ~/foo without its contents to /tmp
rsync -d ~/foo/ /tmp # copy ~/foo's contents to /tmp

Common:
-n: dry run

--delete: if the file exists in the destination only, delete it
-u: if the destination file has a more recent modification time, don't update it

-z: enable compression
    use to improve bandwidth => not for local operation unless good CPU

Less common:
-e: "ssh -l user_if_different_than_local"
-I: compare files even if size and timestamp are the same
    (this happens if -t is omitted => not very efficient)
--size-only: compare files based on size only (no modification timestamp)

--ignore-existing: transfer new files only (non existing on destination)
--existing: transfer old files only (already existing on destination)

-W: transfer whole modified files (not only the changed blocks) if network
    bandwidth is not an issue for you but CPU is! (default for local)

Examples:
rsync -ai --no-t --remove-source-files --ignore-existing 2012/ pics/2012
         \    \             \                   \
          \    \             \                   only transfer new files
           \    \             del from source after transfer
            \    don't compare timestamps
             change summaries

rsync -ain --modify-window=432000 2015/ ../pics/2015
                     \
                      timestamps are considered different only above 5 days (432000 seconds)
----------------------- File -----------------------
help/ruby.txt
Rack Servers
============

Rack provides a minimal interface between webservers supporting Ruby and Ruby frameworks

Phusion Passenger (mod_rails, mod_rack) is a free web server and application
server with support for Ruby, Python and Node.js. It is designed to integrate
into the Apache HTTP Server or the nginx web server. It supports arbitrary Ruby
web frameworks through the Rack interface

Unicorn is an HTTP server for Rack applications designed to only serve fast
clients on low-latency, high-bandwidth connections and take advantage of
features in Unix/Unix-like kernels

Bundler
=======

Ensure a bundle of gems get installed at their expected versions.

gem install bundler

Gemfile:
gem "nokogiri", "~> 1.4.4" # I want nokogiri as long as it’s greater than version 1.4.4

bundle install will install the latest stable version of nokogiri in this case.
Using Gemfile.lock, we make sure, say, version 1.5.3 gets installed.

So first, Gemfile.lock gets created after running bundle install, then it gets
used for all subsequent runs of bundle install!

bundle exec executes a script using the gems specified in the script's Gemfile
rather than the systemwide Gemfile
----------------------- File -----------------------
help/sar.txt
By default sar displays CPU stats for the current day

sar 5 4
     \ display 4 lines of data
      CPU stats every 5 seconds

sar -1 -r -s 16:45:00 -e 18:55:00
      \  RAM
       yesterday
       -f /var/log/sa/sa26 for a specific day

sar -m TEMP 0
             average temperature stats since boot
----------------------- File -----------------------
help/scalability.txt
horizontally / out
  add more computers/nodes

vertically / in
  add more CPU, RAM
----------------------- File -----------------------
help/sed.txt
h H - Copy/append pattern space to hold space
g G - Copy/append hold space to pattern space

sed -n '1!G;$!h;$p' myfile

PS = pattern space
HS = hold space

1. read aa: N/A                     , copy to HS        , N/A
2. read bb: append HS to the 2nd PS , copy 2nd PS to HS , N/A
...
5. read ee: append HS to the 5th PS , N/A               , print the last PS

sed reads every line from myfile and puts it in the pattern space.
it then acts on it, possibly adding more lines...
addresses + commands apply on the pattern space, not on myfile.

2p means print the 2nd pattern space
it does NOT mean:
- print the 2nd line from the file (bb in myfile)
- print the 2nd line in the current pattern space (aa below)

myfile        pattern space
  aa -------------- aa
  bb ----------- bb \  this is the 2nd pattern space (bb \n aa)
  cc -------- cc aa /
  dd ----- dd bb
  ee -- ee cc aa
        dd bb
        cc aa
        bb
        aa

NB: d - delete the pattern space; IMMEDIATELY START NEXT CYCLE

Example: only print paragraphs with 'Administration' in them

sed -e '/./{H;$!d}' -e 'x;/Administration/!d' or
sed -r '/^\s*$/!{H;$!d};x;/Administration/!d'
           \
            keep appending till you reach an empty line.
            $!d restarts the cycle, that's why x;/Administration/!d is not executed
            /pattern/d would delete the whole PS, not only the matching line within the PS
----------------------- File -----------------------
help/selinux.txt
https://wiki.gentoo.org/wiki/SELinux/Quick_introduction

SELinux: Mandatory Access Control

With MAC, the permissions are not governed by the owner of the resource
(ex: give r access to others), instead, they are imposed by the administrator.

If regular permissions (DAC) disallow an activity, SELinux is not even consulted

In SELinux, everything has a context (label):

..._u:..._r:..._t:sensitivity
    |     |     |
    |     |     +-- type (file, domain for process)
    |     +-- role
    +-- user

All fields are used to decide access.
Most rules however are made for the type only.

ex: httpd_t not allowed to write to etc_t locations

Note:
a process will get it's domain from the type corresponding to the exe file,
or it will inherit it's parent's domain?

NB: a process running in an unconfined_t domain, won't heed rules!

Policy rules:
allow auditd_t auditd_log_t:file { write };

sesearch --allow --source auditd_t --target auditd_log_t --class file --perm write

Logging locations:
/var/log/avc.log
/var/log/audit... # lines with avc

search logs:
ausearch -m avc -ts today # access vector cache since today

AVC denial:
msg=audit(1363289005.532:184) # when: date -d@1363289005.532
denied { read } for pid=29199 comm="Vim" # denied for command Vim with PID ...
name="passwd" dev="sysfs" ino=30 # trying to read file 'passwd' (inode 30)
scontext = staff_u:staff_r:..._t # Vim
tcontext = system_u:object_r:sysfs_t tclass=file # passwd

find /sys -xdev -inum 30

Troubleshooting:
sestatus
setenforce 0
restorecon
change context
touch /.autorelabel + reboot
----------------------- File -----------------------
help/sensu.txt
sensu client on remote host:
  checks:
    is nginx running?

sensu server processes events:
  exe handlers

sensu services only communicate with the message bus: RabbitMQ

2 check execution schedulers:
  - sensu server schedules and publishes check execution requests to subscribed clients
  - sensu client (monitoring agent) schedules and executes a “standalone” check
----------------------- File -----------------------
help/shift.txt
Define $@
set a b c d e f

$1 $2 $3 $4 $5 $6
a  b  c  d  e  f
b  c  d  e  f    - after shift
d  e  f          - then  shift 2

zsh only
$1 $2 $3 $4 $5 $6
a  b  c  d  e  f
a  b  c  d  e    - after shift -p # pop
----------------------- File -----------------------
help/shred.txt
shred -v --random-source=/dev/urandom -n1 /dev/sdX

- whole disk
- default is -n3 (3 passes) but 1 is enough for large disks
----------------------- File -----------------------
help/shutdown.txt
Stop all CPU functions (kill all?), keep powered on
---------------------------------------------------
halt
halt -p
halt --reboot

Power off
---------
poweroff
poweroff --halt
poweroff --reboot

Reboot
------
reboot
reboot -p
reboot --halt

Schedule power down
-------------------
shutdown -P, --poweroff (or -h)
shutdown -H, --halt
shutdown -r, --reboot

examples:
shutdown -h now
shutdown -r +5 'The system will be rebooted in 5 minutes!'
shutdown -r now
shutdown -r +0

Warm boot:
   no interruption of power.

Cold boot (aka 'hard reset' or 'power drain'):
   poweroff; hold the power button (unplugged) to drain residual power,
   plug back in and boot up with 'clean' power.
----------------------- File -----------------------
help/sockets.txt
UNIX domain sockets are an IPC (inter-process communication) mechanism for local processes.

TCP/IP sockets allow communication over the network. You can use TCP/IP sockets
locally via the loopback interface.

UNIX sockets are subject to file system permissions, while TCP sockets can be
controlled only on the packet filter level.

UNIX sockets are local, so they can avoid some checks and operations (like
routing) which makes them faster and lighter than IP sockets.
----------------------- File -----------------------
help/socks5.txt
                       ---------------
client using the proxy                 socks5 proxy ____________ Internet
VPN client                             VPN server
                       ---------------

ssh -D 1234 remote:
* remote will be our socks5 proxy
* our client (browser) needs to use localhost:1234
----------------------- File -----------------------
help/sort.txt
GNU
---
             .- from 3rd field 2nd char to 4th field 1st char
            /        .-- numeric
           /        /
sort -t: -k3.2,4 -rn
       \           \
        \           *-- reverse
         *- separator for -k

Vim
---
                  .- numeric
                 /
:sort! /\d\+$/ rn
      \         \
       \         *-- on pattern (or after if no r)
        *- reverse
----------------------- File -----------------------
help/ssh/agent-forwarding.txt
                              SSH Agent Forwarding
           http://www.unixwiz.net/techtips/ssh-agent-forwarding.html

Password authentication
-----------------------
password required for every connection

--> user
<-- PASSWORD
--> password (/etc/passwd)

Public Key access
-----------------
passphrase required for every connection

--> user + request to use a public key
<-- KEY CHALLENGE (after looking in ~user/.ssh/authorized_keys)
--> key response (after unlocking id_rsa with the passphrase)

Public Key access with Agent support (local key holder)
-------------------------------------------------------
passphrase required only once and used for all connections

--> user + request to use a public key
<-- KEY CHALLENGE
    * The ssh client receives the key challenge, and forwards it to the waiting agent
    * The agent constructs the key response and hands it back to the ssh process
      (after unlocking id_rsa with the passphrase)
--> key response

Public Key access with Agent Forwarding (localhost -> server1 -> server2)
-------------------------------------------------------------------------
passphrase required only once and used for all connections, plus allow sshing
from remote destinations by forwarding the key challenge to the initial local server

server1 $ --> user + request to use a public key on server2
server1 $ <-- KEY CHALLENGE
              ssh FORWARDS the key challenge to sshd which relays it to localhost,
              which in turn sends a key response, then sshd FORWARDS the key response to ssh
server1 $ --> key response

Forwarding requirements:
localhost
  * ssh key held by the local agent
  * ~/.ssh/config
    Host server1
       ForwardAgent yes # or ssh -A server1

server1
  * AllowAgentForwarding in sshd_config

server1,2
  * public key installed for user
----------------------- File -----------------------
help/ssh/port-forwarding.txt
                              TCP Port forwarding

Local forwarding
----------------
access outside proxying via gateway

             8080:remote:80 gateway # ANYONE connecting to my pc:8080 are forwarded to remote via gateway (4 hops)
   localhost:8080:remote:80 gateway #          services on my pc:8080 are forwarded to remote via gateway
localhost:8080:localhost:80 gateway #          services on my pc:8080 are forwarded to            gateway

Remote forwarding: my pc is the gateway
---------------------------------------
access intranet from outside via my pc (LAN backdoor)

      2222:beyond:22 remote #        ANYONE on remote:2222 are forwarded beyond via my pc, GatewayPorts yes             on remote (4 hops)
   2222:localhost:22 remote #        ANYONE on remote:2222 are forwarded         to my pc, GatewayPorts yes             on remote
   2222:localhost:22 remote #      services on remote:2222 are forwarded         to my pc                                         (default)
IP:2222:localhost:22 remote # IP connecting to remote:2222 are forwarded         to my pc, GatewayPorts clientspecified on remote

also needed: AllowTcpForwarding

NB: in the above examples, 2222 gets forwarded to 22 (ssh). When setting up the
    tunnel on my pc, I need the correct user in order to connect to remote but
    then It's just TCP forwarding. so ssh -p2222 anyuser@remote would work if
    setup correctly; the tunnel is just 'saying': 2222 -> 22 allowed

ssh -fNL
     |+- do not execute a remote command
     +- go to background (exiting the terminal won't close the tunnel)

local forwarding example:
our LAN policy prevents access to imgur.com but our jumphost(gateway) can connect:

# locally
ssh -fNL 3000:imgur.com:3000 gateway # localhost:3000 -> gateway -> imgur.com:3000
ssh -fNL 3000:localhost:3000 gateway # localhost:3000 -> gateway:3000
                   \___________/

ask for port 3000 locally, get port 3000 on the remote

in ~/.ssh/config:
host gateway
   LocalForward 3000 imgur.com:3000 # use it with: ssh -fN gateway

use non privileged ports locally:
ssh -fNL 1443:10.1.0.200:443 gateway

Connect via https://localhost:1443

Secure remote (reverse tunnel) desktop assistance
-------------------------------------------------
ssh -fNR 5901:localhost:5900 gateway # user, no firewall openings needed
ssh -fNL 5902:localhost:5901 gateway # support (5902 was used vs 5900 to avoid interference with a local VNC server)

# then support can connect via:
            Screen Sharing - localhost:5902 # Darwin
vncviewer -SecurityTypes ??? localhost:5902 # Linux

# ^ ask for port 5902 locally, get port 5900 on the user machine.
# a VNC server must listen on port 5900 on the user machine,
# 'Screen Sharing' listens on that port by default.

Dynamic
-------
Forward all ports: "a poor man's VPN"

ssh -fND 4567 gateway # <- the ssh process BECOMES the socks server.

ff request -> (proxy via the local socks server) -> (tunnel to) gateway
                \
                 point to it with - localhost:4567
                 blank all other proxy fields!

It's as if our requests were originating from the gateway.
Connect via https://gateway (or any IP accessible from the gateway)
        NOT https://localhost:4567

why not use ssh -L?
If you need to access https://company.com, https://company.com:1234 and other
ports, you would need to setup as many local tunnels vs a single dynamic one.
----------------------- File -----------------------
help/ssh/sftp.txt
sftp access only

useradd -mN -gwordpress -s /sbin/nologin -c'sftp only account' user1
         |+-- no user group
         +-- create home

Install key in ~user1/.ssh/authorized_keys
                       \   +-- mode 600
                        +-- mode 700

                             all users followed by the exception list
                            /
Match Group marketing User *,!user1,!user2
  ForceCommand internal-sftp -d /reports # auto cd
  PasswordAuthentication no
  ChrootDirectory /srv/
  PermitTunnel no
  AllowAgentForwarding no
  AllowTcpForwarding no
  X11Forwarding no

the chroot must be root owned (0:0) and writeable by root only.

A 2nd Match doesn't reset parameters enabled by the 1st one. Also,
ForceCommand internal-sftp in the 1st match is what rules even if
ForceCommand internal-sftp none is set in a following match

errors

-P 2222 is silently ignored if used after the IP
----------------------- File -----------------------
help/ssh/tips.txt
== Generate

 ssh-keygen -ted25519 -Ccomment
 ssh-keygen -trsa -b4096 -Ccomment

== Fingerprints

....
ssh-add -l
ssh-keygen -lf ~/.ssh/id_rsa
ssh-keygen -Emd5 -lf ~/.ssh/id_rsa

4096 SHA256:AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA ~/.ssh/id_rsa (RSA)
4096 MD5:fc:fc:fc:fc:fc:fc:fc:fc:fc:fc:fc:fc:fc:fc:fc:fc ~/.ssh/id_rsa (RSA)
....

== Chain ssh invocations
`ssh -t user1@server1 'ssh user2@server2'`

== Keys

=== location on servers

`/etc/ssh # HostKey in sshd_config`

=== Convert from OPENSSH PRIVATE KEY to RSA/PEM PRIVATE KEY

- backup `id_rsa`
- `ssh-keygen -m pem -f id_rsa`

=== store

 /usr/bin/ssh-add -K ~/.ssh/id_rsa # store in keychain - Mac OS only
 /usr/bin/ssh-add -A               # add from keychain - Mac OS only
 Host *
    AddKeysToAgent yes
    UseKeychain yes                #                   - Mac OS only
----------------------- File -----------------------
help/storage.txt
Storage options

file
* hierarchy of folders, limited vertical scalability
* direct + NAS storage

block
* no metadata
* decouple the data from the user’s environment
* doesn’t rely on a single path to data—like file storage does—it can be retrieved quickly
* most expensive
* used in SANs (note: it's also network attached storage)

object
* HTTP API
* most metadata
* objects can’t be modified => write the object completely at once
* no lock mechanism
* flat structure, so better at scaling
* least expensive

SDS (Software Defined Storage)

create a storage pool from cheap hardware.

- Unlike LVM it spans physical hosts.

- Unlike monolithic SAN and NAS systems that tightly couple software and
  hardware, SDS products enable users to upgrade the software separately from
  the hardware.

iscsi - scsi block over TCP

db
iSCSI initiator
| | | | multipath
iSCSI target
SAN

dm multipath

dmsetup ls
lsblk

multipath -t : list configs per vendor
perl -ne 'print "$& wwid: ", `/lib/udev/scsi_id -p0x83 -d/dev/$& --whitelist` if /sd.$/' /proc/partitions
----------------------- File -----------------------
help/strace.adoc
Each line contains the system call name, followed by its arguments in
parentheses and its return value (-1 for error)

call(args) = return value

system calls are the kernel interface for userland processes

== open(2)
open returns a file descriptor for use in subsequent system calls

 open("/a/file", O_RDONLY) = 8 # reused by 'read' below
 read(8, "blabla"..., 65536) = 65536

== read(2)
read up to count bytes from fd into buf
`read(fd, buf, count) -> number-of-bytes-read`
----------------------- File -----------------------
help/su.txt
Run as user:
su - <...> -c '...'
----------------------- File -----------------------
help/sudoers.txt
        ALL (ALL) ALL
users hosts=(usr)           commands
users hosts=(usr) NOPASSWD: commands
     \         \
      \         run as
       allowed users

Examples:
---------

1.

%monitoring 10.0.0.10=(sensu) NOPASSWD: ALL

users in the monitoring group and connecting from 10.0.0.10 can run
passwordless any command user sensu has permissions to run:

sudo -usensu check_space

2.

User_Alias ADMINS     = user1, user2
Host_Alias WEBSERVERS = www1, www2, www3
Cmnd_Alias PROCESSES  = /usr/bin/nice, /bin/kill, /usr/bin/renice

ADMINS WEBSERVERS=(ALL) PROCESSES

In this example ADMINS is not a Linux group, it's just a list of users

sudo /bin/kill 1024

Misc:
-----

# mins before asking for password again
Defaults:user timestamp_timeout=510

Run as user:
sudo -u<...> bash -c '...'
----------------------- File -----------------------
help/svn.txt
Resolve conflicts

filename
  First postpone, then edit this file. It has markers for .mine, .r1 & .r2
  Run 'svn resolved filename' when done

filename.mine
  My changes. Before the 'svn up'

filename.rOLDREV
  The file as it was before my changes (in filename.mine)

filename.rNEWREV
  HEAD of repository. Received after 'svn up'

> local unversioned, incoming dir add upon update

with this kind of conflicts,
svn revert -R
will be needed, in addition to any other steps

svn cat https://your-server/path/to/file@rev
----------------------- File -----------------------
help/switch.txt
Connect over serial cable:

minicom -D /dev/ttyS0 -b 9600 -s # then disable flow control
                    \           \
                     \           +- setup
                      +- serial

After setup, you are back on the main screen and nothing happens... unless you press enter!

# BSD
cu -l /dev/cua01 -s 9600

escape: ~. # cu only?

# run these 2 before most commands
enable # en?
conf

show:

show running-config # run?
show interface status
show interfaces configuration
show interfaces port-channel 1
show ip interface
show vlan

assign IP:

interface vlan 1
ip addr 10.1.1.110 /24

set password:

username <user> password <password> privilege 15
enable password <password> # enable root pass

configure Static LAG Port Channel:

interface range gi1/0/1-4 # range optional?
channel-group 1 mode on # creates port-channel 1 (ports grouped under 1) as static LAG (mode on)
end

set ports to stack mode:

stack
stack-port tengigabitethernet 1/0/1 stack
stack-port tengigabitethernet 1/0/2 stack
end

define vlans:

interface range gi1/0/4,gi1/0/12-16
switchport mode access
switchport access vlan 4
exit

save:
copy running-config startup-config

end vs exit?
Te - tengigabitethernet?

Dell

port notation:

Te1/0/1
| | | +- port
| | +- row
| +- switch
+- speed

stack 3 switches:

           Te1/0/2
           |
switch1: ロロロロ- only 1 row of SFP+ ports, so Tex/0/x
          \
switch2: ロロロロ
          \
switch3: ロロロロ
         |
         Te3/0/1

final step: connect Te1/0/2 with Te3/0/1
----------------------- File -----------------------
help/sysctl.txt
The 'default' subtree affects all new interfaces. Assigning a value to a
configuration key beneath default will not affect existing interfaces but acts
as a template for new interfaces.
The highest value set in either the interface specific subtree or the 'all'
subtree will be the effective setting.

Here we want 0 on reverse path filtering for p1p1 so 'all' needs to be 0 else
it would overwrite p1p1's value:

% sysctl -w 'net.ipv4.conf.all.rp_filter=0' # temp: put in /etc/sysctl.conf for persistence
% sysctl -ar '\.rp_filter'

net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.p1p1.rp_filter = 0
----------------------- File -----------------------
help/syslog.txt
                                   syslog-ng

Final means that anything that goes through that log path (past the filters and
to the destination), will not go to any other log paths defined after it. Any
other messages will be sent to the other log paths

Basically it means we always want final unless we want duplicated logs.

final is ~ or stop in rsyslog lingo

                                    rsyslog

NB: the postfix log file is defined in syslog

https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/System_Administrators_Guide/s1-basic_configuration_of_rsyslog.html

rule: filter action

Filters:

* FACILITY.PRIORITY (selector)
  - mail.=crit -/var/log/maillog
         |     +-- no sync after every write to the file
         +-- only crit, not crit and higher

* :PROPERTY, [!]COMPARE_OPERATION, "STRING"
  - :msg, contains, "error"
    :msg, !regex, "fatal .* error"

* if EXPRESSION then ACTION else ACTION (rsyslog's RainerScript)

Actions:

- static path: /var/log/messages

- dynamic path:
  $template DynamicFile,"/var/log/test_logs/%timegenerated%-test.log"
                                            +-- property (building block of a syslog message: rsyslog.conf)
  *.* ?DynamicFile

- remote path: @[(compression level z1-9)]HOST:[PORT]
               +-- add a 2nd @ to use TCP
               ex: *.* @@(z9)example.com:6514

- output channel: primarily used to specify the maximum size a log file can grow to
  $outchannel log_rotation, /var/log/test_log.log, 104857600, /home/joe/log_rotation_script
  *.* :omfile:$log_rotation

- program
  *.* ^test-program;template # format message with template and pass it to test-program
  # a syslog message has a default format that can be altered with $template

- db
  $ModLoad ompgsql
  *.* :PLUGIN:DB_HOST,DB_NAME,DB_USER,DB_PASSWORD;[TEMPLATE]
      +-- ompgsql

- discard
  cron.* ~ # discard any cron syslog messages

Multiple actions example:
kern.=crit user1     # send to user one
& ^test-program;temp # ...
& ~                  # ...

New syntax:
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/System_Administrators_Guide/sec-using_the_new_configuration_format.html

ruleset(name="remote-601") {
    cron.* action(type="omfile" file="/var/log/remote-601-cron")
    mail.* action(type="omfile" file="/var/log/remote-601-mail") # rule: filter action
}

input(type="imtcp" port="601" ruleset="remote-601");

Prevent syslog from reading journal's database, use imuxsock vs imjournal:
https://rsyslog.readthedocs.io/en/latest/configuration/modules/imjournal.html
https://www.rsyslog.com/doc/v8-stable/configuration/modules/imuxsock.html#syssock-use

Logging on CentOS 7:
- apps send log messages to /dev/log
- journald forwards copies of those messages to /run/systemd/journal/syslog
- rsyslog listens on that new socket...

Potential problems:
- missing /run/systemd/journal/syslog socket, probably destroyed during rsyslog restart
  solution: systemctl restart systemd-journald.socket
            systemctl restart rsyslog

- messages not being forwarded to the new socket even though this forwarding is supposed to happen automatically
  solution: explicitly set ForwardToSyslog=yes in /etc/systemd/journald.conf, then
            systemctl restart systemd-journald

Tests:
% logger -p local0.info hello
% lsof /run/systemd/journal/syslog /dev/log
systemd   ... /dev/log
rsyslogd  ... /run/systemd/journal/syslog
systemd-j ... /dev/log
----------------------- File -----------------------
help/systemd.md
# commands
```sh
systemctl enable --now ...
systemctl cat ...
systemctl show ...
systemctl is-(active|enabled|failed) [pattern]
systemctl list-units rhel\* # add -a to also show inactive ones
systemctl edit -l (or --full) ...
```

`systemctl daemon-reload`

```
              add e(x)planations
             /
journalctl -xe -u <unit>
              \
               jump to (e)nd
```

# man pages
- systemd.directives
- systemd.service
----------------------- File -----------------------
help/tar.txt
tar zxvf archive.tgz          : extract
tar zcvf archive.tgz archive/ : create
tar  tvf archive.tgz          : view

replace z with J for .txz

# pigz
tar -Ipigz -cvf output.tgz output
tar cvf - output | pigz --best > output.tgz
----------------------- File -----------------------
help/tcpdump.txt
options
-------
-i any : listen on all interfaces
-nn    : no hostnames or port names
-A     : packet's contents in ASCII
-X     : packet's contents in both hex and ASCII
-s0    : get all packet sizes
-q     : less protocol info (ex: tcp 92 Vs ack 185 win 257 <nop,nop,timestamp...)

source / destination
--------------------
tcpdump host x.x.x.x
tcpdump src x.x.x.x
tcpdump dst x.x.x.x
tcpdump net x.x.x.x/24

protocol
--------
tcpdump icmp

port
----
tcpdump port 636
tcpdump src port 22
tcpdump dst port 389
tcpdump portrange 21-53

size
----
tcpdump less 32
tcpdump greater 128

capture
-------
# can be read by wireshark, snort...
tcpdump -s 1514 port 53 -w capture_file
tcpdump -r capture_file

combine filters with and, or and not
------------------------------------
tcpdump src port 81 and udp
tcpdump udp and src port 53
# To group use ()s but escape them for bash
tcpdump 'src 10.0.0.1 and (dst port 389 or 21)'

SYN, SYN-ACK, ACK
-----------------

BSD tcpdump                                                this is how much I can receive
                                                          /
SYN:     rtsg.1023 > csam.login: S 768512:768512(0) win 4096 <mss 1024>            # initiate connection
SYN ack: csam.login > rtsg.1023: S 947648:947648(0) ack 768513 win 4096 <mss 1024> # acknowledge initiator
ack:     rtsg.1023 > csam.login: . ack 1 win 4096                                  # acknowledge remote

bytes 1st    last
data  768512:768512(0) # 0 means no data
----------------------- File -----------------------
help/tee.txt
Output to pipe and STDOUT:
... | tee /dev/tty | ...
----------------------- File -----------------------
help/tesseract.txt
                                      OCR
                         Optical character recognition

# Optical character recognition from a bg pic
tesseract test.png stdout -l bul
tesseract test.png test -l bul

Note: -lbul is a fail! Use -l bul # space
----------------------- File -----------------------
help/time.txt
                                      Time
                                      ====

Regexes
-------

Date:
#    Year     Month    Day
# 1900-2099 / 01-12 / 01-31
^(?:19|20)\d\d/(?:0?[1-9]|1[0-2])/(?:0?[1-9]|[12]\d|3[01])$

Time:
# 03:24:44 Pm
^(?:0?\d|1\d|2[0-3])(?::[0-5]\d){1,2}[[:space:]]*(?:[aA]|[pP])[mM]$
----------------------- File -----------------------
help/tips_schema.md
﻿One can customize all labels in a XForm:
----------------------------------------

```xml
<xs:element name="description">
   <xs:annotation>
      <xs:appinfo>
         <alf:label>Description</alf:label>
         <alf:alert>A description is required, length restriction: 250 characters</alf:alert>
         <alf:appearance>+minimal+</alf:appearance>
      </xs:appinfo>
   </xs:annotation>
   <xs:simpleType>
      <xs:restriction base="xs:string">
         <xs:minLength value="0"/>
         <xs:maxLength value="250"/>
      </xs:restriction>
   </xs:simpleType>
</xs:element>
```

The above would create a basic `<textarea>` form field (not a **rich text
editor**). Make sure you position the **annotation** element immediately after
the opening `xs:element` tag, no other elements can go in between.

_Note:_ be careful with use of `minimal`, don't add it unless it is needed.

_Note:_ If a value starts with `${` and ends with `}`, the Form UI will look up
for the key/value pair in a chain of property files

These default configured property file locations from highest to lowest precedence are:
* `/Company Home/Data Dictionary/Web Forms/{form name}/strings.properties`
* `/Company Home/Data Dictionary/Web Forms/strings.properties`
* `webclient.properties` (within the Alfresco webapp)

```
key1=val1
key2=val2
```

Webscripts:
-----------

Importing or including within an imported or included schema is a known Alfresco
bug. Your imports or includes must be one level deep atmost. Your webscripts
must reside within: `Data Dictionary/Web Scripts`.

Save Location for your schema:
------------------------------

If you want to use a `normalizedString` field for defining the name under which
your schema should be saved, here is the kind of **Freemaker** output pattern to
be used:
```
/${webapp}/brand/${xml['org:org']['org:brand']['org:name_id']}.xml
```

_See:_ [Output Path Pattern Examples](http://wiki.alfresco.com/wiki/WCM_Forms_Developer_Guide#Output_Path_Pattern_Examples)

Links:
------

* [xml schemas](http://www.learn-xml-schema-tutorial.com/ "xsd tutorial")
* [schema namespaces](http://www.ibm.com/developerworks/library/xml-schema/ "The basics of using XML Schema to define elements")
* [Forms Authoring Guide](http://wiki.alfresco.com/wiki/Forms_Authoring_Guide#Overview_of_supported_XML_Schema_structures_and_data_types "Supported structures and data types")
* [Alfresco Widgets](http://wiki.alfresco.com/wiki/Creating_XForms_Widgets "Creating XForms Widgets")
* [Unbeatable JavaScript Tools](http://docs.dojocampus.org/quickstart/index "Dojo")
* [Alfresco Developers](http://wiki.alfresco.com/wiki/Main_Page "Alfresco wiki")
* [Learning Alfresco Web Forms by Examples](http://drquyong.com/myblog/?p=65 "Dr. Q's Workshop")
----------------------- File -----------------------
help/tmux.txt
Remove annoying selected split

select-pane -M
or
right click

Semicolon

If ; is unescaped or outside of ''s
then it separates 'bind' from the next command
else it separates 'bind's commands

bind r source-file ~/.tmux.conf\; display-message "Config reloaded..."
bind r 'source-file ~/.tmux.conf; display-message "Config reloaded..."'
----------------------- File -----------------------
help/tomcat.txt
Version:

$ cd .../tomcat

$ bin/version.sh

or

$ java -cp lib/catalina.jar org.apache.catalina.util.ServerInfo

Extra checks:

$ java -version
----------------------- File -----------------------
help/udev.txt
udev
====

Uses:
-----

* Rename a device node from the default name to something else

* Provide an alternative/persistent name for a device node by creating a
  symbolic link to the default device node

* Name a device node based on the output of a program

* Change permissions and ownership of a device node

* Launch a script when a device node is created or deleted (typically when a
  device is attached or unplugged)

* Rename network interfaces

Example:
--------

/etc/udev/rules.d/83-disk.rules
KERNEL=="hdb", DRIVER=="ide-disk", SYMLINK+="sparedisk"

The above rule says: match a device which was named by the kernel as hdb AND
where the driver is ide-disk. Name the device node with the default name and
create a symbolic link to it named sparedisk

Using attributes for finer control:
-----------------------------------

SUBSYSTEM=="block", ATTR{size}=="234441648", SYMLINK+="my_disk"
----------------------- File -----------------------
help/useradd.txt
Add / Remove user
-----------------

useradd -m -Ggroups -cComment LOGIN
          \
           create home

userdel -r LOGIN
          \
           remove home + mail spool

Add to / Remove from group
--------------------------

gpasswd -a user group
gpasswd -d user group

Modify
------

usermod -a -Ggroups user

# todo: check about /etc/shadow, mv home, ...
usermod -l new-login login
----------------------- File -----------------------
help/video.txt
                                     Video

Frame rate:
-----------
the number of still pictures per unit of time of video
for good quality it must be >= 16

PAL   (Europe, Asia, Australia...)         25 frame/s
SECAM (France, Russia, parts of Africa...) 25 frame/s
NTSC  (USA, Canada, Japan...)              29.97 frame/s
are different color television standards

Aspect ratio:
-------------
width:height => 4:3, 16:9

Video compression:
------------------
image compression + motion compensation

* H.264/MPEG-4 Part 10 or AVC (Advanced Video Coding) is a video compression
  format, and is currently one of the most commonly used formats

* Xvid is a video codec which implements encoding and decoding videos using the
  MPEG-4 Part 2

Multimedia container formats
----------------------------
AVI, MP4, FLV, RealMedia, or Matroska
Such containers usually bundle both compressed video and audio streams (files)

      Codec                     encoders                      Containers
MPEG-2          |                                       |  .MPG
MPEG-4 Part 2   |  DivX, XviD (open), and Nero Digital  |  .AVI
MPEG-4 Part 10  |  x264, Mainconcept, QuickTime         |  .MP4, .MKV or .MOV

Note: MPEG-4 (AVC) or H.264 (this is the name of the standard)

A container will synchronize video and audio frames according to their
Presentation Time Stamp (PTS)
----------------------- File -----------------------
help/vim/easy_align.md
# Manual examples
```vim
%Ea:ar      " right align around :
%Ea\ arl    " align right then left affecting the first 2 spaces only - pyramid!
%Ea\ arl**  " same as above but repeat: rl rl ...
%Ea**\ arl  " same as above
%Ea\ arl*   " align right then always left
%Ea*\ arl   " same as above
%Ea,iu1     " align around , ignoring unmatched lines
%Ea*|       " align around all |s, not the 1st one only
%Ea*|<l0r2  " align around all |s, stick delimiter to the left with 0 margin before it and 2 after
%Ea*/=\+/dr " align around regexes with delimiters right aligned
```

_NB_: regarding the `arl**` and `arl*` examples above, the meaning of `*/**` is different than when not using `a` (see point 3. below)

1. `gaip`

2. Return key to select alignment mode (left, right, or center)

3. N-th delimiter (default: 1)
   - `2`  Around the 2nd occurrences of delimiters
   - `*`  Around all occurrences of delimiters
   - `**` Left-right alternating alignment around all delimiters
   - `-`  Around the last occurrences of delimiters (-1)
   - `-2` Around the second to last occurrences of delimiters

4. a) Predefined delimiter rule
```
<space> : General alignment around whitespaces
   =    : Operators containing equals sign ( =, ==, !=, +=, &&=, ...)
   :    : Suitable for formatting JSON or YAML
   .    : Multi-line method chaining
   ,    : Multi-line method arguments
   &    : LaTeX tables (matches & and \\)
   #    : Ruby/Python comments
   "    : Vim comments
   |    : Table markdown
   >    : Arrows (defined in $MYVIMRC)
```

   b) Or regex delimiter with `Ctrl-x` or `Ctrl-/`

# Interactive mode
```
Ctrl-f | filter          | [gv]/.*/? (limit lines around which we align)
Ctrl-i | indentation     | shallow, deep, none, keep
Ctrl-l | left_margin     | number or string (margin before delimiter)
Ctrl-r | right_margin    | number or string (margin after delimiter)
Ctrl-d | delimiter_align | left, center, right (the way we want the delimiter aligned)
Ctrl-a | align           | string: /[lrc]+\*{0,2}/
<left> | stick_to_left   | { 'stick_to_left': 1, 'left_margin': 0 }
       | stick to right  | set right margin to 0 while left aligning
<down> | *_margin        | { 'left_margin': 0, 'right_margin': 0 } (no margins around delimiter)
```

# Examples

- `Ctrl-a` with `llr*`
```
aa = bb = cc = dd = ee
a = b = c = d = e
aaa = bbb = ccc = ddd = eee

gaip * Ctrl-a llr* =

aa  = bb  =  cc =  dd =  ee
a   = b   =   c =   d =   e
aaa = bbb = ccc = ddd = eee
```

- Pyramid
```
aa bb
a b
aaa bbb

gaap ** <return> <space>
or
gaap Ctrl-a rl <space>

aa bb
a b
aaa bbb
```

_Note_: if aligning doesn't work, `:se ft&`
----------------------- File -----------------------
help/vim/nvim.md
# Install from source in `/usr/local`
https://github.com/neovim/neovim/wiki/Building-Neovim

```sh
sudo make CMAKE_BUILD_TYPE=RelWithDebInfo
sudo make install
```

# prerequisite: `cmake 3` from sources
```sh
openssl-devel
./bootstrap --prefix=/usr/local/cmake3
gmake
sudo gmake install
```

remove installed files with: `parallel --xargs rm < install_manifest.txt`

## `cmake` links
```sh
sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake 10 \
--slave /usr/local/bin/ctest ctest /usr/bin/ctest \
--slave /usr/local/bin/cpack cpack /usr/bin/cpack \
--slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake \
--family cmake

sudo alternatives --install /usr/local/bin/cmake cmake /usr/local/cmake3/bin/cmake 20 \
--slave /usr/local/bin/ctest ctest /usr/local/cmake3/bin/ctest \
--slave /usr/local/bin/cpack cpack /usr/local/cmake3/bin/cpack \
--slave /usr/local/bin/ccmake ccmake /usr/local/cmake3/bin/ccmake \
--family cmake

mv /usr/bin/cmake{,.old}
ln -s /usr/local/cmake3/bin/cmake /usr/bin/
```
----------------------- File -----------------------
help/vim/vim.md
# ; in ranges
```vim
1
2
3
4    oge
5    [cursor]
6
7    oge
8
9

2,/oge/p " find the next occurrence of oge starting from the cursor's position
         " range printed: 2-7

2;/oge/p " find the next occurrence of oge starting from 2
         " range printed: 2-4

:h:;
```

# Buffer loading
```vim
:bun unload,           " :ls :ls!
:bd  unload,           "  ✗  :ls!
:bw  unload, del marks "  ✗   ✗
```

# Clipboard
```vim
cb^=unnamed     " y/d/c go to "" and "*
cb^=unnamedplus " y/d/c go to "" and "+
```

# for wasavi
`se is ic nu report=0 ai`

# Commands on range
```vim
:g    " exe ex commands on range
:norm " exe normal mode commands on range
      " use instead of :g/patt/norm when patt on adjacent lines
```

macro that updates every line in sequence - use one of the above

# `gf` and `'includeexpr`
`'includeexpr'` is used for gf if an unmodified file name can't be found
the problem is gf will always "find" and try to open an http(s):// URL,
which means `'includeexpr'` will never get a chance to modify it!

_Note_: disabling netrw doesn't help

_workaround_: `nmap <buffer> gf :e <cfile>:s###<cr>`

# Open a file by only changing some part in its PATH
`:e%:p:s/development/integration/`

# vimball
Invalid range fix: add newline to the end of file

# Generate a range of lines matching a pattern
```vim
function! easy#PipeRange(includepat) range

   let top = a:firstline
   let bot = a:lastline

   " No range has been given (top == bot), thus apply range extension logic
   if a:includepat != '' && top == bot
      if top < 0 || top > line('$') || getline(top) !~ a:includepat
         return
      endif
      " Extend range upwards while we match the pattern
      while top > 1 && getline(top-1) =~ a:includepat
         let top -= 1
      endwhile
      " Extend range downwards while we match the pattern
      while bot < line('$') && getline(bot+1) =~ a:includepat
         let bot += 1
      endwhile
   endif

   let lines = map(range(top, bot), 'getline(v:val)')

endfunction
```
----------------------- File -----------------------
help/virtualbox.txt
# Install guest additions for:

* full screen
* copy/paste
* host only adapter

mount then run from vm, click on CD in file explorer...

# Enable ssh with default NAT mode:

VM net settings/advanced/port forwarding

host           guest
127.0.0.1 2222 10.0.2.15 22

~/.ssh/config
HostName 127.0.0.1
Port 2222
User <vm-user>

# Other

- NAT adapter for internet access
  ifup ifcfg-enp0s3 + enable at boot!

- host only adapter for ssh to vm:
  Global Tools / Host Network Adapter / add vboxnet0
  use static address: 192.168.56.101
----------------------- File -----------------------
help/vlans.txt
read about ap management network

Create a VLAN:
assign the same PVID, Port VLAN ID, to a bunch of ports

A trunk port can carry traffic for all VLANs. For this to work frames must be tagged with their VLAN id. Normal ports are called access ports

fw - vlan port: tag frame with pvid
ap - trunk port: tag frame with pvid (as defined in the wlans setup on the ap)

can trunk ports be part of a vlan?

T, U, F meaning on Dell 2834 switch

U - untagged, ie: non trunk port
T - tagged, ie: trunk port
F - forbidden...

Define VLAN10:

        1 2 3 4 . . . 22 23 24
Static  U U U U       T  T  T
Current U U U U       T  T  T
   \
    static or saved value vs current temporarily changed value but not persisted?

NB: we have to define both the untagged and the tagged ports

LAG???
----------------------- File -----------------------
help/voip.txt
soft phone ~ voip server - INTERNET - voip server ~ soft phone
           |             |
        analogue      digital


2 options:
- local PBX (Private Branch Exchange)
- cloud telephony provider (hosted voip)

reserve number with PSTN (Public switched telephone network) so our PBX gets contacted

PBX
IP PBX
hosted PBX
----------------------- File -----------------------
help/vpn.txt
As root:

cd /etc/openvpn

wget https://downloads.nordcdn.com/configs/archives/servers/ovpn.zip
unzip ovpn.zip

cd /etc/openvpn/ovpn_udp/

Choose a server
https://nordvpn.com/servers/tools/

openvpn --config /etc/openvpn/ovpn_udp/us298.nordvpn.com.udp.ovpn --auth-user-pass [pass-file]

[pass-file]
login
password (My services / NordVPN / Service credentials)
----------------------- File -----------------------
help/watch.txt
         1s intervall (default: 2s)
        /
watch -n1 -d command
            \
             highlight diffs
----------------------- File -----------------------
help/weechat.txt
Startup

/set irc.look.temporary_servers on
/connect irc.libera.chat
or
/server add lib irc.libera.chat -temp
/connect lib

/j #perl
/filter add irc_smart * irc_smart_filter *

Keys

ctrl+x - switch between servers

alt+1 - buffer
alt+/ - last buffer
alt+m - toggle mouse
alt+a - activity
alt+k - grab a key and insert its code

F9/10 - title to the left/right
----------------------- File -----------------------
help/wget.adoc
== Download "folder" from URI

`wget -r -q -nH -np --cut-dirs=2 -R'index.html*' https://people.redhat.com/kwalker/repos/rpm-deathwatch/`
----------------------- File -----------------------
help/wiki-markup.md
# Jira / Confluence

## Links
- external: `[text|https://...]`
- local:    `[text|^attachment.txt]`

## Monospace
`{{..}}`

# Markdown

## Links
`[Duck Duck Go](https://duckduckgo.com)`
----------------------- File -----------------------
help/windows.txt
Licenses

OEM - original equipment manufacturer:
Linked to your computer, cannot be transfered in case of hardware failure.
Your product activation key is recorded on the motherboard, it won't be
required when you reinstall windows on the same pc.

Full retail version:
Can be transfered (not copied) to another computer.
To be used on a SINGLE pc only at any given time.

Boot

Create bootable USB 2 (not 3) drive using Rufus from a Windows pc!
For old computer/BIOS choose MBR/ntfs during the .iso copy phase.

BIOS settings:
- legacy boot
- no secure or fast boot (can be disabled by setting admin pass)

In order to choose between Home and Pro versions, a file named ei.cfg must be
present in the Sources folder of your bootable media with these contents:

[Channel]
_Default
[VL]
0

Dvorak
Language settings/English (United Kingdom) -> options -> US dvorak

use dvorak layout at startup:
typing/advanced keyboard settings (at the bottom)
----------------------- File -----------------------
help/write.txt
write <user> [tty]

Send a message (as arg or on STDIN) to all users
wall [message]

Control access to your terminal by others
mesg [y|n]
----------------------- File -----------------------
help/xargs.md
```bash
cmd1 | xargs cmd2
```

execute cmd2 on cmd1's output lines in bulk (and in parallel with `-P0`)

```bash
printf '%s ' 1 2 3 4 | xargs -n2 # default command is echo
# 1 2
# 3 4
```
----------------------- File -----------------------
help/xml.txt
svn log --xml authorized_keys | xml2

echo 'cat //date/text() | //msg/text()' | xmllint --shell <(svn log --xml authorized_keys)

svn log --xml authorized_keys | xpath '//date/text() | //msg/text()' 2>&1 | sed 's/--\s*NODE\s*--/\r/' | dos2unix | sed '/^$/d' | tail -n+2 | paste - - | sed 's/T[[:digit:]]\{2\}.*Z//'

# echo {} was causing quotes problems because $msg contained 's
# solution: bash -c '...' $0 $1 $2 ...
#           echo "$1" -> messages with quotes properly quoted
svn log --xml authorized_keys2 | xpath '//date/text() | //msg/text()' 2>&1 | sed 's/--\s*NODE\s*--/\r/' | dos2unix | sed '/^$/d' | tail -n+2 | paste - - | tr '\n' '\0' | xargs -0 -I{} bash -c 'echo "$1" | { read dte msg; echo "$(date +"%d %b %Y" -d"$dte"), $msg"; }' _ {}
----------------------- File -----------------------
help/xz.txt
# compress
xz -v -T0 file
    |  +- use as many threads as cores
    +- show progress

xz -cv -T0 file > /debug/file.xz # output to a different partition

# decompress
unxz file.xz
xz -d file.xz

# list info
xz -l file.xz

# compress a folder
tar cvf etc.tar /etc/ && xz -v -T0 etc.tar # etc.tar.xz
tar Jcvf etc.txz /etc/

# misc
xzcat
xzgrep
----------------------- File -----------------------
help/yaml.txt
YAML is a superset of JSON => you can use json within a .yml file

--- # start of new yaml document (needed for single files with many yaml docs)
... #   end of     yaml document

key: > long line
  split for readability

key: | heredoc lines
  with newlines preserved

chomp modifiers - and +:
  >- and |- will remove the final newline

null_value_key: null or ~

Notes:
* quotes aren't needed around strings
* booleans: true, on, yes

List/array/sequence

[item1, item2]
-------------- or
- item1
- item2

Dictionary/hash/map/object

{item1: value1, item2: value2}
------------------------------ or
item1: value1
item2: value2

'? ' -> complex dictionary key

Set (map with null values)

? item1
? item2
-------------- or
{item1, item2}

Anchors

define

nested dictionary: &keys
  key1: value1

dictionary: &list
  - item1

dictionary key: &value my string

use

# MERGE the above keys
# GitLab - `extends: .job` (merge .job's keys)
dictionary1:
  <<: *keys

# include (NOT MERGE) the above list
dictionary2:
  - *list

# replace with 'my string'
# GitLab - `my_var: !reference [.vars, variables, Z_VAR]` (reuse only part of .vars's config)
dictionary3:
  key1: *value
----------------------- File -----------------------
help/yum.md
# History manipulations
```sh
yum history
yum history list|package-list|summary # '*pattern*' or id
yum history info id
yum history undo id
```

# List available versions (`apt-cache policy`)
```sh
yum list erlang
yum list --showduplicates erlang
```

# Disable stuff
```sh
yum --disablerepo='pgdg*' history
yum --disableplugin priorities
```

# Which package provides a given command (`rpm -qf =...`)
```sh
yum provides nc
```

# CentOS Vault
```sh
/etc/yum.repos.d/CentOS-Base.repo

baseurl=http://vault.centos.org/6.8/os/$basearch/
baseurl=http://vault.centos.org/6.8/updates/$basearch/
baseurl=http://vault.centos.org/6.8/extras/$basearch/
```
_Note_: also, comment 'mirrorlist' entries out
----------------------- File -----------------------
help/zend_framework.md
﻿The Front Controller
====================

`Zend_Controller_Front` is the unique entry point to our application. It serves
as a registry to the _router_, _dispatcher_, and _action controllers_ trough
parameters:
```php
$front->setParam('noErrorHandler', true);
$front->setParam('noViewRenderer', true);
```

Different useful methods:
-------------------------

```php
$front->setControllerDirectory(''); // used by the Dispatcher
$front->throwExceptions(true);      // instead of storing them into the Response
$front->returnResponse(true);       // check the stored exceptions
```
```php
$front->getRouter();
```
`Zend_Controller_Front` triggers different dispatch events which could be
observed by plugins. Thus, we don't need to extend the front controller to add
functionality.

The Zend_Controller basics
==========================

1\. `Zend_Router_Rewrite`

Writes controller/action/parameter into `Zend_Controller_Request_Http` with it's own methods:
```php
$request->setControllerName('');
$request->setActionName('');
```
2\. `Zend_Controller_Dispatcher_Standard`

Pulls controller/action/parameter from `Zend_Controller_Request_Http` with it's own methods:
```php
$request->getControllerName('');
$request->getActionName('');
$request->getUserParam('');
```
3\. `Zend_Controller_Dispatcher_Standard`

Loop `while !$request->isDispatched()`:
- Controller instantiated
- Action called

The workflow of `Zend_Controller` is relatively simple. A request is received by
`Zend_Controller_Front`, which in turn calls `Zend_Controller_Router_Rewrite` to
determine which controller (and action in that controller) to dispatch.
`Zend_Controller_Router_Rewrite` decomposes the **URI** in order to set the
controller and action names in the request. `Zend_Controller_Front` then enters
a dispatch loop. It calls `Zend_Controller_Dispatcher_Standard`, passing it the
request, to dispatch to the controller and action specified in the request (or
use defaults). After the controller has finished, control returns to
`Zend_Controller_Front`. If the controller has indicated that another controller
should be dispatched by resetting the dispatched status of the request, the loop
continues and another dispatch is performed. Otherwise, the process ends.

Routing
=======

*Note*: *Reverse Matching*
Routes are matched in reverse order so make sure your most generic routes are defined first.

1\. **Variable route**

```php
$route = new Zend_Controller_Router_Route(
    'archive/:year',
    array(
        'year'       => 2006,      // variable defaults (:year is our var part)
        'controller' => 'archive',
        'action'     => 'show'
    ),
    array('year' => '\d+')         // variable requirements
);
$router->addRoute('archive', $route);
```
2\. **Static route**

```php
$route = new Zend_Controller_Router_Route_Static(
    'login',
    array('controller' => 'auth', 'action' => 'login')
);
$router->addRoute('login', $route);
```
3\. **Regex route**

```php
$route = new Zend_Controller_Router_Route_Regex(
    'archive(?:/(\d+))?',
    array(
        1            => '2006',
        'controller' => 'archive',
        'action'     => 'show'
    )
    array(
        // Zend_Controller_Action::_getParam('year'); or
        // Zend_Controller_Request::getParam('year');
        1 => 'year' // or 'year' => 1
    )
);
$router->addRoute('archive', $route);
```

Dispatching
===========

```php
public function bazAction()
{
    // forward to an action in another controller in another module,
    // Foo_BarController::bazAction():
    $this->_forward(
        'baz',                  // action
        'bar',                  // controller
        'foo',                  // module
        array('baz' => 'bogus') // params
    );
}
```

Useful links
============

http://framework.zend.com/manual/en/zend.controller.html
----------------------- File -----------------------
help/zip.txt
zip    ts.zip t*      : add
zip -d ts.zip tee.txt : remove

unzip -l ts.zip : list
----------------------- File -----------------------
help/zombies.txt
                                    Zombies
                                    =======

You can’t kill a zombie process because it’s already dead. Zombies are
basically dead processes that haven’t been cleaned up. The child notifies his
parent that it has terminated, with the SIGCHLD signal. The parent then is
supposed to call wait() in order to gather info from the dead process. After
wait() is called, the zombie process is completely removed from memory. Zombies
keep a descriptor in memory + they preserve their PIDs. Zombies are harmless
unless there are so many it results in a shortage of PIDs.

Solutions:
* Resend SIGCHLD
* Kill the parent which will make init the new parent and rely on init to call
  wait() which it does periodically
* Reprogram the parent so it doesn't ignore SIGCHLD and calls wait()
----------------------- File -----------------------
help/zsh.txt
                                      zsh

# As your day to day user
# Change your default shell to zsh
chsh -s /bin/zsh

# Re-log, then:
if [[ -n $XDG_CONFIG_HOME && -n $XDG_DATA_HOME ]]
then
   mkdir -p {$XDG_CONFIG_HOME,$XDG_DATA_HOME}/zsh
fi

Right align and pad with :-: : :
for i in qw qwer qwerty; echo ${(l:19::-:: :)i}
---------------- qw
-------------- qwer
------------ qwerty

Query package owning pgrep:
rpm -qf =pgrep # dpkg -S =pgrep

> and < are 'synonyms' for cat

Truncate file:
:>file

Send both STDIN and STDERR over the pipe:
httpd -S |& grep ...

Alt-q:
Kill the whole line
do stuff, then on Enter
paste the line back

zle
---

Alt-x will open the minibuffer

Call a builtin widget, even if the behaviour has been redefined:
Ex: .end-of-line # prefix with .

Globs
-----

Glob operators:
                *, ?, []
                <x-y> numeric range
                ^x    anything except x # extended_glob
                x~y   x but not y       # extended_glob
                x#    0 or more of x    # extended_glob
                x##   1 or more of x    # extended_glob
            Ex:
                ls -l **/*<1-10>.txt

Globbing flags:
(#X) - affect text to their right; extended_glob required

                [[ hello == (#i)HELLO ]] && echo match
                ls -d -- (#i)*vim* # match files case insensitively

                (#cN,M) is equivalent to {N,M} in regex

                echo ${(L)variable} # to lower case, (U for upper)
                echo $variable:l    # to lower case  (u for upper)

Mystery: unsetopt CASE_GLOB then [[ hello == HELLO ]] && echo match # no output?
A: string matching isn't globbing. Find docs about this

Glob qualifiers:
                print -l **/*(/)    # dirs only
                print -l **/*(.)    # files only
                print -l **/*(@,.)  # links or files
                print -l **/*(L0)   # empty files
                print -l **/*(Lk+3) # greater than 3 KB
                print -l **/*(mh-1) # modified in the last hour
                print -l **/*(e:'[[ $REPLY != sometest ]]':) # estring

                '-' must be followed by the wanted qualifiers.
                Ex: -.,= means files + sockets and symlinks to those

Modifiers:
: inside a qualifier list => the rest is interpreted as a modifier

                print -l **/*(u0^@:t) # basename (tail) of files (minus links) owned by root
                echo ${foo:gs/r/@}    # parameter expansion with global substitution

Recursive globbing:
                **/ is equivalent to (*/)#

Parameter Expansion Flags:
                line=one:two:three
                print -l ${(s.:.)line} # field splitting at :
                           j:string:   # join words in an array

Word splitting exception:

% rm $(echo a b; echo '*')
rm: cannot remove ‘a’: No such file or directory
rm: cannot remove ‘b’: No such file or directory
rm: cannot remove ‘*’: No such file or directory

% rm "$(echo a b; echo '*')"
rm: cannot remove ‘a b\n*’: No such file or directory

% rm ${(f)"$(echo a b; echo '*')"}
rm: cannot remove ‘a b’: No such file or directory
rm: cannot remove ‘*’: No such file or directory

Open clean zsh with modified PATH:
exec env PATH=/usr/local/bin/:$PATH zsh -f

http://zsh.sourceforge.net/Doc/Release/Jobs-_0026-Signals.html
A job being run in the background will suspend if it tries to read from the terminal. =>
{ cmd1; cmd2;... cmdn; } </dev/null &

dash colors question:
My list separator is #. After zstyle ':completion:*:options' list-colors '=(\# *)=32' ls -<tab> shows descriptions in green but ls --<tab> is unchanged...

zsh compinit: insecure directories and files, run compaudit for list
compaudit | xargs chmod g-w
compaudit | xargs chown root

== Change your login shell
if `chsh -s /bin/zsh` doesn't work, use the terminal:
'run a custom command instead of my shell: `zsh --login`'
----------------------- File -----------------------
help/zsh_completion.txt
https://github.com/zsh-users/zsh-completions/blob/master/zsh-completions-howto.org

don't forget to run compinit after any changes!

debug with:
bindkey '^xh' _complete_help # show tags in current context

_arguments

(- :)-h forbids any options (-) or arguments (:) after -h
(--help)-h forbids --help after -h
-d+ allows -d /dir and -d/dir
action _path_files -/ completes dirs
::topic: is an optional (::) non-option arg with no action associated
